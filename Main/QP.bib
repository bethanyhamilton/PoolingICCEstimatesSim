
@article{adams2004,
  title = {Patterns of Intra-Cluster Correlation from Primary Care Research to Inform Study Design and Analysis},
  author = {Adams, Geoffrey and Gulliford, Martin C. and Ukoumunne, Obioha C. and Eldridge, Sandra and Chinn, Susan and Campbell, Michael J.},
  year = {2004},
  month = aug,
  journal = {Journal of Clinical Epidemiology},
  volume = {57},
  number = {8},
  pages = {785--794},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2003.12.013},
  abstract = {Objective To provide information concerning the magnitude of the intraclass correlation coefficient (ICC) for cluster-based studies set in primary care. Study design and setting Reanalysis of data from 31 cluster-based studies in primary care to estimate intraclass correlation coefficients from random effects models using maximum likelihood estimation. Results ICCs were estimated for 1,039 variables. The median ICC was 0.010 (interquartile range [IQR] 0 to 0.032, range 0 to 0.840). After adjusting for individual- and cluster-level characteristics, the median ICC was 0.005 (IQR 0 to 0.021). A given measure showed widely varying ICC estimates in different datasets. In six datasets, the ICCs for SF-36 physical functioning scale ranged from 0.001 to 0.055 and for SF-36 general health from 0 to 0.072. In four datasets, the ICC for systolic blood pressure ranged from 0 to 0.052 and for diastolic blood pressure from 0 to 0.108. Conclusion The precise magnitude of between-cluster variation for a given measure can rarely be estimated in advance. Studies should be designed with reference to the overall distribution of ICCs and with attention to features that increase efficiency.},
  langid = {english},
  keywords = {Cluster randomization,Cluster sampling,Design effect,General practice,Intraclass correlation,Primary care},
  file = {C\:\\Users\\micro\\Zotero\\storage\\TNY97NBU\\Adams et al. - 2004 - Patterns of intra-cluster correlation from primary.pdf;C\:\\Users\\micro\\Zotero\\storage\\2EWNGBYR\\S0895435604000459.html}
}

@article{ahnUseEstimatedIntraclass2012,
  title = {Use of the Estimated Intraclass Correlation for Correcting Differences in Effect Size by Level},
  author = {Ahn, Soyeon and Myers, Nicolas D. and Jin, Ying},
  year = {2012},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {44},
  number = {2},
  pages = {490--502},
  issn = {1554-3528},
  doi = {10.3758/s13428-011-0153-1},
  abstract = {In a meta-analysis of intervention or group comparison studies, researchers often encounter the circumstance in which the standardized mean differences (d-effect sizes) are computed at multiple levels (e.g., individual vs. cluster). Cluster-level d-effect sizes may be inflated and, thus, may need to be corrected using the intraclass correlation (ICC) before being combined with individual-level d-effect sizes. The ICC value, however, is seldom reported in primary studies and, thus, may need to be computed from other sources. This article proposes a method for estimating the ICC value from the reported standard deviations within a particular meta-analysis (i.e., estimated ICC) when an appropriate default ICC value (Hedges, 2009b) is unavailable. A series of simulations provided evidence that the proposed method yields an accurate and precise estimated ICC value, which can then be used for correct estimation of a d-effect size. The effects of other pertinent factors (e.g., number of studies) were also examined, followed by discussion of related limitations and future research in this area.},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\WZA52XYQ\\Ahn et al. - 2012 - Use of the estimated intraclass correlation for co.pdf}
}

@incollection{AnalysisVarianceEstimation1992,
  title = {Analysis of {{Variance Estimation}} for {{Unbalanced Data}}},
  booktitle = {Variance {{Components}}},
  year = {1992},
  pages = {168--231},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470316856.ch5},
  abstract = {The prelims comprise: Model formulation ANOVA estimation Henderson's Method I Henderson's Method II Henderson's Method III Method III applied to the 2-way crossed classification Nested models Other forms of ANOVA estimation Comparing different forms of ANOVA estimation Estimating fixed effects in mixed models Summary Exercises},
  chapter = {5},
  isbn = {978-0-470-31685-6},
  langid = {english},
  keywords = {connected data,maximum likelihood,mean squares,unbalanced data,variance components},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470316856.ch5},
  file = {C\:\\Users\\micro\\Zotero\\storage\\DT8Y5YEI\\1992 - Analysis of Variance Estimation for Unbalanced Dat.pdf;C\:\\Users\\micro\\Zotero\\storage\\R3ULN5AP\\9780470316856.html}
}

@incollection{BalancedData1992,
  title = {Balanced {{Data}}},
  booktitle = {Variance {{Components}}},
  year = {1992},
  pages = {112--167},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470316856.ch4},
  abstract = {The prelims comprise: Establishing analysis of variance tables Expected mean squares, E(MS) The 2-way crossed classification ANOVA estimation Normality assumptions A matrix formulation of mixed models Maximum likelihood estimation (ML) Restricted maximum likelihood Estimating fixed effects in mixed models Summary Exercises},
  chapter = {4},
  isbn = {978-0-470-31685-6},
  langid = {english},
  keywords = {balanced data,nested factor,population,random effects factors,variance components},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470316856.ch4},
  file = {C\:\\Users\\micro\\Zotero\\storage\\KE9ZP6FV\\1992 - Balanced Data.pdf;C\:\\Users\\micro\\Zotero\\storage\\UJ6D8B8Y\\9780470316856.html}
}

@article{bates2015,
  title = {Fitting Linear Mixed-Effects Models Using {{lme4}}},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {67},
  number = {1},
  pages = {1--48},
  doi = {10.18637/jss.v067.i01}
}

@article{blitstein2005,
  title = {Increasing the {{Degrees}} of {{Freedom}} in {{Future Group Randomized Trials}}: {{The}} Df* {{Approach}}},
  shorttitle = {Increasing the {{Degrees}} of {{Freedom}} in {{Future Group Randomized Trials}}},
  author = {Blitstein, Jonathan L. and Murray, David M. and Hannan, Peter J. and Shadish, William R.},
  year = {2005},
  month = jun,
  journal = {Evaluation Review},
  volume = {29},
  number = {3},
  pages = {268--286},
  publisher = {{SAGE Publications Inc}},
  issn = {0193-841X},
  doi = {10.1177/0193841X04273258},
  abstract = {This article builds on the previous article by Blitstein et al. (2005), which showed how external estimates of intraclass correlation can be used to improve the precision for the analysis of an existing group randomized trial. The authors extend that work to sample size estimation and power analysis for future group-randomized trials. Often this approach will allow a smaller study than would otherwise be possible without sacrificing statistical power. Such studies are needed, for example, as pilot studies to help plan for a full-scale efficacy trial, as replication studies, or in situations in which resource constraints prohibit a larger trial. The authors discuss the circumstances under which this strategy will be most helpful and the risks associated with conducting smaller studies.},
  langid = {english},
  keywords = {causal inference,degrees of freedom,group-randomized trial,power},
  file = {C\:\\Users\\micro\\Zotero\\storage\\EEDSYSL4\\Blitstein et al. - 2005 - Increasing the Degrees of Freedom in Future Group .pdf}
}

@article{bloomUsingCovariatesImprove2007,
  title = {Using {{Covariates}} to {{Improve Precision}} for {{Studies That Randomize Schools}} to {{Evaluate Educational Interventions}}},
  author = {Bloom, Howard S. and {Richburg-Hayes}, Lashawn and Black, Alison Rebeck},
  year = {2007},
  journal = {Educational Evaluation and Policy Analysis},
  volume = {29},
  number = {1},
  pages = {30--59},
  publisher = {{[American Educational Research Association, Sage Publications, Inc.]}},
  issn = {0162-3737},
  abstract = {This article examines how controlling statistically for baseline covariates, especially pretests, improves the precision of studies that randomize schools to measure the impacts of educational interventions on student achievement. Empirical findings from five urban school districts indicate that (1) pretests can reduce the number of randomized schools needed for a given level of precision to about half of what would be needed otherwise for elementary schools, one fifth for middle schools, and one tenth for high schools, and (2) school-level pretests are as effective in this regard as student-level pretests. Furthermore, the precision-enhancing power of pretests (3) declines only slightly as the number of years between the pretest and posttests increases; (4) improves only slightly with pretests for more than 1 baseline year; and (5) is substantial, even when the pretest differs from the posttest. The article compares these findings with past research and presents an approach for quantifying their uncertainty.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\MU89HHGS\\Bloom et al. - 2007 - Using Covariates to Improve Precision for Studies .pdf}
}

@article{bobakEstimationInterraterIntraclass2018a,
  title = {Estimation of an Inter-Rater Intra-Class Correlation Coefficient That Overcomes Common Assumption Violations in the Assessment of Health Measurement Scales},
  author = {Bobak, Carly A. and Barr, Paul J. and O'Malley, A. James},
  year = {2018},
  month = sep,
  journal = {BMC Medical Research Methodology},
  volume = {18},
  number = {1},
  pages = {93},
  issn = {1471-2288},
  doi = {10.1186/s12874-018-0550-6},
  abstract = {Intraclass correlation coefficients (ICC) are recommended for the assessment of the reliability of measurement scales. However, the ICC is subject to a variety of statistical assumptions such as normality and stable variance, which are rarely considered in health applications.},
  keywords = {Bayesian analysis,Hierarchical regression,ICC,Observer OPTION5,Reliability,Shared decision making,Variance function modelling},
  file = {C\:\\Users\\micro\\Zotero\\storage\\BE8HPYKF\\Bobak et al. - 2018 - Estimation of an inter-rater intra-class correlati.pdf;C\:\\Users\\micro\\Zotero\\storage\\8XA8RM3I\\s12874-018-0550-6.html}
}

@article{browne2006a,
  ids = {browne2006},
  title = {A Comparison of {{Bayesian}} and Likelihood-Based Methods for Fitting Multilevel Models},
  author = {Browne, William J. and Draper, David},
  year = {2006},
  month = sep,
  journal = {Bayesian Analysis},
  volume = {1},
  number = {3},
  pages = {473--514},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/06-BA117},
  abstract = {We use simulation studies, whose design is realistic for educational and medical research (as well as other fields of inquiry), to compare Bayesian and likelihood-based methods for fitting variance-components (VC) and random-effects logistic regression (RELR) models. The likelihood (and approximate likelihood) approaches we examine are based on the methods most widely used in current applied multilevel (hierarchical) analyses: maximum likelihood (ML) and restricted ML (REML) for Gaussian outcomes, and marginal and penalized quasi-likelihood (MQL and PQL) for Bernoulli outcomes. Our Bayesian methods use Markov chain Monte Carlo (MCMC) estimation, with adaptive hybrid Metropolis-Gibbs sampling for RELR models, and several diffuse prior distributions (\$\textbackslash Gamma\^\{ -1 \}( \textbackslash epsilon, \textbackslash epsilon )\$ and \$U( 0, \textbackslash frac\{ 1 \}\{ \textbackslash epsilon \} )\$ priors for variance components). For evaluation criteria we consider bias of point estimates and nominal versus actual coverage of interval estimates in repeated sampling. In two-level VC models we find that (a) both likelihood-based and Bayesian approaches can be made to produce approximately unbiased estimates, although the automatic manner in which REML accomplishes this is an advantage, but (b) both approaches had difficulty achieving nominal coverage in small samples and with small values of the intraclass correlation. With the three-level RELR models we examine we find that (c) quasi-likelihood methods for estimating random-effects variances perform badly with respect to bias and coverage in the example we simulated, and (d) Bayesian diffuse-prior methods lead to well-calibrated point and interval RELR estimates. While it is true that the likelihood-based methods we study are considerably faster computationally than MCMC, (i) steady improvements in recent years in both hardware speed and efficiency of Monte Carlo algorithms and (ii) the lack of calibration of likelihood-based methods in some common hierarchical settings combine to make MCMC-based Bayesian fitting of multilevel models an attractive approach, even with rather large data sets. Other analytic strategies based on less approximate likelihood methods are also possible but would benefit from further study of the type summarized here.},
  keywords = {adaptive MCMC,bias,Calibration,diffuse priors,hierarchical modeling,hybrid Metropolis-Gibbs sampling,IGLS,interval coverage,intraclass correlation,mixed models,MQL,PQL,random-effects logistic regression,REML,RIGLS,variance-components models},
  file = {C\:\\Users\\micro\\Zotero\\storage\\A9EWJSGX\\Browne and Draper - 2006 - A comparison of Bayesian and likelihood-based meth.pdf;C\:\\Users\\micro\\Zotero\\storage\\TRMXNCTN\\Browne and Draper - 2006 - A comparison of Bayesian and likelihood-based meth.pdf;C\:\\Users\\micro\\Zotero\\storage\\QSGPC7GP\\06-BA117.html}
}

@article{burch2011,
  title = {Assessing the Performance of Normal-Based and {{REML-based}} Confidence Intervals for the Intraclass Correlation Coefficient},
  author = {Burch, Brent D.},
  year = {2011},
  month = feb,
  journal = {Computational Statistics \& Data Analysis},
  volume = {55},
  number = {2},
  pages = {1018--1028},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2010.08.007},
  abstract = {Using normal distribution assumptions, one can obtain confidence intervals for variance components in a variety of applications. A normal-based interval, which has exact coverage probability under normality, is usually constructed from a pivot so that the endpoints of the interval depend on the data as well as the distribution of the pivotal quantity. Alternatively, one can employ a point estimation technique to form a large-sample (or approximate) confidence interval. A commonly used approach to estimate variance components is the restricted maximum likelihood (REML) method. The endpoints of a REML-based confidence interval depend on the data and the asymptotic distribution of the REML estimator. In this paper, simulation studies are conducted to evaluate the performance of the normal-based and the REML-based intervals for the intraclass correlation coefficient under non-normal distribution assumptions. Simulated coverage probabilities and expected lengths provide guidance as to which interval procedure is favored for a particular scenario. Estimating the kurtosis of the underlying distribution plays a central role in implementing the REML-based procedure. An empirical example is given to illustrate the usefulness of the REML-based confidence intervals under non-normality.},
  langid = {english},
  keywords = {Asymptotic distributions,Kurtosis,One-way random effects model,Pivotal quantity},
  file = {C\:\\Users\\micro\\Zotero\\storage\\F2HVX3ZW\\Burch - 2011 - Assessing the performance of normal-based and REML.pdf;C\:\\Users\\micro\\Zotero\\storage\\Y2LA8IQA\\S0167947310003221.html}
}

@article{chakraborty2009,
  title = {A Simulation Based Technique to Estimate Intracluster Correlation for a Binary Variable},
  author = {Chakraborty, Hrishikesh and Moore, Janet and Carlo, Waldemar A. and Hartwell, Tyler D. and Wright, Linda L.},
  year = {2009},
  month = jan,
  journal = {Contemporary Clinical Trials},
  volume = {30},
  number = {1},
  pages = {71--80},
  issn = {15517144},
  doi = {10.1016/j.cct.2008.07.008},
  abstract = {Cluster randomized trials have become the design of choice for evaluating the effect of selected interventions on well-known health indicators such as neonatal mortality rate, episiotomy rate, and postpartum hemorrhage rate in a community setting. Determining the sample size of a cluster randomized trial requires a reliable estimate of cluster size and the intracluster correlation (ICC), because sample size can be substantially impacted by these parameters. During the design phase of a trial, the investigators may have estimates of the valid range of the health indicator which is the primary outcome variable. Furthermore, investigators often have an estimate of the average cluster size or range of cluster sizes that exist among the proposed samples they are planning to include in the trial. We present in this article a simulation technique to estimate the ICC value and its distribution for known binary outcome variables and a varying number of clusters and cluster sizes. We applied this technique to estimate ICC values and confidence intervals for a multi-country trial assessing the effect of neonatal resuscitation to decrease seven-day neonatal mortality, where communities within a country were clusters. This simulation technique can be used to estimate the possible ranges of the ICC values and to help to design an appropriately powered trial.},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\E9MAPK7C\\Chakraborty et al. - 2009 - A simulation based technique to estimate intraclus.pdf}
}

@article{chakraborty2015,
  title = {Cluster {{Randomized Trials}}: {{Considerations}} for {{Design}} and {{Analysis}}},
  shorttitle = {Cluster {{Randomized Trials}}},
  author = {Chakraborty, Hrishikesh and Lyons, Genevieve},
  year = {2015},
  month = sep,
  journal = {Journal of Statistical Theory and Practice},
  volume = {9},
  number = {3},
  pages = {685--698},
  issn = {1559-8616},
  doi = {10.1080/15598608.2014.992081},
  abstract = {Scientists often use randomized controlled trials to compare a newly developed treatment to the existing one, or to a placebo. Patients are randomly assigned to a treatment, and they are compared with respect to the outcome of interest. The cluster randomized trial (CRT) is a type of randomized controlled trial in which the treatments are randomized at the group, rather than individual, level. The intracluster correlation (ICC) measures the degree of similarity between individuals within clusters. CRTs can be designed in several ways; it is essential that researchers carefully plan the study, from sample size calculations to ICC calculation to analysis, in order to get valid and meaningful results. In this article we review and discuss the considerations essential to conducting a successful CRT using both frequentist and Bayesian approaches, and we discuss recent trends in CRT analysis, including highlighting new methodology for both binary and continuous data.},
  langid = {english}
}

@phdthesis{chang2015,
  type = {Thesis},
  title = {Sufficient Sample Sizes for the Multivariate Multilevel Regression Model},
  author = {Chang, Wanchen},
  year = {2015},
  month = may,
  abstract = {The three-level multivariate multilevel model (MVMM) is a multivariate extension of the conventional univariate two-level hierarchical linear model (HLM) and is used for estimating and testing the effects of explanatory variables on a set of correlated continuous outcome measures. Two simulation studies were conducted to investigate the sample size requirements for restricted maximum likelihood (REML) estimation of three-level MVMMs, the effects of sample sizes and other design characteristics on estimation, and the performance of the MVMMs compared to corresponding two-level HLMs. The model for the first study was a random-intercept MVMM, and the model for the second study was a fully-conditional MVMM. Study conditions included number of clusters, cluster size, intraclass correlation coefficient, number of outcomes, and correlations between pairs of outcomes. The accuracy and precision of estimates were assessed with parameter bias, relative parameter bias, relative standard error bias, and 95\% confidence interval coverage. Empirical power and type I error rates were also calculated. Implications of the results for applied researchers and suggestions for future methodological studies are discussed.},
  langid = {english},
  annotation = {Accepted: 2015-09-08T18:27:31Z},
  file = {C\:\\Users\\micro\\Zotero\\storage\\Y6GBNDP8\\Chang - 2015 - Sufficient sample sizes for the multivariate multi.pdf;C\:\\Users\\micro\\Zotero\\storage\\EATGUZYL\\31009.html}
}

@article{coffman2008,
  title = {Asymptotic Distribution Free Interval Estimation: {{For}} an Intraclass Correlation Coefficient with Applications to Longitudinal Data},
  shorttitle = {Asymptotic Distribution Free Interval Estimation},
  author = {Coffman, Donna L. and {Maydeu-Olivares}, Alberto and Arnau, Jaume},
  year = {2008},
  journal = {Methodology: European Journal of Research Methods for the Behavioral and Social Sciences},
  volume = {4},
  number = {1},
  pages = {4--9},
  publisher = {{Hogrefe \& Huber Publishers}},
  issn = {1614-1881},
  doi = {10.1027/1614-2241.4.1.4},
  abstract = {Confidence intervals for the intraclass correlation coefficient (ICC) have been proposed under the assumption of multivariate normality. We propose confidence intervals which do not require distributional assumptions. We performed a simulation study to assess the coverage rates of normal theory (NT) and asymptotically distribution free (ADF) intervals. We found that the ADF intervals performed better than the NT intervals when kurtosis was greater than 4. When violations of distributional assumptions were not too severe, both the intervals performed about the same. The point estimate of the ICC was robust to distributional violations. We provide R code for computing the ADF confidence intervals for the ICC. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
  keywords = {autocorrelation,confidence intervals,distributional assumptions,intraclass correlation,Mathematical Modeling,model assumptions,Statistical Correlation},
  file = {C\:\\Users\\micro\\Zotero\\storage\\KP46XQNE\\Coffman et al. - 2008 - Asymptotic distribution free interval estimation .pdf}
}

@book{cohenStatisticalPowerAnalysis2013,
  title = {Statistical {{Power Analysis}} for the {{Behavioral Sciences}}},
  author = {Cohen, Jacob},
  year = {2013},
  month = sep,
  publisher = {{Academic Press}},
  abstract = {Statistical Power Analysis for the Behavioral Sciences, Revised Edition emphasizes the importance of statistical power analysis. This edition discusses the concepts and types of power analysis, t test for means, significance of a product moment rs, and differences between correlation coefficients. The test that a proportion is .50 and sign test, differences between proportions, and chi-square tests for goodness of fit and contingency tables are also elaborated. This text likewise covers the F tests of variance proportions in multiple regression/correlation analysis and computational procedures. This publication is intended for behavioral and biosocial scientists who use statistical inference, but also serves as a supplementary textbook for intermediate level courses in applied statistics in behavioral/biosocial science.},
  googlebooks = {rEe0BQAAQBAJ},
  isbn = {978-1-4832-7648-9},
  langid = {english},
  keywords = {Social Science / Essays,Social Science / Reference}
}

@incollection{ComputingMLREML1992,
  title = {Computing {{ML}} and {{REML Estimates}}},
  booktitle = {Variance {{Components}}},
  year = {1992},
  pages = {290--314},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470316856.ch8},
  abstract = {The prelims comprise: Introduction Iterative methods based on derivatives The EM algorithm General methods that converge rapidly for balanced data Pooling estimators from subsets of a large data set Example: the I-way random model Discussion Summary Exercises},
  chapter = {8},
  isbn = {978-0-470-31685-6},
  langid = {english},
  keywords = {analytical expressions,derivative equations,maximum likelihood estimation,numerical techniques,variance component estimators},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470316856.ch8},
  file = {C\:\\Users\\micro\\Zotero\\storage\\RMUXFD23\\1992 - Computing ML and REML Estimates.pdf;C\:\\Users\\micro\\Zotero\\storage\\SC2J4DAJ\\9780470316856.html}
}

@article{corbeil1976,
  title = {A {{Comparison}} of {{Variance Component Estimators}}},
  author = {Corbeil, R. R. and Searle, S. R.},
  year = {1976},
  journal = {Biometrics},
  volume = {32},
  number = {4},
  pages = {779--791},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  doi = {10.2307/2529264},
  abstract = {Explicit solutions have been derived for the maximum likelihood (ML) and restricted maximum likelihood (REML) equations under normality for four common variance components models with balanced (equal subclass numbers) data. Solutions of the REML equations are identical to analysis of variance (AOV) estimators. Ratios of mean squared errors of REML and ML solutions have also been derived. Unbalanced (unequal subclass numbers) data have been used in a series of numerical trials to compare ML and REML procedures with three other estimation methods using a two-way crossed classification mixed model with no interaction and zero or one observation per cell. Results are similar to those reported by Hocking and Kutner [1975] for the balanced incomplete block design. Collectively, these studies and those of Klotz, Milton and Zacks [1969] point, with some exceptions, to the greater efficiency of ML estimators under a range of experimental settings.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\SPJID7PZ\\Corbeil and Searle - 1976 - A Comparison of Variance Component Estimators.pdf}
}

@article{cornfield1978,
  ids = {CornfieldJ1978Rbga},
  title = {Randomization by {{Group}}: {{A Formal Analysis}}},
  shorttitle = {{{SYMPOSIUM ON CHD PREVENTION TRIALS}}},
  author = {Cornfield, Jerome},
  year = {1978},
  month = aug,
  journal = {American Journal of Epidemiology},
  volume = {108},
  number = {2},
  pages = {100--102},
  publisher = {{School of Hygiene and Public Health of the Johns Hopkins University}},
  address = {{United States}},
  issn = {0002-9262},
  doi = {10.1093/oxfordjournals.aje.a112592},
  keywords = {Analysis of Variance,Humans,Research Design - standards},
  file = {C\:\\Users\\micro\\Zotero\\storage\\9U5YBBLN\\CORNFIELD - 1978 - SYMPOSIUM ON CHD PREVENTION TRIALS DESIGN ISSUES .pdf;C\:\\Users\\micro\\Zotero\\storage\\JTPHM94Z\\53261.html}
}

@article{davisEstimationInferenceThreeLevel,
  title = {Estimation and {{Inference}} of the {{Three-Level Intraclass Correlation Coefficient}}},
  author = {Davis, Matthew David},
  pages = {110},
  abstract = {Since the early 1900's, the intraclass correlation coefficient (ICC) has been used to quantify the level of agreement among different assessments on the same object. By comparing the level of variability that exists within subjects to the overall error, a measure of the agreement among the different assessments can be calculated. Historically, this has been performed using subject as the only random effect. However, there are many cases where other nested effects, such as site, should be controlled for when calculating the ICC to determine the chance corrected agreement adjusted for other nested factors. We will present a unified framework to estimate both the two-level and three-level ICC for both binomial and multinomial outcomes. In addition, the corresponding standard errors and confidence intervals for both ICC measurements will be displayed. Finally, an example of the effect that controlling for site can have on ICC measures will be presented for subjects nested within genotyping plates comparing genetically determined race to patient reported race.},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\XP8BGRLC\\Davis - Estimation and Inference of the Three-Level Intrac.pdf}
}

@article{dedrick2009,
  ids = {dedrick2009multilevel},
  title = {Multilevel {{Modeling}}: {{A Review}} of {{Methodological Issues}} and                {{Applications}}},
  shorttitle = {Multilevel {{Modeling}}},
  author = {Dedrick, Robert F. and Ferron, John M. and Hess, Melinda R. and Hogarty, Kristine Y. and Kromrey, Jeffrey D. and Lang, Thomas R. and Niles, John D. and Lee, Reginald S.},
  year = {2009},
  month = mar,
  journal = {Review of Educational Research},
  volume = {79},
  number = {1},
  pages = {69--102},
  publisher = {{American Educational Research Association}},
  issn = {0034-6543},
  doi = {10.3102/0034654308325581},
  abstract = {This study analyzed the reporting of multilevel modeling applications of a sample of 99 articles from 13 peer-reviewed journals in education and the social sciences. A checklist, derived from the methodological literature on multilevel modeling and focusing on the issues of model development and specification, data considerations, estimation, and inference, was used to analyze the articles. The most common applications were two-level models where individuals were nested within contexts. Most studies were non-experimental and used nonprobability samples. The amount of data at each level varied widely across studies, as did the number of models examined. Analyses of reporting practices indicated some clear problems, with many articles not reporting enough information for a reader to critique the reported analyses. For example, in many articles, one could not determine how many models were estimated, what covariance structure was assumed, what type of centering if any was used, whether the data were consistent with assumptions, whether outliers were present, or how the models were estimated. Guidelines for researchers reporting multilevel analyses are provided.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\ZS7ZGDTR\\Dedrick et al. - 2009 - Multilevel Modeling A Review of Methodological Is.pdf}
}

@article{demetrashvili2016,
  title = {Confidence Intervals for Intraclass Correlation Coefficients in Variance Components Models},
  author = {Demetrashvili, Nino and Wit, Ernst C and {van den Heuvel}, Edwin R},
  year = {2016},
  month = oct,
  journal = {Statistical Methods in Medical Research},
  volume = {25},
  number = {5},
  pages = {2359--2376},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0962-2802},
  doi = {10.1177/0962280214522787},
  abstract = {Confidence intervals for intraclass correlation coefficients in agreement studies with continuous outcomes are model-specific and no generic approach exists. This paper provides two generic approaches for intraclass correlation coefficients of the form {$\sum$}Qq=1{$\sigma$}2q/({$\sum$}Qq=1{$\sigma$}2q+{$\sum$}Pp=Q+1{$\sigma$}2p){$\sum$}q=1Q{$\sigma$}q2/({$\sum$}q=1Q{$\sigma$}q2+{$\sum$}p=Q+1P{$\sigma$}p2){$<$}math display="inline" id="mml-math1-0962280214522787" overflow="scroll" altimg="eq-00001.gif"{$><$}mrow{$><$}munderover{$><$}mo{$>\sum<$}/mo{$><$}mrow{$><$}mi{$>$}q{$<$}/mi{$><$}mo{$>$}={$<$}/mo{$><$}mn{$>$}1{$<$}/mn{$><$}/mrow{$><$}mrow{$><$}mi{$>$}Q{$<$}/mi{$><$}/mrow{$><$}/munderover{$><$}msubsup{$><$}mrow{$><$}mi{$>\sigma<$}/mi{$><$}/mrow{$><$}mrow{$><$}mi{$>$}q{$<$}/mi{$><$}/mrow{$><$}mrow{$><$}mn{$>$}2{$<$}/mn{$><$}/mrow{$><$}/msubsup{$><$}mo{$>$}/{$<$}/mo{$><$}mo stretchy="false"{$>$}({$<$}/mo{$><$}munderover{$><$}mo{$>\sum<$}/mo{$><$}mrow{$><$}mi{$>$}q{$<$}/mi{$><$}mo{$>$}={$<$}/mo{$><$}mn{$>$}1{$<$}/mn{$><$}/mrow{$><$}mrow{$><$}mi{$>$}Q{$<$}/mi{$><$}/mrow{$><$}/munderover{$><$}msubsup{$><$}mrow{$><$}mi{$>\sigma<$}/mi{$><$}/mrow{$><$}mrow{$><$}mi{$>$}q{$<$}/mi{$><$}/mrow{$><$}mrow{$><$}mn{$>$}2{$<$}/mn{$><$}/mrow{$><$}/msubsup{$><$}mo{$>$}+{$<$}/mo{$><$}munderover{$><$}mo{$>\sum<$}/mo{$><$}mrow{$><$}mi{$>$}p{$<$}/mi{$><$}mo{$>$}={$<$}/mo{$><$}mi{$>$}Q{$<$}/mi{$><$}mo{$>$}+{$<$}/mo{$><$}mn{$>$}1{$<$}/mn{$><$}/mrow{$><$}mrow{$><$}mi{$>$}P{$<$}/mi{$><$}/mrow{$><$}/munderover{$><$}msubsup{$><$}mrow{$><$}mi{$>\sigma<$}/mi{$><$}/mrow{$><$}mrow{$><$}mi{$>$}p{$<$}/mi{$><$}/mrow{$><$}mrow{$><$}mn{$>$}2{$<$}/mn{$><$}/mrow{$><$}/msubsup{$><$}mo stretchy="false"{$>$}){$<$}/mo{$><$}/mrow{$><$}/math{$>$}. The first approach uses Satterthwaite's approximation and an F-distribution. The second approach uses the first and second moments of the intraclass correlation coefficient estimate in combination with a Beta distribution. Both approaches are based on the restricted maximum likelihood estimates for the variance components involved. Simulation studies are conducted to examine the coverage probabilities of the confidence intervals for agreement studies with a mix of small sample sizes. Two different three-way variance components models and balanced and unbalanced one-way random effects models are investigated. The proposed approaches are compared with other approaches developed for these specific models. The approach based on the F-distribution provides acceptable coverage probabilities, but the approach based on the Beta distribution results in accurate coverages for most settings in both balanced and unbalanced designs. A real agreement study is provided to illustrate the approaches.},
  langid = {english},
  keywords = {agreement study,ANOVA,Beta distribution,F-distribution,REML},
  file = {C\:\\Users\\micro\\Zotero\\storage\\TJLFDCWT\\Demetrashvili et al. - 2016 - Confidence intervals for intraclass correlation co.pdf}
}

@article{donner1980,
  title = {The {{Estimation}} of {{Intraclass Correlation}} in the {{Analysis}} of {{Family Data}}},
  author = {Donner, Allan and Koval, John J.},
  year = {1980},
  journal = {Biometrics},
  volume = {36},
  number = {1},
  pages = {19--25},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  doi = {10.2307/2530491},
  abstract = {The maximum likelihood estimator of the intraclass correlation coefficient {$\rho$} in samples of unequal size from a multivariate normal distribution has been derived and compared to several other estimators, using Monte Carlo simulation. It is recommended that maximum likelihood be used if no prior knowledge concerning the value of {$\rho$} exists, or if the value of {$\rho$} is thought to be high. For low to moderate values of {$\rho$} it is recommended that the analysis of variance estimator be used, with those families having only one member deleted from the analysis.}
}

@article{donner1980a,
  title = {The Large Sample Variance of an Intraclass Correlation},
  author = {Donner, Allan and Koval, John J.},
  year = {1980},
  month = jan,
  journal = {Biometrika},
  volume = {67},
  number = {3},
  pages = {719--722},
  issn = {0006-3444},
  doi = {10.1093/biomet/67.3.719},
  abstract = {A derivation is given of the large sample variance of the maximum likelihood estimator rM of the intraclass correlation coefficient p in samples of unequal size ni from a multivariate normal distribution.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\XUJ4UHS3\\Donner and Koval - 1980 - The large sample variance of an intraclass correla.pdf;C\:\\Users\\micro\\Zotero\\storage\\44FMXKU3\\628149.html}
}

@article{donner1986,
  title = {A {{Comparison}} of {{Confidence Interval Methods}} for the {{Intraclass Correlation Coefficient}}},
  author = {Donner, Allan and Wells, George},
  year = {1986},
  journal = {Biometrics},
  volume = {42},
  number = {2},
  pages = {401--412},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  doi = {10.2307/2531060},
  abstract = {Different methods of obtaining confidence intervals for the intraclass correlation coefficient {$\rho$} in the unbalanced one-way random-effects model are investigated, focusing on applications to family studies. Methods based on simple modifications of formulas for the case of equal group sizes are found to provide adequate coverage at small to moderate values of {$\rho$}. A method based on the large-sample standard error of the sample intraclass correlation, as derived by Smith (1956, Annals of Human Genetics 21, 363-373), is shown to provide consistently good coverage at all values of {$\rho$}. A method proposed by Thomas and Hultquist (1978, Annals of Statistics 6, 582-587) also provides consistently good coverage, but generates mean interval widths substantially greater than those generated by Smith's method at values of {$\rho$} likely to arise in practice.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\FS5RUSHX\\Donner and Wells - 1986 - A Comparison of Confidence Interval Methods for th.pdf}
}

@article{donner2002,
  title = {Issues in the Meta-Analysis of Cluster Randomized Trials},
  author = {Donner, Allan and Klar, Neil},
  year = {2002},
  journal = {Statistics in Medicine},
  volume = {21},
  number = {19},
  pages = {2971--2980},
  issn = {1097-0258},
  doi = {10.1002/sim.1301},
  abstract = {Meta-analyses involving the synthesis of evidence from cluster randomization trials are being increasingly reported. These analyses raise challenging methodologic issues beyond those raised by meta-analyses which include only individually randomized trials. In this paper we review and comment on a selected number of these issues, including problems of study heterogeneity, difficulties in estimating design effects from individual trials and the choice of statistical methods. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {clustering,community trials,correlated binary data,intraclass correlation},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.1301},
  file = {C\:\\Users\\micro\\Zotero\\storage\\HEJM9MKI\\Donner and Klar - 2002 - Issues in the meta-analysis of cluster randomized .pdf;C\:\\Users\\micro\\Zotero\\storage\\VQM8QYHC\\sim.html}
}

@article{donner2004,
  title = {Pitfalls of and {{Controversies}} in {{Cluster Randomization Trials}}},
  author = {Donner, Allan and Klar, Neil},
  year = {2004},
  month = mar,
  journal = {American Journal of Public Health},
  volume = {94},
  number = {3},
  pages = {416--422},
  publisher = {{American Public Health Association}},
  issn = {0090-0036},
  doi = {10.2105/AJPH.94.3.416},
  abstract = {It is now well known that standard statistical procedures become invalidated when applied to cluster randomized trials in which the unit of inference is the individual. A resulting consequence is that researchers conducting such trials are faced with a multitude of design choices, including selection of the primary unit of inference, the degree to which clusters should be matched or stratified by prognostic factors at baseline, and decisions related to cluster subsampling. Moreover, application of ethical principles developed for individually randomized trials may also require modification. We discuss several topics related to these issues, with emphasis on the choices that must be made in the planning stages of a trial and on some potential pitfalls to be avoided.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\GPCGWQUB\\Donner and Klar - 2004 - Pitfalls of and Controversies in Cluster Randomiza.pdf}
}

@article{donnerClusterRandomizationTrials2000,
  title = {Cluster Randomization Trials},
  author = {Donner, A. and Klar, N.},
  year = {2000},
  month = apr,
  journal = {Statistical Methods in Medical Research},
  volume = {9},
  number = {2},
  pages = {79--80},
  publisher = {{Sage Publications Ltd.}},
  address = {{London, United Kingdom}},
  issn = {09622802},
  doi = {http://dx.doi.org.ezproxy.lib.utexas.edu/10.1191/096228000669355658},
  copyright = {\textcopyright{} Arnold 2000},
  langid = {english},
  keywords = {Mathematics,Medical Sciences,Statistics},
  file = {C\:\\Users\\micro\\Zotero\\storage\\YYPSTKDD\\Donner and Klar - 2000 - Cluster randomization trials.pdf}
}

@article{donnerReviewInferenceProcedures1986a,
  title = {A {{Review}} of {{Inference Procedures}} for the {{Intraclass Correlation Coefficient}} in the {{One-Way Random Effects Model}}},
  author = {Donner, Allan},
  year = {1986},
  journal = {International Statistical Review / Revue Internationale de Statistique},
  volume = {54},
  number = {1},
  pages = {67--82},
  publisher = {{[Wiley, International Statistical Institute (ISI)]}},
  issn = {0306-7734},
  doi = {10.2307/1403259},
  abstract = {Recent theory and methodology for inferences concerning the intraclass correlation coefficient are reviewed, under the assumption of an underlying random effects model. Topics discussed include point and interval estimation, significance-testing for nonzero values of the intraclass correlation, and inference procedures in multiple samples. /// On pr\'esente une revue de la th\'eorie et de la m\'ethodologie r\'ecentes de l'inf\'erence en ce qui concerne le coefficient de corr\'elation intraclasse, en se basant sur l'hypoth\`ese d'un mod\`ele d'effets al\'eatoires. Les sujets discut\'es comprennent l'estimation ponctuelle et par intervalles, des tests significatifs pour des valeurs non-z\'ero de la corr\'elation intraclasse et des proc\'edures d'inf\'erence pour des \'echantillons multiples.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\E9TR3D72\\Donner - 1986 - A Review of Inference Procedures for the Intraclas.pdf}
}

@article{dreyhaupt2017,
  title = {Cluster-Randomized {{Studies}} in {{Educational Research}}: {{Principles}} and {{Methodological Aspects}}},
  shorttitle = {Cluster-Randomized {{Studies}} in {{Educational Research}}},
  author = {Dreyhaupt, Jens and Mayer, Benjamin and Keis, Oliver and {\"O}chsner, Wolfgang and Muche, Rainer},
  year = {2017},
  month = may,
  journal = {GMS Journal for Medical Education},
  volume = {34},
  number = {2},
  pages = {Doc26},
  issn = {2366-5017},
  doi = {10.3205/zma001103},
  abstract = {An increasing number of studies are being performed in educational research to evaluate new teaching methods and approaches. These studies could be performed more efficiently and deliver more convincing results if they more strictly applied and complied with recognized standards of scientific studies. Such an approach could substantially increase the quality in particular of prospective, two-arm (intervention) studies that aim to compare two different teaching methods. A key standard in such studies is randomization, which can minimize systematic bias in study findings; such bias may result if the two study arms are not structurally equivalent. If possible, educational research studies should also achieve this standard, although this is not yet generally the case. Some difficulties and concerns exist, particularly regarding organizational and methodological aspects. An important point to consider in educational research studies is that usually individuals cannot be randomized, because of the teaching situation, and instead whole groups have to be randomized (so-called ``cluster randomization''). Compared with studies with individual randomization, studies with cluster randomization normally require (significantly) larger sample sizes and more complex methods for calculating sample size. Furthermore, cluster-randomized studies require more complex methods for statistical analysis. The consequence of the above is that a competent expert with respective special knowledge needs to be involved in all phases of cluster-randomized studies., Studies to evaluate new teaching methods need to make greater use of randomization in order to achieve scientifically convincing results. Therefore, in this article we describe the general principles of cluster randomization and how to implement these principles, and we also outline practical aspects of using cluster randomization in prospective, two-arm comparative educational research studies.},
  pmcid = {PMC5450430},
  pmid = {28584874},
  file = {C\:\\Users\\micro\\Zotero\\storage\\4ZUVRN3N\\Dreyhaupt et al. - 2017 - Cluster-randomized Studies in Educational Research.pdf}
}

@techreport{dsginideco1,
  type = {Statistical Software Components},
  title = {: Decomposition of Inequality Change into pro-Poor Growth and Mobility Components},
  author = {Jenkins, Stephen P. and Van Kerm, Philippe},
  year = {2009},
  number = {S457009},
  institution = {{Boston College Department of Economics}}
}

@article{elalili2020,
  title = {Taking the {{Analysis}} of {{Trial-Based Economic Evaluations}} to the {{Next Level}}: {{The Importance}} of {{Accounting}} for {{Clustering}}},
  shorttitle = {Taking the {{Analysis}} of {{Trial-Based Economic Evaluations}} to the {{Next Level}}},
  author = {El Alili, Mohamed and {van Dongen}, Johanna M. and Goldfeld, Keith S. and Heymans, Martijn W. and {van Tulder}, Maurits W. and Bosmans, Judith E.},
  year = {2020},
  month = nov,
  journal = {PharmacoEconomics},
  volume = {38},
  number = {11},
  pages = {1247--1261},
  issn = {1179-2027},
  doi = {10.1007/s40273-020-00946-y},
  abstract = {The aim of this study was to assess the performance and impact of multilevel modelling (MLM) compared with ordinary least squares (OLS) regression in trial-based economic evaluations with clustered data.},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\DFHKWW97\\El Alili et al. - 2020 - Taking the Analysis of Trial-Based Economic Evalua.pdf}
}

@article{eldridge2006,
  title = {Sample Size for Cluster Randomized Trials: Effect of Coefficient of Variation of Cluster Size and Analysis Method},
  shorttitle = {Sample Size for Cluster Randomized Trials},
  author = {Eldridge, Sandra M and Ashby, Deborah and Kerry, Sally},
  year = {2006},
  month = oct,
  journal = {International Journal of Epidemiology},
  volume = {35},
  number = {5},
  pages = {1292--1300},
  issn = {0300-5771},
  doi = {10.1093/ije/dyl129},
  abstract = {Background  Cluster randomized trials are increasingly popular. In many of these trials, cluster sizes are unequal. This can affect trial power, but standard sample size formulae for these trials ignore this. Previous studies addressing this issue have mostly focused on continuous outcomes or methods that are sometimes difficult to use in practice.Methods We show how a simple formula can be used to judge the possible effect of unequal cluster sizes for various types of analyses and both continuous and binary outcomes. We explore the practical estimation of the coefficient of variation of cluster size required in this formula and demonstrate the formula's performance for a hypothetical but typical trial randomizing UK general practices.Results The simple formula provides a good estimate of sample size requirements for trials analysed using cluster-level analyses weighting by cluster size and a conservative estimate for other types of analyses. For trials randomizing UK general practices the coefficient of variation of cluster size depends on variation in practice list size, variation in incidence or prevalence of the medical condition under examination, and practice and patient recruitment strategies, and for many trials is expected to be {$\sim$}0.65. Individual-level analyses can be noticeably more efficient than some cluster-level analyses in this context.Conclusions When the coefficient of variation is \&lt;0.23, the effect of adjustment for variable cluster size on sample size is negligible. Most trials randomizing UK general practices and many other cluster randomized trials should account for variable cluster size in their sample size calculations.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\LPXDLVKG\\Eldridge et al. - 2006 - Sample size for cluster randomized trials effect .pdf;C\:\\Users\\micro\\Zotero\\storage\\JTG2JFCZ\\762170.html}
}

@article{eldridgeIntraClusterCorrelationCoefficient2009,
  title = {The {{Intra-Cluster Correlation Coefficient}} in {{Cluster Randomized Trials}}: {{A Review}} of {{Definitions}}},
  shorttitle = {The {{Intra-Cluster Correlation Coefficient}} in {{Cluster Randomized Trials}}},
  author = {Eldridge, Sandra M. and Ukoumunne, Obioha C. and Carlin, John B.},
  year = {2009},
  month = dec,
  journal = {International Statistical Review},
  volume = {77},
  number = {3},
  pages = {378--394},
  publisher = {{Wiley-Blackwell}},
  issn = {03067734},
  doi = {10.1111/j.1751-5823.2009.00092.x},
  abstract = {The intra-cluster correlation coefficient (ICC) of the primary outcome plays a key role in the design and analysis of cluster randomized trials (CRTs), but the precise definition of this parameter is somewhat elusive, especially in the context of non-normally distributed outcomes. In this paper, we provide a unified treatment of ICC as used in CRTs. We present a general definition of the ICC that may be expressed in different ways depending on the modelling approach used to describe the data, illustrating how this general definition is applied to continuous and dichotomous outcomes. Greater complexity arises for dichotomous outcomes; in particular, the usual definition of the ICC cannot be related directly to the parameters of the logistic-normal model that is commonly used for dichotomous outcomes. We show how the definition of the ICC is different when covariates are introduced. Finally, we use our framework and definition of the ICC to draw out implications for those interpreting and choosing values of the ICC when planning CRTs. (English)},
  keywords = {ANALYSIS of variance,CLUSTER analysis (Statistics),cluster randomized trials,GENETICS,Intra-cluster correlation coefficient,LOGISTIC model (Demography),PROBABILITY theory},
  file = {C\:\\Users\\micro\\Zotero\\storage\\APBGKPD9\\Eldridge et al. - 2009 - The Intra-Cluster Correlation Coefficient in Clust.pdf}
}

@article{evans2001,
  title = {A Comparison of Generalized Linear Mixed Model Procedures with Estimating Equations for Variance and Covariance Parameter Estimation in Longitudinal Studies and Group Randomized Trials},
  author = {Evans, Brent A. and Feng, Ziding and Peterson, Arthur V.},
  year = {2001},
  journal = {Statistics in Medicine},
  volume = {20},
  number = {22},
  pages = {3353--3373},
  issn = {1097-0258},
  doi = {10.1002/sim.991},
  abstract = {Response data in longitudinal studies and group randomized trials are gathered on units that belong to clusters, within which data are usually positively correlated. Therefore, estimates and confidence intervals for intraclass correlation or variance components are helpful when designing a longitudinal study or group randomized trial. Data simulated from both study designs are used to investigate the estimation of variance and covariance parameters from the following procedures: for continuous outcomes, restricted maximum likelihood (REML) and estimating equations (EE); for binary outcomes, restricted pseudo-likelihood (REPL) and estimating equations (EE). We evaluate these procedures to see which provide valid and precise estimates as well as correct standard errors for the intraclass correlation coefficient or variance components. REML seems the better choice for estimating terms related to correlation for models with normal outcomes, especially in group randomized trial situations. Results for REML and EE are mixed when outcomes are continuous and non-normal. With binary outcomes neither REPL nor EE provides satisfactory estimation or inference in longitudinal study situations, while REPL is preferable for group randomized trials. Copyright \textcopyright{} 2001 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2001 John Wiley \& Sons, Ltd.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.991},
  file = {C\:\\Users\\micro\\Zotero\\storage\\2AQXH9KN\\Evans et al. - 2001 - A comparison of generalized linear mixed model pro.pdf;C\:\\Users\\micro\\Zotero\\storage\\GVXJ3JHA\\sim.html}
}

@article{feng1996,
  title = {A {{Comparison}} of {{Statistical Methods}} for {{Clustered Data Analysis}} with {{Gaussian Error}}},
  author = {Feng, Ziding and McLERRAN, Dale and Grizzle, James},
  year = {1996},
  journal = {Statistics in Medicine},
  volume = {15},
  number = {16},
  pages = {1793--1806},
  issn = {1097-0258},
  doi = {10.1002/(SICI)1097-0258(19960830)15:16<1793::AID-SIM332>3.0.CO;2-2},
  abstract = {We investigate by simulation the properties of four different estimation procedures under a linear model for correlated data with Gaussian error: maximum likelihood based on the normal mixed linear model; generalized estimating equations; a four-stage method, and a bootstrap method that resamples clusters rather than individuals. We pay special attention to the group randomized trials where the number of independent clusters is small, cluster sizes are big, and the correlation within the cluster is weak. We show that for balanced and near balanced data when the number of independent clusters is small ({$\leqslant$}10), the bootstrap is superior if analysts do not want to impose strong distribution and covariance structure assumptions. Otherwise, ML and four-stage methods are slightly better. All four methods perform well when the number of independent clusters reaches 50.},
  copyright = {Copyright \textcopyright{} 1996 John Wiley \& Sons, Ltd.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291097-0258\%2819960830\%2915\%3A16\%3C1793\%3A\%3AAID-SIM332\%3E3.0.CO\%3B2-2},
  file = {C\:\\Users\\micro\\Zotero\\storage\\3BKITEAT\\Feng et al. - 1996 - A Comparison of Statistical Methods for Clustered .pdf;C\:\\Users\\micro\\Zotero\\storage\\FUHJX3JT\\(SICI)1097-0258(19960830)15161793AID-SIM3323.0.html}
}

@article{feng2001,
  title = {Selected {{Statistical Issues}} in {{Group Randomized Trials}}},
  author = {Feng, Ziding and Diehr, Paula and Peterson, Arthur and McLerran, Dale},
  year = {2001},
  journal = {Annual Review of Public Health},
  volume = {22},
  number = {1},
  pages = {167--187},
  doi = {10.1146/annurev.publhealth.22.1.167},
  abstract = {Group randomized trials (GRTs) in public health research typically use a small number of randomized groups with a relatively large number of participants per group. Two fundamental features characterize GRTs: a positive correlation of outcomes within a group, and the small number of groups. Appropriate consideration of these fundamental features is essential for design and analysis. This paper presents the fundamental features of GRTs and the importance of considering these features in design and analysis. It also reviews and contrasts the main analytic methods proposed for GRTs, emphasizing the assumptions required to make these methods valid and efficient. Also discussed are various design issues, along with guidelines for choosing among them. A real data example illustrates these issues and methods.},
  pmid = {11274517},
  annotation = {\_eprint: https://doi.org/10.1146/annurev.publhealth.22.1.167},
  file = {C\:\\Users\\micro\\Zotero\\storage\\BJKCWXYZ\\Feng et al. - 2001 - Selected Statistical Issues in Group Randomized Tr.pdf}
}

@incollection{feng2004,
  title = {Small {{Sample Inference}} for {{Clustered Data}}},
  booktitle = {Proceedings of the {{Second Seattle Symposium}} in {{Biostatistics}}: {{Analysis}} of {{Correlated Data}}},
  author = {Feng, Ziding and Braun, Thomas and McCulloch, Charles},
  editor = {Lin, D. Y. and Heagerty, P. J.},
  year = {2004},
  series = {Lecture {{Notes}} in {{Statistics}}},
  pages = {71--87},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4419-9076-1_5},
  abstract = {When the number of independent units is not adequate to invoke large sample approximations in clustered data analysis, a situation that often arises in group randomized trials (GRTs), valid and efficient small sample inference becomes important. We review the current methods for analyzing data from small numbers of clusters, namely methods based on full distribution assumptions (mixed effect models), semi-parametric methods based on Generalized Estimating Equations (GEE), and non-parametric methods based on permutation tests.},
  isbn = {978-1-4419-9076-1},
  langid = {english},
  keywords = {Correlated data,Generalized Estimating Equations (GEE),group randomized trials,linear mixed models,permutation tests,small sample inference}
}

@article{fisher1970statistical,
  title = {Statistical Methods for Research Workers, {{Hafner}}},
  author = {Fisher, RA},
  year = {1970},
  journal = {Darien, CT}
}

@manual{fisher2017a,
  type = {Manual},
  title = {Robumeta: {{Robust}} Variance Meta-Regression},
  author = {Fisher, Zachary and Tipton, Elizabeth and Zhipeng, Hou},
  year = {2017}
}

@article{fisherTheoryStatisticalEstimation1925,
  title = {Theory of {{Statistical Estimation}}},
  author = {Fisher, R. A.},
  year = {1925},
  month = jul,
  journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
  volume = {22},
  number = {5},
  pages = {700--725},
  publisher = {{Cambridge University Press}},
  issn = {1469-8064, 0305-0041},
  doi = {10.1017/S0305004100009580},
  abstract = {It has been pointed out to me that some of the statistical ideas employed in the following investigation have never received a strictly logical definition and analysis. The idea of a frequency curve, for example, evidently implies an infinite hypothetical population distributed in a definite manner; but equally evidently the idea of an infinite hypothetical population requires a more precise logical specification than is contained in that phrase. The same may be said of the intimately connected idea of random sampling. These ideas have grown up in the minds of practical statisticians and lie at the basis especially of recent work; there can be no question of their pragmatic value. It was no part of my original intention to deal with the logical bases of these ideas, but some comments which Dr Burnside has kindly made have convinced me that it may be desirable to set out for criticism the manner in which I believe the logical foundations of these ideas may be established.},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\Q8LDPRMA\\Fisher - 1925 - Theory of Statistical Estimation.pdf;C\:\\Users\\micro\\Zotero\\storage\\4DKWMRR7\\7A05FB68C83B36C0E91D42C76AB177D4.html}
}

@article{flynn2001,
  title = {Design and {{Analysis}} of {{Cluster Randomization Trials}} in {{Health Research}}.: {{Allan Donner}} and {{Neil Klar}}. {{London}}: {{Arnold}}, 2000, Pp.178, \textsterling 35.00. {{ISBN}}: 0-340-69153-0.},
  shorttitle = {Design and {{Analysis}} of {{Cluster Randomization Trials}} in {{Health Research}}.},
  author = {Flynn, Terry N},
  year = {2001},
  month = apr,
  journal = {International Journal of Epidemiology},
  volume = {30},
  number = {2},
  pages = {407--408},
  issn = {0300-5771},
  doi = {10.1093/ije/30.2.407-a},
  abstract = {Randomizing individuals to treatments is not always feasible and cluster randomized trials are increasingly being utilized in the evaluation of health care interventions. However, standard textbooks on clinical trial methodology rarely explore the statistical, practical and ethical concerns raised by cluster randomization. Furthermore, research into these issues is not confined to the biostatistics literature but is spread across several fields. The result has been poor standards of design, analysis and reporting, which this book seeks to rectify by providing researchers with a unified account of the issues particular to cluster randomized trials.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\HNCDE7YY\\Flynn - 2001 - Design and Analysis of Cluster Randomization Trial.pdf;C\:\\Users\\micro\\Zotero\\storage\\5E8AY3UT\\713812.html}
}

@article{freemanActiveLearningIncreases2014,
  title = {Active Learning Increases Student Performance in Science, Engineering, and Mathematics},
  author = {Freeman, Scott and Eddy, Sarah L. and McDonough, Miles and Smith, Michelle K. and Okoroafor, Nnadozie and Jordt, Hannah and Wenderoth, Mary Pat},
  year = {2014},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {111},
  number = {23},
  pages = {8410--8415},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1319030111},
  abstract = {To test the hypothesis that lecturing maximizes learning and course performance, we metaanalyzed 225 studies that reported data on examination scores or failure rates when comparing student performance in undergraduate science, technology, engineering, and mathematics (STEM) courses under traditional lecturing versus active learning. The effect sizes indicate that on average, student performance on examinations and concept inventories increased by 0.47 SDs under active learning (n = 158 studies), and that the odds ratio for failing was 1.95 under traditional lecturing (n = 67 studies). These results indicate that average examination scores improved by about 6\% in active learning sections, and that students in classes with traditional lecturing were 1.5 times more likely to fail than were students in classes with active learning. Heterogeneity analyses indicated that both results hold across the STEM disciplines, that active learning increases scores on concept inventories more than on course examinations, and that active learning appears effective across all class sizes\textemdash although the greatest effects are in small (n {$\leq$} 50) classes. Trim and fill analyses and fail-safe n calculations suggest that the results are not due to publication bias. The results also appear robust to variation in the methodological rigor of the included studies, based on the quality of controls over student quality and instructor identity. This is the largest and most comprehensive metaanalysis of undergraduate STEM education published to date. The results raise questions about the continued use of traditional lecturing as a control in research studies, and support active learning as the preferred, empirically validated teaching practice in regular classrooms.},
  chapter = {Social Sciences},
  copyright = {\textcopyright{}  . Freely available online through the PNAS open access option.},
  langid = {english},
  pmid = {24821756},
  keywords = {constructivism,evidence-based teaching,scientific teaching,undergraduate education},
  file = {C\:\\Users\\micro\\Zotero\\storage\\A8P5YY3G\\Freeman et al. - 2014 - Active learning increases student performance in s.pdf;C\:\\Users\\micro\\Zotero\\storage\\EBBVRE7L\\8410.html}
}

@article{giraudeau2006,
  title = {Model Mis-Specification and Overestimation of the Intraclass Correlation Coefficient in Cluster Randomized Trials},
  author = {Giraudeau, Bruno},
  year = {2006},
  journal = {Statistics in Medicine},
  volume = {25},
  number = {6},
  pages = {957--964},
  issn = {1097-0258},
  doi = {10.1002/sim.2260},
  abstract = {Intraclass correlation coefficient (ICC) estimates must be provided when reporting the results of a cluster randomized trial. This study demonstrates that estimating this parameter with one-way ANOVA and an underlying mixed-effects statistical model leads to biased estimates. The bias depends on the effect size of the studied treatment. Copyright \textcopyright{} 2005 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {bias,cluster randomized trial,intraclass correlation coefficient,mixed-effects model,one-way random effects model},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.2260},
  file = {C\:\\Users\\micro\\Zotero\\storage\\G5BS2PA4\\Giraudeau - 2006 - Model mis-specification and overestimation of the .pdf;C\:\\Users\\micro\\Zotero\\storage\\JFP25YGP\\sim.html}
}

@article{glassPrimarySecondaryMetaAnalysis1976,
  title = {Primary, {{Secondary}}, and {{Meta-Analysis}} of {{Research}}},
  author = {GLASS, GENE V},
  year = {1976},
  month = nov,
  journal = {Educational Researcher},
  volume = {5},
  number = {10},
  pages = {3--8},
  publisher = {{American Educational Research Association}},
  issn = {0013-189X},
  doi = {10.3102/0013189X005010003},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\KAB6IFSB\\GLASS - 1976 - Primary, Secondary, and Meta-Analysis of Research.pdf}
}

@article{grahamEffectsWritingLearning2020,
  title = {The {{Effects}} of {{Writing}} on {{Learning}} in {{Science}}, {{Social Studies}}, and {{Mathematics}}: {{A Meta-Analysis}}},
  shorttitle = {The {{Effects}} of {{Writing}} on {{Learning}} in {{Science}}, {{Social Studies}}, and {{Mathematics}}},
  author = {Graham, Steve and Kiuhara, Sharlene A. and MacKay, Meade},
  year = {2020},
  month = apr,
  journal = {Review of Educational Research},
  volume = {90},
  number = {2},
  pages = {179--226},
  publisher = {{American Educational Research Association}},
  issn = {0034-6543},
  doi = {10.3102/0034654320914744},
  abstract = {This meta-analysis examined if students writing about content material in science, social studies, and mathematics facilitated learning (k = 56 experiments). Studies in this review were true or quasi-experiments (with pretests), written in English, and conducted with students in Grades 1 to 12 in which the writing-to-learn activity was part of instruction. Studies were not included if the control condition used writing to support learning (except when treatment students spent more time engaging in writing-to-learn activities), study attrition exceeded 20\%, instructional time and content coverage differed between treatment and control conditions, pretest scores approached ceiling levels, letter grades were the learning outcome, and students attended a special school for students with disabilities. As predicted, writing about content reliably enhanced learning (effect size = 0.30). It was equally effective at improving learning in science, social studies, and mathematics as well as the learning of elementary, middle, and high school students. Writing-to-learn effects were not moderated by the features of writing activities, instruction, or assessment. Furthermore, variability in obtained effects were not related to features of study quality. Directions for future research and implications for practice are provided.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\CYRM97F2\\Graham et al. - 2020 - The Effects of Writing on Learning in Science, Soc.pdf}
}

@article{guittet2006,
  title = {Planning a Cluster Randomized Trial with Unequal Cluster Sizes: Practical Issues Involving Continuous Outcomes},
  shorttitle = {Planning a Cluster Randomized Trial with Unequal Cluster Sizes},
  author = {Guittet, Lydia and Ravaud, Philippe and Giraudeau, Bruno},
  year = {2006},
  month = dec,
  journal = {BMC Medical Research Methodology},
  volume = {6},
  number = {1},
  pages = {17},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-6-17},
  abstract = {Background: Cluster randomization design is increasingly used for the evaluation of health-care, screeening or educational interventions. At the planning stage, sample size calculations usually consider an average cluster size without taking into account any potential imbalance in cluster size. However, there may exist high discrepancies in cluster sizes. Methods: We performed simulations to study the impact of an imbalance in cluster size on power. We determined by simulations to which extent four methods proposed to adapt the sample size calculations to a pre-specified imbalance in cluster size could lead to adequately powered trials. Results: We showed that an imbalance in cluster size can be of high influence on the power in the case of severe imbalance, particularly if the number of clusters is low and/or the intraclass correlation coefficient is high. In the case of a severe imbalance, our simulations confirmed that the minimum variance weights correction of the variation inflaction factor (VIF) used in the sample size calculations has the best properties. Conclusion: Publication of cluster sizes is important to assess the real power of the trial which was conducted and to help designing future trials. We derived an adaptation of the VIF from the minimum variance weights correction to be used in case the imbalance can be a priori formulated such as "a proportion ({$\gamma$}) of clusters actually recruit a proportion ({$\tau$}) of subjects to be included ({$\gamma$} {$\leq$} {$\tau$})".},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\DC6NMD9U\\Guittet et al. - 2006 - Planning a cluster randomized trial with unequal c.pdf}
}

@article{harville1977maximum,
  title = {Maximum Likelihood Approaches to Variance Component Estimation and to Related Problems},
  author = {Harville, David A},
  year = {1977},
  journal = {Journal of the American statistical association},
  volume = {72},
  number = {358},
  pages = {320--338},
  publisher = {{Taylor \& Francis}}
}

@article{hedbergReferenceValuesWithinDistrict2014,
  title = {Reference {{Values}} of {{Within-District Intraclass Correlations}} of {{Academic Achievement}} by {{District Characteristics}}: {{Results From}} a {{Meta-Analysis}} of {{District-Specific Values}}},
  shorttitle = {Reference {{Values}} of {{Within-District Intraclass Correlations}} of {{Academic Achievement}} by {{District Characteristics}}},
  author = {Hedberg, E. C. and Hedges, Larry V.},
  year = {2014},
  month = dec,
  journal = {Evaluation Review},
  volume = {38},
  number = {6},
  pages = {546--582},
  publisher = {{SAGE Publications Inc}},
  issn = {0193-841X},
  doi = {10.1177/0193841X14554212},
  abstract = {Background:Randomized experiments are often considered the strongest designs to study the impact of educational interventions. Perhaps the most prevalent class of designs used in large-scale education experiments is the cluster randomized design in which entire schools are assigned to treatments. In cluster randomized trials that assign schools to treatments within a set of school districts, the statistical power of the test for treatment effects depends on the within-district school-level intraclass correlation (ICC). Hedges and Hedberg (2014) recently computed within-district ICC values in 11 states using three-level models (students in schools in districts) that pooled results across all the districts within each state. Although values from these analyses are useful when working with a representative sample of districts, they may be misleading for other samples of districts because the magnitude of ICCs appears to be related to district size. To plan studies with small or nonrepresentative samples of districts, better information are needed about the relation of within-district school-level ICCs to district size.Objective:Our objective is to explore the relation between district size and within-district ICCs to provide reference values for math and reading achievement for Grades 3?8 by district size, poverty level, and urbanicity level. These values are not derived from pooling across all districts within a state as in previous work but are based on the direct calculation of within-district school-level ICCs for each school district.Research Design:We use mixed models to estimate over 7,000 district-specific ICCs for math and reading achievement in 11 states and for Grades 3?8. We then perform a random effects meta-analysis on the estimated within-district ICCs. Our analysis is performed by grade and subject for different strata designated by district size (number of schools), urbanicity, and poverty rates.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\NFZN9A8J\\Hedberg and Hedges - 2014 - Reference Values of Within-District Intraclass Cor.pdf}
}

@article{hedges,
  title = {Power {{Analysis}} in {{Education Research}}},
  author = {Hedges, Larry V and Rhoads, Christopher},
  pages = {88},
  abstract = {This paper provides a guide to calculating statistical power for the complex multilevel designs that are used in most field studies in education research. For multilevel evaluation studies in the field of education, it is important to account for the impact of clustering on the standard errors of estimates of treatment effects. Using ideas from survey research, the paper explains how sample design induces random variation in the quantities observed in a randomized experiment, and how this random variation relates to statistical power. The manner in which statistical power depends upon the values of intraclass correlations, sample sizes at the various levels, the standardized average treatment effect (effect size), the multiple correlation between covariates and the outcome at different levels, and the heterogeneity of treatment effects across sampling units is illustrated. Both hierarchical and randomized block designs are considered. The paper demonstrates that statistical power in complex designs involving clustered sampling can be computed simply from standard power tables using the idea of operational effect sizes: effect sizes multiplied by a design effect that depends on features of the complex experimental design. These concepts are applied to provide methods for computing power for each of the research designs most frequently used in education research.},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\76I7P7YB\\Hedges and Rhoads - Power Analysis in Education Research.pdf}
}

@inproceedings{hedges2009,
  title = {Power {{Analysis}} in {{Education Research}}},
  booktitle = {National {{Center}} for {{Special Education Research}}},
  author = {Hedges, Larry V and Rhoads, Christopher},
  year = {2009},
  pages = {3006},
  publisher = {{U.S. Department of Education}},
  address = {{Washington, DC:}},
  abstract = {This paper provides a guide to calculating statistical power for the complex multilevel designs that are used in most field studies in education research. For multilevel evaluation studies in the field of education, it is important to account for the impact of clustering on the standard errors of estimates of treatment effects. Using ideas from survey research, the paper explains how sample design induces random variation in the quantities observed in a randomized experiment, and how this random variation relates to statistical power. The manner in which statistical power depends upon the values of intraclass correlations, sample sizes at the various levels, the standardized average treatment effect (effect size), the multiple correlation between covariates and the outcome at different levels, and the heterogeneity of treatment effects across sampling units is illustrated. Both hierarchical and randomized block designs are considered. The paper demonstrates that statistical power in complex designs involving clustered sampling can be computed simply from standard power tables using the idea of operational effect sizes: effect sizes multiplied by a design effect that depends on features of the complex experimental design. These concepts are applied to provide methods for computing power for each of the research designs most frequently used in education research.},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\ZBYHNKQG\\Hedges and Rhoads - Power Analysis in Education Research.pdf}
}

@article{hedges2010,
  title = {Robust Variance Estimation in Meta-Regression with Dependent Effect Size Estimates},
  author = {Hedges, Larry V. and Tipton, Elizabeth and Johnson, Matthew C.},
  year = {2010},
  journal = {Research Synthesis Methods},
  volume = {1},
  number = {1},
  pages = {39--65},
  issn = {1759-2887},
  doi = {10.1002/jrsm.5},
  abstract = {Conventional meta-analytic techniques rely on the assumption that effect size estimates from different studies are independent and have sampling distributions with known conditional variances. The independence assumption is violated when studies produce several estimates based on the same individuals or there are clusters of studies that are not independent (such as those carried out by the same investigator or laboratory). This paper provides an estimator of the covariance matrix of meta-regression coefficients that are applicable when there are clusters of internally correlated estimates. It makes no assumptions about the specific form of the sampling distributions of the effect sizes, nor does it require knowledge of the covariance structure of the dependent estimates. Moreover, this paper demonstrates that the meta-regression coefficients are consistent and asymptotically normally distributed and that the robust variance estimator is valid even when the covariates are random. The theory is asymptotic in the number of studies, but simulations suggest that the theory may yield accurate results with as few as 20\textendash 40 studies. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {dependent effects,meta analysis,meta regression,robust standard errors},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.5},
  file = {C\:\\Users\\micro\\Zotero\\storage\\V7IGQGRN\\Hedges et al. - 2010 - Robust variance estimation in meta-regression with.pdf;C\:\\Users\\micro\\Zotero\\storage\\5PVH84QC\\jrsm.html}
}

@article{hedges2013,
  title = {Intraclass {{Correlations}} and {{Covariate Outcome Correlations}} for {{Planning Two-}} and {{Three-Level Cluster-Randomized Experiments}} in {{Education}}},
  author = {Hedges, Larry V. and Hedberg, E. C.},
  year = {2013},
  month = dec,
  journal = {Evaluation Review},
  volume = {37},
  number = {6},
  pages = {445--489},
  publisher = {{SAGE Publications Inc}},
  issn = {0193-841X},
  doi = {10.1177/0193841X14529126},
  abstract = {Background:Cluster-randomized experiments that assign intact groups such as schools or school districts to treatment conditions are increasingly common in educational research. Such experiments are inherently multilevel designs whose sensitivity (statistical power and precision of estimates) depends on the variance decomposition across levels. This variance decomposition is usually summarized by the intraclass correlation (ICC) structure and, if covariates are used, the effectiveness of the covariates in explaining variation at each level of the design.Objectives:This article provides a compilation of school- and district-level ICC values of academic achievement and related covariate effectiveness based on state longitudinal data systems. These values are designed to be used for planning group-randomized experiments in education. The use of these values to compute statistical power and plan two- and three-level group-randomized experiments is illustrated.Research Design:We fit several hierarchical linear models to state data by grade and subject to estimate ICCs and covariate effectiveness. The total sample size is over 4.8 million students. We then compare our average of state estimates with the national work by Hedges and Hedberg.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\9L2HX6QH\\Hedges and Hedberg - 2013 - Intraclass Correlations and Covariate Outcome Corr.pdf}
}

@article{hedgesEffectSizesClusterRandomized2007,
  title = {Effect {{Sizes}} in {{Cluster-Randomized Designs}}},
  author = {Hedges, Larry V.},
  year = {2007},
  month = dec,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {32},
  number = {4},
  pages = {341--370},
  publisher = {{American Educational Research Association}},
  issn = {1076-9986},
  doi = {10.3102/1076998606298043},
  abstract = {Multisite research designs involving cluster randomization are becoming increasingly important in educational and behavioral research. Researchers would like to compute effect size indexes based on the standardized mean difference to compare the results of cluster-randomized studies (and corresponding quasi-experiments) with other studies and to combine information across studies in meta-analyses. This article addresses the problem of defining effect sizes in multilevel designs and computing estimates of those effect sizes and their standard errors from information that is likely to be reported in journal articles. Three effect sizes are defined corresponding to different standardizations. Estimators of each effect size index are also presented along with their sampling distributions (including standard errors).},
  langid = {english},
  keywords = {cluster-randomized trials,effect size,group-randomized trials,meta-analysis,multilevel experiments},
  file = {C\:\\Users\\micro\\Zotero\\storage\\97YRP72D\\Hedges - 2007 - Effect Sizes in Cluster-Randomized Designs.pdf}
}

@article{hedgesEffectSizesThreeLevel2011,
  title = {Effect {{Sizes}} in {{Three-Level Cluster-Randomized Experiments}}},
  author = {Hedges, Larry V.},
  year = {2011},
  month = jun,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {36},
  number = {3},
  pages = {346--380},
  publisher = {{American Educational Research Association}},
  issn = {1076-9986},
  doi = {10.3102/1076998610376617},
  abstract = {Research designs involving cluster randomization are becoming increasingly important in educational and behavioral research. Many of these designs involve two levels of clustering or nesting (students within classes and classes within schools). Researchers would like to compute effect size indexes based on the standardized mean difference to compare the results of cluster-randomized studies with other studies and to combine information across studies in meta-analyses. This article addresses the problem of defining effect sizes in designs with two levels of clustering and computing estimates of those effect sizes and their standard errors from information that is likely to be reported in journal articles. Five effect sizes are defined corresponding to different standardizations. Estimators of each effect size index are also presented along with their sampling distributions (including standard errors).},
  langid = {english},
  keywords = {cluster randomized trials,effect size,multilevel experiments,randomized trials,social experiments},
  file = {C\:\\Users\\micro\\Zotero\\storage\\JSRN9SAQ\\Hedges - 2011 - Effect Sizes in Three-Level Cluster-Randomized Exp.pdf}
}

@article{hedgesEstimationEffectSize1982,
  title = {Estimation of Effect Size from a Series of Independent Experiments},
  author = {Hedges, Larry V.},
  year = {1982},
  month = sep,
  journal = {Psychological Bulletin},
  volume = {92},
  number = {2},
  pages = {490--499},
  publisher = {{American Psychological Association}},
  issn = {0033-2909},
  doi = {10.1037/0033-2909.92.2.490},
  abstract = {Extends statistical theory for procedures based on the Glass estimator of effect size for methods used in the quantitative synthesis of research. An unbiased estimator of effect size is given. A weighted estimator of effect size based on data from several experiments is defined and shown to be optimal (asymptotically efficient). An approximate (large-sample) test for homogeneity of effect size across experiments is also given. The results of an empirical sampling study show that the large-sample distributions of the weighted estimator and the homogeneity statistic are quite accurate when the experimental and control group sample sizes exceed 10 and the effect sizes are smaller than about 1.5. (12 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Estimation,estimation of effect sizes for 2 or more independent studies,Experimentation,Statistical Significance},
  file = {C\:\\Users\\micro\\Zotero\\storage\\PR7N54AE\\Hedges - 1982 - Estimation of effect size from a series of indepen.pdf}
}

@article{hedgesIntraclassCorrelationValues2007a,
  title = {Intraclass {{Correlation Values}} for {{Planning Group-Randomized Trials}} in {{Education}}},
  author = {Hedges, Larry V. and Hedberg, E. C.},
  year = {2007},
  month = mar,
  journal = {Educational Evaluation and Policy Analysis},
  volume = {29},
  number = {1},
  pages = {60--87},
  publisher = {{American Educational Research Association}},
  issn = {0162-3737},
  doi = {10.3102/0162373707299706},
  abstract = {Experiments that assign intact groups to treatment conditions are increasingly common in social research. In educational research, the groups assigned are often schools. The design of group-randomized experiments requires knowledge of the intraclass correlation structure to compute statistical power and sample sizes required to achieve adequate power. This article provides a compilation of intraclass correlation values of academic achievement and related covariate effects that could be used for planning group-randomized experiments in education. It also provides variance component information that is useful in planning experiments involving covariates. The use of these values to compute the statistical power of group-randomized experiments is illustrated.},
  langid = {english},
  keywords = {cluster randomized trials,experiments,intraclass correlation,statistical power},
  file = {C\:\\Users\\micro\\Zotero\\storage\\A6JLSJY4\\Hedges and Hedberg - 2007 - Intraclass Correlation Values for Planning Group-R.pdf}
}

@incollection{hedgesStatisticalMethodsMetaAnalysis1985,
  title = {Statistical {{Methods}} in {{Meta-Analysis}}},
  booktitle = {Stat {{Med}}},
  author = {Hedges, Larry and Olkin, Ingram},
  year = {1985},
  month = jan,
  volume = {20},
  doi = {10.2307/1164953},
  file = {C\:\\Users\\micro\\Zotero\\storage\\T7RJIH8W\\Hedges and Olkin - 1985 - Statistical Methods in Meta-Analysis.pdf}
}

@article{hedgesVarianceIntraclassCorrelations2012,
  title = {The {{Variance}} of {{Intraclass Correlations}} in {{Three-}} and {{Four-Level Models}}},
  author = {Hedges, Larry V. and Hedberg, E. C. and Kuyper, Arend M.},
  year = {2012},
  month = dec,
  journal = {Educational and Psychological Measurement},
  volume = {72},
  number = {6},
  pages = {893--909},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10.1177/0013164412445193},
  abstract = {Intraclass correlations are used to summarize the variance decomposition in populations with multilevel hierarchical structure. There has recently been considerable interest in estimating intraclass correlations from surveys or designed experiments to provide design parameters for planning future large-scale randomized experiments. The large sample distribution of estimates of the intraclass correlation in two-level situations is well known. Contemporary educational and social experiments often involve sampling designs with three or four levels of nesting, leading to three- and four-level intraclass correlation structures. The present article provides expressions for the large sample variances and covariances of estimates of three- and four-level intraclass correlation structures.},
  langid = {english},
  keywords = {intraclass correlation,multilevel models,standard errors},
  file = {C\:\\Users\\micro\\Zotero\\storage\\F3IHVI2G\\Hedges et al. - 2012 - The Variance of Intraclass Correlations in Three- .pdf}
}

@article{hedgesWhatAreEffect2008,
  title = {What {{Are Effect Sizes}} and {{Why Do We Need Them}}?},
  author = {Hedges, Larry V},
  year = {2008},
  month = dec,
  journal = {Child Development Perspectives},
  volume = {2},
  number = {3},
  pages = {167--171},
  publisher = {{Wiley-Blackwell}},
  issn = {17508592},
  doi = {10.1111/j.1750-8606.2008.00060.x},
  abstract = {Effect sizes are quantitative indexes of the relations between variables found in research studies. They can provide a broadly understandable summary of research findings that can be used to compare different studies or summarize results across studies. Unlike statistical significance (p values), effect sizes represent strength of relationships without regard to sample size. Three families of effect sizes are widely used: the standardized mean difference family, the standardized regression coefficient family, and the odds ratio family.},
  keywords = {effect size,EFFECT sizes (Statistics),MATHEMATICAL variables,meta-analysis,meta‐analysis,p values,RATIO \& proportion,REGRESSION analysis,RESEARCH,SAMPLE size (Statistics),statistical significance,statistics,STATISTICS},
  file = {C\:\\Users\\micro\\Zotero\\storage\\VXYMGEZM\\Hedges - 2008 - What Are Effect Sizes and Why Do We Need Them.pdf}
}

@incollection{HierarchicalModelsBayesian1992,
  title = {Hierarchical {{Models}} and {{Bayesian Estimation}}},
  booktitle = {Variance {{Components}}},
  year = {1992},
  pages = {315--366},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470316856.ch9},
  abstract = {The prelims comprise: Basic principles Variance estimation in the normal hierarchy Estimation of effects Other types of hierarchies Practical considerations in hierarchical modeling Philosophical considerations in hierarchical modeling Summary Exercises},
  chapter = {9},
  isbn = {978-0-470-31685-6},
  langid = {english},
  keywords = {Bayesian methodology,estimation principles,mixed model,posterior distribution,prior distribution},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470316856.ch9},
  file = {C\:\\Users\\micro\\Zotero\\storage\\M39ZN9HU\\1992 - Hierarchical Models and Bayesian Estimation.pdf;C\:\\Users\\micro\\Zotero\\storage\\XBTVDH6C\\9780470316856.html}
}

@article{higgins2009re,
  title = {A Re-Evaluation of Random-Effects Meta-Analysis},
  author = {Higgins, Julian PT and Thompson, Simon G and Spiegelhalter, David J},
  year = {2009},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {172},
  number = {1},
  pages = {137--159},
  publisher = {{Wiley Online Library}}
}

@article{hogben,
  title = {{{BIOLOGICAL MONOGRAPHS AND MANUALS}}},
  author = {Hogben, L T and Ponder, E},
  pages = {336},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\MIXD46XM\\Hogben and Ponder - BIOLOGICAL MONOGRAPHS AND MANUALS.pdf}
}

@article{hoogland1998,
  title = {Robustness {{Studies}} in {{Covariance Structure Modeling}}: {{An Overview}} and a {{Meta-Analysis}}},
  shorttitle = {Robustness {{Studies}} in {{Covariance Structure Modeling}}},
  author = {Hoogland, Jeffrey J. and Boomsma, Anne},
  year = {1998},
  month = feb,
  journal = {Sociological Methods \& Research},
  volume = {26},
  number = {3},
  pages = {329--367},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124198026003003},
  abstract = {In covariance structure modeling, several estimation methods are available. The robustness of an estimator against specific violations of assumptions can be determined empirically by means of a Monte Carlo study. Many such studies in covariance structure analysis have been published, but the conclusions frequently seem to contradict each other. An overview of robustness studies in covariance structure analysis is given, and an attempt is made to generalize findings. Robustness studies are described and distinguished from each other systematically by means of certain characteristics. These characteristics serve as explanatory variables in a meta-analysis concerning the behavior of parameter estimators, standard error estimators, and goodness-of-fit statistics when the model is correctly specified.},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\GXEW8KWC\\HOOGLAND and BOOMSMA - 1998 - Robustness Studies in Covariance Structure Modelin.pdf}
}

@article{hox2001,
  ids = {hox2001a},
  title = {The {{Accuracy}} of {{Multilevel Structural Equation Modeling With Pseudobalanced Groups}} and {{Small Samples}}},
  author = {Hox, Joop J. and Maas, Cora J. M.},
  year = {2001},
  month = apr,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {8},
  number = {2},
  pages = {157--174},
  issn = {1070-5511, 1532-8007},
  doi = {10.1207/S15328007SEM0802_1},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\JIRI6N5W\\Hox and Maas - 2001 - The Accuracy of Multilevel Structural Equation Mod.pdf;C\:\\Users\\micro\\Zotero\\storage\\UYA9WPIZ\\Hox and Maas - 2001 - The Accuracy of Multilevel Structural Equation Mod.pdf}
}

@article{hsu2017,
  title = {The {{Impact}} of {{Intraclass Correlation}} on the {{Effectiveness}} of {{Level-Specific Fit Indices}} in {{Multilevel Structural Equation Modeling}}: {{A Monte Carlo Study}}},
  shorttitle = {The {{Impact}} of {{Intraclass Correlation}} on the {{Effectiveness}} of {{Level-Specific Fit Indices}} in {{Multilevel Structural Equation Modeling}}},
  author = {Hsu, Hsien-Yuan and Lin, Jr-Hung and Kwok, Oi-Man and Acosta, Sandra and Willson, Victor},
  year = {2017},
  month = jan,
  journal = {Educational and Psychological Measurement},
  volume = {77},
  number = {1},
  pages = {5--31},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10.1177/0013164416642823},
  abstract = {Several researchers have recommended that level-specific fit indices should be applied to detect the lack of model fit at any level in multilevel structural equation models. Although we concur with their view, we note that these studies did not sufficiently consider the impact of intraclass correlation (ICC) on the performance of level-specific fit indices. Our study proposed to fill this gap in the methodological literature. A Monte Carlo study was conducted to investigate the performance of (a) level-specific fit indices derived by a partially saturated model method (e.g., CFIPS\_BCFIPS\_B{$<$}math display="inline" id="math1-0013164416642823" overflow="scroll" altimg="eq-00001.gif"{$><$}mrow{$><$}mi{$>$}CF{$<$}/mi{$><$}msub{$><$}mrow{$><$}mi{$>$}I{$<$}/mi{$><$}/mrow{$><$}mrow{$><$}mi{$>$}PS{$<$}/mi{$><$}mo{$>\_<$}/mo{$><$}mi{$>$}B{$<$}/mi{$><$}/mrow{$><$}/msub{$><$}/mrow{$><$}/math{$>$} and CFIPS\_WCFIPS\_W{$<$}math display="inline" id="math2-0013164416642823" overflow="scroll" altimg="eq-00002.gif"{$><$}mrow{$><$}mi{$>$}CF{$<$}/mi{$><$}msub{$><$}mrow{$><$}mi{$>$}I{$<$}/mi{$><$}/mrow{$><$}mrow{$><$}mi{$>$}PS{$<$}/mi{$><$}mo{$>\_<$}/mo{$><$}mi{$>$}W{$<$}/mi{$><$}/mrow{$><$}/msub{$><$}/mrow{$><$}/math{$>$}) and (b) SRMRWSRMRW{$<$}math display="inline" id="math3-0013164416642823" overflow="scroll" altimg="eq-00003.gif"{$><$}mrow{$><$}mi{$>$}SRM{$<$}/mi{$><$}msub{$><$}mrow{$><$}mi{$>$}R{$<$}/mi{$><$}/mrow{$><$}mrow{$><$}mi{$>$}W{$<$}/mi{$><$}/mrow{$><$}/msub{$><$}/mrow{$><$}/math{$>$} and SRMRBSRMRB{$<$}math display="inline" id="math4-0013164416642823" overflow="scroll" altimg="eq-00004.gif"{$><$}mrow{$><$}mi{$>$}SRM{$<$}/mi{$><$}msub{$><$}mrow{$><$}mi{$>$}R{$<$}/mi{$><$}/mrow{$><$}mrow{$><$}mi{$>$}B{$<$}/mi{$><$}/mrow{$><$}/msub{$><$}/mrow{$><$}/math{$>$} in terms of their performance in multilevel structural equation models across varying ICCs. The design factors included intraclass correlation (ICC: ICC1 = 0.091 to ICC6 = 0.500), numbers of groups in between-level models (NG: 50, 100, 200, and 1,000), group size (GS: 30, 50, and 100), and type of misspecification (no misspecification, between-level misspecification, and within-level misspecification). Our simulation findings raise a concern regarding the performance of between-level-specific partial saturated fit indices in low ICC conditions: the performances of both TLIPS\_BTLIPS\_B{$<$}math display="inline" id="math5-0013164416642823" overflow="scroll" altimg="eq-00005.gif"{$><$}mrow{$><$}mi{$>$}TL{$<$}/mi{$><$}msub{$><$}mrow{$><$}mi{$>$}I{$<$}/mi{$><$}/mrow{$><$}mrow{$><$}mi{$>$}PS{$<$}/mi{$><$}mo{$>\_<$}/mo{$><$}mi{$>$}B{$<$}/mi{$><$}/mrow{$><$}/msub{$><$}/mrow{$><$}/math{$>$} and RMSEAPS\_BRMSEAPS\_B{$<$}math display="inline" id="math6-0013164416642823" overflow="scroll" altimg="eq-00006.gif"{$><$}mrow{$><$}mi{$>$}RMSE{$<$}/mi{$><$}msub{$><$}mrow{$><$}mi{$>$}A{$<$}/mi{$><$}/mrow{$><$}mrow{$><$}mi{$>$}PS{$<$}/mi{$><$}mo{$>\_<$}/mo{$><$}mi{$>$}B{$<$}/mi{$><$}/mrow{$><$}/msub{$><$}/mrow{$><$}/math{$>$} were more influenced by ICC compared with CFIPS\_BCFIPS\_B{$<$}math display="inline" id="math7-0013164416642823" overflow="scroll" altimg="eq-00007.gif"{$><$}mrow{$><$}mi{$>$}CF{$<$}/mi{$><$}msub{$><$}mrow{$><$}mi{$>$}I{$<$}/mi{$><$}/mrow{$><$}mrow{$><$}mi{$>$}PS{$<$}/mi{$><$}mo{$>\_<$}/mo{$><$}mi{$>$}B{$<$}/mi{$><$}/mrow{$><$}/msub{$><$}/mrow{$><$}/math{$>$} and SRMRB. However, when traditional cutoff values (RMSEA{$\leq$} 0.06; CFI, TLI{$\geq$} 0.95; SRMR{$\leq$} 0.08) were applied, CFIPS\_BCFIPS\_B{$<$}math display="inline" id="math8-0013164416642823" overflow="scroll" altimg="eq-00008.gif"{$><$}mrow{$><$}mi{$>$}CF{$<$}/mi{$><$}msub{$><$}mrow{$><$}mi{$>$}I{$<$}/mi{$><$}/mrow{$><$}mrow{$><$}mi{$>$}PS{$<$}/mi{$><$}mo{$>\_<$}/mo{$><$}mi{$>$}B{$<$}/mi{$><$}/mrow{$><$}/msub{$><$}/mrow{$><$}/math{$>$} and TLIPS\_BTLIPS\_B{$<$}math display="inline" id="math9-0013164416642823" overflow="scroll" altimg="eq-00009.gif"{$><$}mrow{$><$}mi{$>$}TL{$<$}/mi{$><$}msub{$><$}mrow{$><$}mi{$>$}I{$<$}/mi{$><$}/mrow{$><$}mrow{$><$}mi{$>$}PS{$<$}/mi{$><$}mo{$>\_<$}/mo{$><$}mi{$>$}B{$<$}/mi{$><$}/mrow{$><$}/msub{$><$}/mrow{$><$}/math{$>$} were still able to detect misspecified between-level models even when ICC was as low as 0.091 (ICC1). On the other hand, both RMSEAPS\_BRMSEAPS\_B{$<$}math display="inline" id="math10-0013164416642823" overflow="scroll" altimg="eq-00010.gif"{$><$}mrow{$><$}mi{$>$}RMSE{$<$}/mi{$><$}msub{$><$}mrow{$><$}mi{$>$}A{$<$}/mi{$><$}/mrow{$><$}mrow{$><$}mi{$>$}PS{$<$}/mi{$><$}mo{$>\_<$}/mo{$><$}mi{$>$}B{$<$}/mi{$><$}/mrow{$><$}/msub{$><$}/mrow{$><$}/math{$>$} and SRMRBSRMRB{$<$}math display="inline" id="math11-0013164416642823" overflow="scroll" altimg="eq-00011.gif"{$><$}mrow{$><$}mi{$>$}SRM{$<$}/mi{$><$}msub{$><$}mrow{$><$}mi{$>$}R{$<$}/mi{$><$}/mrow{$><$}mrow{$><$}mi{$>$}B{$<$}/mi{$><$}/mrow{$><$}/msub{$><$}/mrow{$><$}/math{$>$} were not recommended under low ICC conditions.},
  langid = {english},
  keywords = {intraclass correlation,level-specific fit index,model evaluation,multilevel structural equation modeling},
  file = {C\:\\Users\\micro\\Zotero\\storage\\8EJM568M\\Hsu et al. - 2017 - The Impact of Intraclass Correlation on the Effect.pdf}
}

@article{huang2016alternatives,
  title = {Alternatives to Multilevel Modeling for the Analysis of Clustered Data},
  author = {Huang, Francis L},
  year = {2016},
  journal = {The Journal of Experimental Education},
  volume = {84},
  number = {1},
  pages = {175--196},
  publisher = {{Taylor \& Francis}}
}

@article{huang2018,
  title = {Using {{Cluster Bootstrapping}} to {{Analyze Nested Data With}} a {{Few Clusters}}},
  author = {Huang, Francis L.},
  year = {2018},
  month = apr,
  journal = {Educational and Psychological Measurement},
  volume = {78},
  number = {2},
  pages = {297--318},
  issn = {0013-1644},
  doi = {10.1177/0013164416678980},
  abstract = {Cluster randomized trials involving participants nested within intact treatment and control groups are commonly performed in various educational, psychological, and biomedical studies. However, recruiting and retaining intact groups present various practical, financial, and logistical challenges to evaluators and often, cluster randomized trials are performed with a low number of clusters (\textasciitilde 20 groups). Although multilevel models are often used to analyze nested data, researchers may be concerned of potentially biased results due to having only a few groups under study. Cluster bootstrapping has been suggested as an alternative procedure when analyzing clustered data though it has seen very little use in educational and psychological studies. Using a Monte Carlo simulation that varied the number of clusters, average cluster size, and intraclass correlations, we compared standard errors using cluster bootstrapping with those derived using ordinary least squares regression and multilevel models. Results indicate that cluster bootstrapping, though more computationally demanding, can be used as an alternative procedure for the analysis of clustered data when treatment effects at the group level are of primary interest. Supplementary material showing how to perform cluster bootstrapped regressions using R is also provided.},
  pmcid = {PMC5965657},
  pmid = {29795957},
  file = {C\:\\Users\\micro\\Zotero\\storage\\436KCQNZ\\Huang - 2018 - Using Cluster Bootstrapping to Analyze Nested Data.pdf}
}

%Institute of Education Sciences
@misc{institute_of_education_sciences_what_2022,
	title = {What {Works} {Clearinghouse} {Procedures} and {Standards} {Handbook}, {Version} 5.0},
	url = {https://ies.ed.gov/ncee/WWC/Docs/referenceresources/Final_WWC-HandbookVer5_0-0-508.pdf},
	language = {en},
	author = {IES},
	year = {2022},
	file = {What Works Clearinghouse Procedures and Standards .pdf:C\:\\Users\\betha\\Zotero\\storage\\BHW5VKA9\\What Works Clearinghouse Procedures and Standards .pdf:application/pdf},
}


@article{ionan2014,
  title = {Comparison of Confidence Interval Methods for an Intra-Class Correlation Coefficient ({{ICC}})},
  author = {Ionan, Alexei C. and Polley, Mei-Yin C. and McShane, Lisa M. and Dobbin, Kevin K.},
  year = {2014},
  month = nov,
  journal = {BMC Medical Research Methodology},
  volume = {14},
  number = {1},
  pages = {121},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-121},
  abstract = {The intraclass correlation coefficient (ICC) is widely used in biomedical research to assess the reproducibility of measurements between raters, labs, technicians, or devices. For example, in an inter-rater reliability study, a high ICC value means that noise variability (between-raters and within-raters) is small relative to variability from patient to patient. A confidence interval or Bayesian credible interval for the ICC is a commonly reported summary. Such intervals can be constructed employing either frequentist or Bayesian methodologies.},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\43X8WFLC\\Ionan et al. - 2014 - Comparison of confidence interval methods for an i.pdf}
}

@article{jacob2010,
  title = {New {{Empirical Evidence}} for the {{Design}} of {{Group Randomized Trials}} in {{Education}}},
  author = {Jacob, Robin and Zhu, Pei and Bloom, Howard},
  year = {2010},
  month = mar,
  journal = {Journal of Research on Educational Effectiveness},
  volume = {3},
  number = {2},
  pages = {157--198},
  publisher = {{Routledge}},
  issn = {1934-5747},
  doi = {10.1080/19345741003592428},
  abstract = {This article provides practical guidance for researchers who are designing studies that randomize groups to measure the impacts of educational interventions. The article (a) provides new empirical information about the values of parameters that influence the precision of impact estimates (intraclass correlations and R 2 values) and includes outcomes other than standardized test scores and data with a three-level structure rather than a two-level structure, and (b) discusses the error (both generalizability and estimation error) that exists in estimates of key design parameters and the implications this error has for design decisions. Data for the paper come primarily from two studies: the Chicago Literacy Initiative: Making Better Early Readers Study (CLIMBERS) and the School Breakfast Pilot Project (SBPP). The analysis sample from CLIMBERS comprised 430 four-year-old children from 47 preschool classrooms in 23 Chicago public schools. The analysis sample from the SBPP study comprised 1,151 third graders from 233 classrooms in 111 schools from 6 school districts. Student achievement data from the Reading First Impact Study is also used to supplement the discussion.},
  keywords = {group randomized trials,Intraclass correlations,R2,study design},
  annotation = {\_eprint: https://doi.org/10.1080/19345741003592428},
  file = {C\:\\Users\\micro\\Zotero\\storage\\B9B49AD7\\Jacob et al. - 2010 - New Empirical Evidence for the Design of Group Ran.pdf;C\:\\Users\\micro\\Zotero\\storage\\NB4YF3IB\\19345741003592428.html}
}

@article{jennrich1976newton,
  title = {Newton-{{Raphson}} and Related Algorithms for Maximum Likelihood Variance Component Estimation},
  author = {Jennrich, Robert I and Sampson, PF},
  year = {1976},
  journal = {Technometrics : a journal of statistics for the physical, chemical, and engineering sciences},
  volume = {18},
  number = {1},
  pages = {11--17},
  publisher = {{Taylor \& Francis}}
}

@techreport{johnson1970continuous,
  title = {Continuous Univariate Distributions},
  author = {Johnson, Norman L},
  year = {1970}
}

@article{kerkhoff2022,
  title = {Obtaining {{Sound Intraclass Correlation}} and {{Variance Estimates}} in {{Three-Level Models}}: {{The Role}} of {{Sampling-Strategies}}},
  shorttitle = {Obtaining {{Sound Intraclass Correlation}} and {{Variance Estimates}} in {{Three-Level Models}}},
  author = {Kerkhoff, Denise and Nussbeck, Fridtjof W.},
  year = {2022},
  month = mar,
  journal = {Methodology},
  volume = {18},
  number = {1},
  pages = {5--23},
  issn = {1614-2241},
  doi = {10.5964/meth.7265},
  abstract = {Three-level clustered data commonly occur in social and behavioral research and are prominently analyzed using multilevel modeling. The influence of the clustering on estimation results is assessed with the intraclass correlation coefficients (ICCs), which indicate the fraction of variance in the outcome located at each higher level. However, ICCs are prone to bias due to high requirements regarding the overall sample size and the sample size at each data level. In Monte Carlo simulations, we investigate how these sample characteristics influence the bias of the ICCs and statistical power of the variance components using robust ML-estimation. Results reveal considerable underestimation on Level-3 and the importance of the Level-3 sample size in combination with the ICC sizes. Based on our results, we derive concise sampling recommendations and discuss limits to our inferences.},
  copyright = {Copyright (c) 2022 Denise Kerkhoff, Fridtjof W. Nussbeck},
  langid = {english},
  keywords = {bias,hierarchical linear modeling,Monte Carlo simulation,sample size,statistical power},
  file = {C\:\\Users\\micro\\Zotero\\storage\\5NAG4DHF\\Kerkhoff and Nussbeck - 2022 - Obtaining Sound Intraclass Correlation and Varianc.pdf}
}

@article{kistnerExactDistributionsIntraclass2004,
  title = {Exact Distributions of Intraclass Correlation and {{Cronbach}}'s Alpha with {{Gaussian}} Data and General Covariance},
  author = {Kistner, Emily O. and Muller, Keith E.},
  year = {2004},
  journal = {Psychometrika},
  volume = {69},
  number = {3},
  pages = {459--474},
  publisher = {{Springer-Verlag, Springer}},
  address = {{New York}},
  issn = {0033-3123},
  doi = {10.1007/BF02295646},
  abstract = {Intraclass correlation and Cronbach's alpha are widely used to describe reliability of tests and measurements. Even with Gaussian data, exact distributions are known only for compound symmetric covariance (equal variances and equal correlations). Recently, large sample Gaussian approximations were derived for the distribution functions.New exact results allow calculating the exact distribution function and other properties of intraclass correlation and Cronbach's alpha, for Gaussian data with any covariance pattern, not just compound symmetry. Probabilities are computed in terms of the distribution function of a weighted sum of independent chi-square random variables.NewF approximations for the distribution functions of intraclass correlation and Cronbach's alpha are much simpler and faster to compute than the exact forms. Assuming the covariance matrix is known, the approximations typically provide sufficient accuracy, even with as few as ten observations.Either the exact or approximate distributions may be used to create confidence intervals around an estimate of reliability. Monte Carlo simulations led to a number of conclusions. Correctly assuming that the covariance matrix is compound symmetric leads to accurate confidence intervals, as was expected from previously known results. However, assuming and estimating a general covariance matrix produces somewhat optimistically narrow confidence intervals with 10 observations. Increasing sample size to 100 gives essentially unbiased coverage. Incorrectly assuming compound symmetry leads to pessimistically large confidence intervals, with pessimism increasing with sample size. In contrast, incorrectly assuming general covariance introduces only a modest optimistic bias in small samples. Hence the new methods seem preferable for creating confidence intervals, except when compound symmetry definitely holds.},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\WFX5AHQ5\\Kistner and Muller - 2004 - Exact distributions of intraclass correlation and .pdf}
}

@article{kivlighan2020,
  title = {Does the Group in Group Psychotherapy Matter? {{A}} Meta-Analysis of the Intraclass Correlation Coefficient in Group Treatment Research},
  shorttitle = {Does the Group in Group Psychotherapy Matter?},
  author = {Kivlighan, D. Martin III and Aloe, Ariel M. and Adams, Marie C. and Garrison, Yunkyoung Loh and Obrecht, Ashlie and Ho, Yu Chak Sunny and Kim, Ji Youn Cindy and Hooley, Isaac W. and Chan, Laurence and Deng, Kuo},
  year = {2020},
  month = apr,
  journal = {Journal of Consulting and Clinical Psychology},
  volume = {88},
  number = {4},
  pages = {322--337},
  publisher = {{American Psychological Association}},
  issn = {0022-006X},
  doi = {10.1037/ccp0000474},
  abstract = {Objective: Over the last 3 decades, group treatment researchers have become increasingly knowledgeable of the impact of within-group dependency on analyses of group treatment data and of mutual influence processes that occur within therapy groups. Despite these advancements, there remains a lack of consensus on the magnitude of mutual influence, or group effects, in group treatment research. As such, this study sought to estimate the size of group effects on members' posttreatment outcomes by meta-analyzing the intraclass correlation coefficients (ICCs) in group treatment research. In addition, we tested several moderators of the ICC, including outcome type, outcome reactivity, outcome specificity, group format, treatment length, and group size. Method: Using robust variance estimations, we meta-analyzed 169 effect sizes from 37 group treatment studies. Results: Findings indicated an average ICC of 0.06. Group size, group format, treatment length, outcome specificity, and outcome type did not significantly moderate the ICC; however, we did find evidence to suggest that the ICC varies as a function of outcome reactivity, with observer-rated outcome measures resulting in the largest ICC. Conclusion: These findings suggest that interdependence in group treatment research is an important concept both theoretically and statistically. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Apparent Size,Estimation,Experimentation,group effects,group psychotherapy,Group Psychotherapy,Group Size,Interdependence,intraclass correlation coefficient,meta-analysis,mutual influence,Treatment Outcomes},
  file = {C\:\\Users\\micro\\Zotero\\storage\\QZIVLFBA\\Kivlighan et al. - 2020 - Does the group in group psychotherapy matter A me.pdf}
}

@article{klarCurrentFutureChallenges2001,
  title = {Current and Future Challenges in the Design and Analysis of Cluster Randomization Trials},
  author = {Klar, Neil and Donner, Allan},
  year = {2001},
  journal = {Statistics in Medicine},
  volume = {20},
  number = {24},
  pages = {3729--3740},
  issn = {1097-0258},
  doi = {10.1002/sim.1115},
  abstract = {Randomized trials in which the unit of randomization is a community, worksite, school or family are becoming widely used in the evaluation of life-style interventions for the prevention of disease. The increasing interest in adopting a cluster randomization design is being matched by rapid methodological developments. In this paper we describe several of these developments. Brief mention is also made of issues related to economic analysis and to the planning and conduct of meta-analyses for cluster randomization trials. Recommendations for reporting are also discussed. Copyright \textcopyright{} 2001 John Wiley \& Sons, Ltd.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.1115},
  file = {C\:\\Users\\micro\\Zotero\\storage\\44SYYK65\\Klar and Donner - 2001 - Current and future challenges in the design and an.pdf;C\:\\Users\\micro\\Zotero\\storage\\RCMB7C5X\\sim.html}
}

@article{konstantopoulos2010,
  title = {Power {{Analysis}} in {{Two-Level Unbalanced Designs}}},
  author = {Konstantopoulos, Spyros},
  year = {2010},
  month = mar,
  journal = {The Journal of Experimental Education},
  volume = {78},
  number = {3},
  pages = {291--317},
  issn = {0022-0973, 1940-0683},
  doi = {10.1080/00220970903292876},
  langid = {english}
}

@article{kul2014,
  title = {Intraclass Correlation Coefficients for Cluster Randomized Trials in Care Pathways and Usual Care: Hospital Treatment for Heart Failure},
  shorttitle = {Intraclass Correlation Coefficients for Cluster Randomized Trials in Care Pathways and Usual Care},
  author = {Kul, Seval and Vanhaecht, Kris and Panella, Massimiliano},
  year = {2014},
  month = feb,
  journal = {BMC Health Services Research},
  volume = {14},
  number = {1},
  pages = {84},
  issn = {1472-6963},
  doi = {10.1186/1472-6963-14-84},
  abstract = {Cluster randomized trials are increasingly being used in healthcare evaluation to show the effectiveness of a specific intervention. Care pathways (CPs) are becoming a popular tool to improve the quality of health-care services provided to heart failure patients. In order to perform a well-designed cluster randomized trial to demonstrate the effectiveness of Usual care (UC) and CP in heart failure treatment, the intraclass correlation coefficient (ICC) should be available before conducting a trial to estimate the required sample size. This study reports ICCs for both demographical and outcome variables from cluster randomized trials of heart failure patients in UC and care pathways.},
  keywords = {Care pathways,Heart failure,Intraclass correlation coefficient,Multicenter cluster randomized trials},
  file = {C\:\\Users\\micro\\Zotero\\storage\\VIPJEDJD\\Kul et al. - 2014 - Intraclass correlation coefficients for cluster ra.pdf;C\:\\Users\\micro\\Zotero\\storage\\LCUC37ZU\\1472-6963-14-84.html}
}

@article{kush2021,
  title = {Statistical {{Power}} for {{Randomized Controlled Trials}} with {{Clusters}} of {{Varying Size}}},
  author = {Kush, Joseph M. and Konold, Timothy R. and Bradshaw, Catherine P.},
  year = {2021},
  month = feb,
  journal = {The Journal of Experimental Education},
  volume = {0},
  number = {0},
  pages = {1--17},
  publisher = {{Routledge}},
  issn = {0022-0973},
  doi = {10.1080/00220973.2021.1873089},
  abstract = {In two-level designs, the total sample is a function of both the number of Level 2 clusters and the average number of Level 1 units per cluster. Traditional multilevel power calculations rely on either the arithmetic average or the harmonic mean when estimating the average number of Level 1 units across clusters of unbalanced size. The current study compares these two approaches with simulation-based power estimates in cluster randomized controlled trial designs with unbalanced cluster size. Results from the Monte Carlo study demonstrated that the largest differences in simulated and calculated power occurred in study designs with large variability in the number of Level 1 units sampled. We discuss implications of these findings for the design of cluster randomized trials.},
  keywords = {Harmonic mean,multilevel models,power,sample size,unbalanced clusters},
  annotation = {\_eprint: https://doi.org/10.1080/00220973.2021.1873089},
  file = {C\:\\Users\\micro\\Zotero\\storage\\B4FYQ564\\Kush et al. - 2021 - Statistical Power for Randomized Controlled Trials.pdf;C\:\\Users\\micro\\Zotero\\storage\\39XXP4TD\\00220973.2021.html}
}

@article{lai2015,
  title = {Examining the {{Rule}} of {{Thumb}} of {{Not Using Multilevel Modeling}}: {{The}} ``{{Design Effect Smaller Than Two}}'' {{Rule}}},
  shorttitle = {Examining the {{Rule}} of {{Thumb}} of {{Not Using Multilevel Modeling}}},
  author = {Lai, Mark H. C. and Kwok, Oi-man},
  year = {2015},
  month = jul,
  journal = {The Journal of Experimental Education},
  volume = {83},
  number = {3},
  pages = {423--438},
  publisher = {{Routledge}},
  issn = {0022-0973},
  doi = {10.1080/00220973.2014.907229},
  abstract = {Educational researchers commonly use the rule of thumb of ``design effect smaller than 2'' as the justification of not accounting for the multilevel or clustered structure in their data. The rule, however, has not yet been systematically studied in previous research. In the present study, we generated data from three different models (which differ in the location of the clustering effect). With a 3 (design effect) \texttimes{} 5 (cluster size) \texttimes{} 4 (number of clusters) Monte Carlo simulation study we found that the rule should not be applied when researchers: (a) are interested in the effects of higher-level predictors, or (b) have a cluster size less than 10. Implications of the findings and limitations of the study are discussed.},
  keywords = {clustering,design effects,intraclass correlation,multilevel,simulation studies},
  annotation = {\_eprint: https://doi.org/10.1080/00220973.2014.907229},
  file = {C\:\\Users\\micro\\Zotero\\storage\\A2WAUFHJ\\Lai and Kwok - 2015 - Examining the Rule of Thumb of Not Using Multileve.pdf;C\:\\Users\\micro\\Zotero\\storage\\CBESQI2C\\00220973.2014.html}
}

@article{lai2016,
  title = {Estimating {{Standardized Effect Sizes}} for {{Two-}} and {{Three-Level Partially Nested Data}}},
  author = {Lai, Mark H. C. and Kwok, Oi-man},
  year = {2016},
  month = nov,
  journal = {Multivariate Behavioral Research},
  volume = {51},
  number = {6},
  pages = {740--756},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10.1080/00273171.2016.1231606},
  abstract = {Although previous research has discussed an effect size estimator for partially nested cluster randomized designs, the existing estimator (a) is not efficient when used with primary data, (b) can be biased when the homogeneity of variance assumption is violated, and (c) has not yet been empirically evaluated for its finite sample properties. The present paper addresses these limitations by proposing an alternative maximum likelihood estimator for obtaining standardized mean difference effect size and the corresponding sampling variance for partially nested data, as well as the variants that do not make an assumption of homogeneity of variance. The typical estimator, denoted as d (dW with pooled SD and dC with control arm SD), requires input of summary statistics such as observed means, variances, and the intraclass correlation, and is useful for meta-analyses and secondary data analyses; the newly proposed estimator {$\delta\sphat$} ({$\delta\sphat$}W and {$\delta\sphat$}C) takes parameter estimates from a correctly specified multilevel model as input and is mainly of interest to researchers doing primary research. The simulation results showed that the two methods (d and {$\delta\sphat$}) produced unbiased point and variance estimates for effect size. As expected, in general, {$\delta\sphat$} was more efficient than d with unequal cluster sizes, especially with large average cluster size and large intraclass correlation. Furthermore, under heterogeneous variances, {$\delta\sphat$} demonstrated a greater relative efficiency with small sample size for the unclustered control arm. Real data examples, one from a youth preventive program and one from an eating disorder intervention, were used to demonstrate the methods presented. In addition, we extend the discussion to a scenario with a three-level treatment arm and an unclustered control arm, and illustrate the procedures for effect size estimation using a hypothetical example of multiple therapy groups of clients clustered within therapists.},
  pmid = {27802077},
  keywords = {Effect size,multilevel,partial clustering,partially nested},
  annotation = {\_eprint: https://doi.org/10.1080/00273171.2016.1231606},
  file = {C\:\\Users\\micro\\Zotero\\storage\\47QLNN7R\\Lai and Kwok - 2016 - Estimating Standardized Effect Sizes for Two- and .pdf}
}

@article{langan2019,
  title = {A Comparison of Heterogeneity Variance Estimators in Simulated Random-Effects Meta-Analyses},
  author = {Langan, Dean and Higgins, Julian P.T. and Jackson, Dan and Bowden, Jack and Veroniki, Areti Angeliki and Kontopantelis, Evangelos and Viechtbauer, Wolfgang and Simmonds, Mark},
  year = {2019},
  journal = {Research Synthesis Methods},
  volume = {10},
  number = {1},
  pages = {83--98},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1316},
  abstract = {Studies combined in a meta-analysis often have differences in their design and conduct that can lead to heterogeneous results. A random-effects model accounts for these differences in the underlying study effects, which includes a heterogeneity variance parameter. The DerSimonian-Laird method is often used to estimate the heterogeneity variance, but simulation studies have found the method can be biased and other methods are available. This paper compares the properties of nine different heterogeneity variance estimators using simulated meta-analysis data. Simulated scenarios include studies of equal size and of moderate and large differences in size. Results confirm that the DerSimonian-Laird estimator is negatively biased in scenarios with small studies and in scenarios with a rare binary outcome. Results also show the Paule-Mandel method has considerable positive bias in meta-analyses with large differences in study size. We recommend the method of restricted maximum likelihood (REML) to estimate the heterogeneity variance over other methods. However, considering that meta-analyses of health studies typically contain few studies, the heterogeneity variance estimate should not be used as a reliable gauge for the extent of heterogeneity in a meta-analysis. The estimated summary effect of the meta-analysis and its confidence interval derived from the Hartung-Knapp-Sidik-Jonkman method are more robust to changes in the heterogeneity variance estimate and show minimal deviation from the nominal coverage of 95\% under most of our simulated scenarios.},
  langid = {english},
  keywords = {DerSimonian-Laird,heterogeneity,random-effects,REML,simulation},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1316},
  file = {C\:\\Users\\micro\\Zotero\\storage\\R6QPHLEB\\Langan et al. - 2019 - A comparison of heterogeneity variance estimators .pdf;C\:\\Users\\micro\\Zotero\\storage\\L69V5RH3\\jrsm.html}
}

@article{LIANGKUNG-YEE1986Ldau,
  ids = {liang},
  title = {Longitudinal Data Analysis Using Generalized Linear Models},
  author = {Liang, Kung-Yee and Zeger, Scott L},
  year = {1986},
  journal = {Biometrika},
  volume = {73},
  number = {1},
  pages = {13--22},
  publisher = {{Oxford University Press}},
  address = {{Oxford}},
  issn = {0006-3444},
  abstract = {This paper proposes an extension of generalized linear models to the analysis of longitudinal data. We introduce a class of estimating equations that give consistent estimates of the regression parameters and of their variance under mild assumptions about the time dependence. The estimating equations are derived without specifying the joint distribution of a subject's observations yet they reduce to the score equations for niultivariate Gaussian outcomes. Asymptotic theory is presented for the general class of estimators. Specific cases in which we assume independence, m-dependence and exchangeable correlation structures from each subject are discussed. Efficiency of the pioposecl estimators in two simple situations is considered. The approach is closely related to quasi-likelihood.},
  copyright = {Copyright 1986 Biometrika Trust},
  langid = {english},
  keywords = {Consistent estimators,Correlations,Covariance matrices,Estimating equation,Estimators,Exact sciences and technology,Generalized linear model,Linear regression,Longitudinal data,Mathematics,Matrices,Multivariate analysis,Probability and statistics,Quasi-likelihood,Repeated measures,Sciences and techniques of general use,Statistical variance,Statistics,Time dependence},
  file = {C\:\\Users\\micro\\Zotero\\storage\\RF92AWQ3\\Liang and Zeger - Longitudinal data analysis using generalized linea.pdf}
}

@misc{lk2020muthen,
  title = {Mplus {{User}}'s {{Guide}}},
  author = {Muth{\'e}n, L K and Muth{\'e}n, B O},
  year = {1998},
  address = {{Los Angeles, CA}},
  howpublished = {Muth\'en \& Muth\'en}
}

@article{maas2005,
  ids = {maasSufficientSampleSizes2005},
  title = {Sufficient {{Sample Sizes}} for {{Multilevel Modeling}}},
  author = {Maas, Cora J. M. and Hox, Joop J.},
  year = {2005},
  journal = {Methodology: European Journal of Research Methods for the Behavioral and Social Sciences},
  volume = {1},
  number = {3},
  pages = {86--92},
  publisher = {{Hogrefe \& Huber Publishers}},
  issn = {1614-1881},
  doi = {10.1027/1614-2241.1.3.86},
  abstract = {An important problem in multilevel modeling is what constitutes a sufficient sample size for accurate estimation. In multilevel analysis, the major restriction is often the higher-level sample size. In this paper, a simulation study is used to determine the influence of different sample sizes at the group level on the accuracy of the estimates (regression coefficients and variances) and their standard errors. In addition, the influence of other factors, such as the lowest-level sample size and different variance distributions between the levels (different intraclass correlations), is examined. The results show that only a small sample size at level two (meaning a sample of 50 or less) leads to biased estimates of the second-level standard errors. In all of the other simulated conditions the estimates of the regression coefficients, the variance components, and the standard errors are unbiased and accurate. (PsycINFO Database Record (c) 2020 APA, all rights reserved)},
  keywords = {accuracy,Biased Sampling,cluster sampling,error bias,Errors,Experimentation,group estimates,hierarchical linear model,intraclass correlations,Mathematical Modeling,multilevel modeling,regression modeling,sample size,Sample Size,simulation models,standard errors,Statistical Correlation,Statistical Regression},
  file = {C\:\\Users\\micro\\Zotero\\storage\\HSHPJD2S\\Maas and Hox - 2005 - Sufficient Sample Sizes for Multilevel Modeling.pdf;C\:\\Users\\micro\\Zotero\\storage\\RGI52FLK\\methodology05.pdf}
}

@article{macleod2020,
  title = {Estimating the {{Intracluster Correlation Coefficient}} for the {{Clinical Sign}} ``{{Trachomatous Inflammation}}\textemdash{{Follicular}}'' in {{Population-Based Trachoma Prevalence Surveys}}: {{Results From}} a {{Meta-Regression Analysis}} of 261 {{Standardized Preintervention Surveys Carried Out}} in {{Ethiopia}}, {{Mozambique}}, and {{Nigeria}}},
  shorttitle = {Estimating the {{Intracluster Correlation Coefficient}} for the {{Clinical Sign}} ``{{Trachomatous Inflammation}}\textemdash{{Follicular}}'' in {{Population-Based Trachoma Prevalence Surveys}}},
  author = {Macleod, Colin K and Bailey, Robin L and Dejene, Michael and Shafi, Oumer and Kebede, Biruck and Negussu, Nebiyu and Mpyet, Caleb and Olobio, Nicholas and Alada, Joel and Abdala, Mariamo and Willis, Rebecca and Hayes, Richard and Solomon, Anthony W},
  year = {2020},
  month = jan,
  journal = {American Journal of Epidemiology},
  volume = {189},
  number = {1},
  pages = {68--76},
  issn = {0002-9262},
  doi = {10.1093/aje/kwz196},
  abstract = {Sample sizes in cluster surveys must be greater than those in surveys using simple random sampling in order to obtain similarly precise prevalence estimates, because results from subjects examined in the same cluster cannot be assumed to be independent. Therefore, a crucial aspect of cluster sampling is estimation of the intracluster correlation coefficient ({$\rho$}): the degree of relatedness of outcomes in a given cluster, defined as the proportion of total variance accounted for by between-cluster variation. In infectious disease epidemiology, this coefficient is related to transmission patterns and the natural history of infection; its value also depends on particulars of survey design. Estimation of {$\rho$} is often difficult due to the lack of comparable survey data with which to calculate summary estimates. Here we use a parametric bootstrap model to estimate {$\rho$} for the ocular clinical sign ``trachomatous inflammation\textemdash follicular'' (TF) among children aged 1\textendash 9 years within population-based trachoma prevalence surveys. We present results from a meta-regression analysis of data from 261 such surveys completed using standardized methods in Ethiopia, Mozambique, and Nigeria in 2012\textendash 2015. Consistent with the underlying theory, we found that {$\rho$} increased with increasing overall TF prevalence and smaller numbers of children examined per cluster. Estimates of {$\rho$} for TF were independently higher in Ethiopia than in the other countries.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\BR6C52K9\\Macleod et al. - 2020 - Estimating the Intracluster Correlation Coefficien.pdf;C\:\\Users\\micro\\Zotero\\storage\\VTZRWIBJ\\5566627.html}
}

@article{macleodEstimatingIntraclusterCorrelation2020,
  title = {Estimating the {{Intracluster Correlation Coefficient}} for the {{Clinical Sign}} ``{{Trachomatous Inflammation}}\textemdash{{Follicular}}'' in {{Population-Based Trachoma Prevalence Surveys}}: {{Results From}} a {{Meta-Regression Analysis}} of 261 {{Standardized Preintervention Surveys Carried Out}} in {{Ethiopia}}, {{Mozambique}}, and {{Nigeria}}},
  shorttitle = {Estimating the {{Intracluster Correlation Coefficient}} for the {{Clinical Sign}} ``{{Trachomatous Inflammation}}\textemdash{{Follicular}}'' in {{Population-Based Trachoma Prevalence Surveys}}},
  author = {Macleod, Colin K and Bailey, Robin L and Dejene, Michael and Shafi, Oumer and Kebede, Biruck and Negussu, Nebiyu and Mpyet, Caleb and Olobio, Nicholas and Alada, Joel and Abdala, Mariamo and Willis, Rebecca and Hayes, Richard and Solomon, Anthony W},
  year = {2020},
  month = jan,
  journal = {American Journal of Epidemiology},
  volume = {189},
  number = {1},
  pages = {68--76},
  issn = {0002-9262},
  doi = {10.1093/aje/kwz196},
  abstract = {Sample sizes in cluster surveys must be greater than those in surveys using simple random sampling in order to obtain similarly precise prevalence estimates, because results from subjects examined in the same cluster cannot be assumed to be independent. Therefore, a crucial aspect of cluster sampling is estimation of the intracluster correlation coefficient ({$\rho$}): the degree of relatedness of outcomes in a given cluster, defined as the proportion of total variance accounted for by between-cluster variation. In infectious disease epidemiology, this coefficient is related to transmission patterns and the natural history of infection; its value also depends on particulars of survey design. Estimation of {$\rho$} is often difficult due to the lack of comparable survey data with which to calculate summary estimates. Here we use a parametric bootstrap model to estimate {$\rho$} for the ocular clinical sign ``trachomatous inflammation\textemdash follicular'' (TF) among children aged 1\textendash 9 years within population-based trachoma prevalence surveys. We present results from a meta-regression analysis of data from 261 such surveys completed using standardized methods in Ethiopia, Mozambique, and Nigeria in 2012\textendash 2015. Consistent with the underlying theory, we found that {$\rho$} increased with increasing overall TF prevalence and smaller numbers of children examined per cluster. Estimates of {$\rho$} for TF were independently higher in Ethiopia than in the other countries.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\IMS64Y57\\Macleod et al. - 2020 - Estimating the Intracluster Correlation Coefficien.pdf;C\:\\Users\\micro\\Zotero\\storage\\RVIWCD6K\\5566627.html}
}

@article{manthena,
  title = {{{SAS Proc Mixed}}: {{A Statistical Programmer}}'s {{Best Friend}} in {{QoL Analyses}}},
  author = {Manthena, Janaki},
  pages = {9},
  abstract = {SAS PROC MIXED is a powerful procedure that can be used to efficiently and comprehensively analyze longitudinal data such as many patient-reported outcomes (PRO) measurements overtime, especially when missing data are prevalent. This paper illustrates the commonly used statements and options in this procedure when used in such analyses. We will present a statistical programmer's perspective on how to calculate Least Square (LS) Mean, Standard Error, difference in LS Means between treatment arms, and corresponding 95\% confidence interval at each time point using this procedure. This will be demonstrated using examples of PROC MIXED focusing on both linear mixed models and pattern mixture models on imputed and original QLQ-C30 questionnaire data, respectively.},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\D7MLC29C\\Manthena - SAS Proc Mixed A Statistical Programmer's Best Fr.pdf}
}

@incollection{MaximumLikelihoodML1992,
  title = {Maximum {{Likelihood}} ({{ML}}) and {{Restricted Maximum Likelihood}} ({{REML}})},
  booktitle = {Variance {{Components}}},
  year = {1992},
  pages = {232--257},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470316856.ch6},
  abstract = {The prelims comprise: The model and likelihood function The ML estimation equations Asymptotic dispersion matrices for ML estimators Some remarks on computing ML results for 2-way crossed classification, balanced data a. 2-way crossed, random model, with interaction Restricted maximum likelihood (REML) Estimating fixed effects in mixed models ML or REML? Summary Exercises},
  chapter = {6},
  isbn = {978-0-470-31685-6},
  langid = {english},
  keywords = {probability distribution,random effects,residual errors,residual maximum likelihood,variance components},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470316856.ch6},
  file = {C\:\\Users\\micro\\Zotero\\storage\\B3W5FWB6\\1992 - Maximum Likelihood (ML) and Restricted Maximum Lik.pdf;C\:\\Users\\micro\\Zotero\\storage\\3BY64XDR\\9780470316856.html}
}

@article{mcneish2016,
  title = {The {{Effect}} of {{Small Sample Size}} on {{Two-Level Model Estimates}}: {{A Review}} and {{Illustration}}},
  shorttitle = {The {{Effect}} of {{Small Sample Size}} on {{Two-Level Model Estimates}}},
  author = {McNeish, Daniel and Stapleton, Laura},
  year = {2016},
  month = jun,
  journal = {Educational Psychology Review},
  volume = {28},
  number = {2},
  pages = {295--314},
  publisher = {{Springer Nature}},
  issn = {1040726X},
  doi = {10.1007/s10648-014-9287-x},
  abstract = {Multilevel models are an increasingly popular method to analyze data that originate from a clustered or hierarchical structure. To effectively utilize multilevel models, one must have an adequately large number of clusters; otherwise, some model parameters will be estimated with bias. The goals for this paper are to (1) raise awareness of the problems associated with a small number of clusters, (2) review previous studies on multilevel models with a small number of clusters, (3) to provide an illustrative simulation to demonstrate how a simple model becomes adversely affected by small numbers of clusters, (4) to provide researchers with remedies if they encounter clustered data with a small number of clusters, and (5) to outline methodological topics that have yet to be addressed in the literature.},
  keywords = {HIERARCHICAL clustering (Cluster analysis),HLM,MAXIMUM likelihood statistics,Mixed model,Multilevel model,MULTILEVEL models,SAMPLE size (Statistics),SIMULATION methods \& models,Small number of clusters,Small sample},
  file = {C\:\\Users\\micro\\Zotero\\storage\\SK963MC9\\McNeish and Stapleton - 2016 - The Effect of Small Sample Size on Two-Level Model.pdf}
}

@article{mcneish2017,
  title = {Challenging {{Conventional Wisdom}} for {{Multivariate Statistical Models With Small Samples}}},
  author = {McNeish, Daniel},
  year = {2017},
  month = dec,
  journal = {Review of Educational Research},
  volume = {87},
  number = {6},
  pages = {1117--1151},
  publisher = {{American Educational Research Association}},
  issn = {0034-6543},
  doi = {10.3102/0034654317727727},
  abstract = {In education research, small samples are common because of financial limitations, logistical challenges, or exploratory studies. With small samples, statistical principles on which researchers rely do not hold, leading to trust issues with model estimates and possible replication issues when scaling up. Researchers are generally aware of such pitfalls and often attempt to tailor their analyses to accommodate small samples. Despite well-intentioned efforts, conventional statistical axioms do not always translate to best practice with small samples, and common recommendations can, counterintuitively, exacerbate small sample problems. In this article, we overview the landscape of small sample techniques and note why conventionally recommended approaches can fail with small samples while also suggesting lesser known alternatives that tend to perform better in statistical research but are not widely adopted in education research. Topics include bootstrapping, latent variable model fit, and Bayesian methods for multilevel, latent variable, and growth models. Simulated and real-data examples are interspersed throughout.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\GTVRI97C\\McNeish - 2017 - Challenging Conventional Wisdom for Multivariate S.pdf}
}

@article{mcneish2019,
  title = {Fixed Effects Models versus Mixed Effects Models for Clustered Data: {{Reviewing}} the Approaches, Disentangling the Differences, and Making Recommendations},
  shorttitle = {Fixed Effects Models versus Mixed Effects Models for Clustered Data},
  author = {McNeish, Daniel and Kelley, Ken},
  year = {2019},
  month = feb,
  journal = {Psychological Methods},
  volume = {24},
  number = {1},
  pages = {20--35},
  publisher = {{American Psychological Association}},
  issn = {1082-989X},
  doi = {10.1037/met0000182},
  abstract = {Clustered data are common in many fields. Some prominent examples of clustering are employees clustered within supervisors, students within classrooms, and clients within therapists. Many methods exist that explicitly consider the dependency introduced by a clustered data structure, but the multitude of available options has resulted in rigid disciplinary preferences. For example, those working in the psychological, organizational behavior, medical, and educational fields generally prefer mixed effects models, whereas those working in economics, behavioral finance, and strategic management generally prefer fixed effects models. However, increasingly interdisciplinary research has caused lines that separate the fields grounded in psychology and those grounded in economics to blur, leading to researchers encountering unfamiliar statistical methods commonly found in other disciplines. Persistent discipline-specific preferences can be particularly problematic because (a) each approach has certain limitations that can restrict the types of research questions that can be appropriately addressed, and (b) analyses based on the statistical modeling decisions common in one discipline can be difficult to understand for researchers trained in alternative disciplines. This can impede cross-disciplinary collaboration and limit the ability of scientists to make appropriate use of research from adjacent fields. This article discusses the differences between mixed effects and fixed effects models for clustered data, reviews each approach, and helps to identify when each approach is optimal. We then discuss the within\textendash between specification, which blends advantageous properties of each framework into a single model. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Cluster Analysis,Data Sets,Effect Size (Statistical),fixed effect model,HLM,Models,multilevel model,panel data,random coefficients model},
  file = {C\:\\Users\\micro\\Zotero\\storage\\QW63WJUB\\McNeish and Kelley - 2019 - Fixed effects models versus mixed effects models f.pdf}
}

@article{moeyaert2017,
  title = {Methods for Dealing with Multiple Outcomes in Meta-Analysis: A Comparison between Averaging Effect Sizes, Robust Variance Estimation and Multilevel Meta-Analysis},
  shorttitle = {Methods for Dealing with Multiple Outcomes in Meta-Analysis},
  author = {Moeyaert, Mariola and Ugille, Maaike and Natasha Beretvas, S. and Ferron, John and Bunuan, Rommel and {Van den Noortgate}, Wim},
  year = {2017},
  month = nov,
  journal = {International Journal of Social Research Methodology},
  volume = {20},
  number = {6},
  pages = {559--572},
  publisher = {{Routledge}},
  issn = {1364-5579},
  doi = {10.1080/13645579.2016.1252189},
  abstract = {This study investigates three methods to handle dependency among effect size estimates in meta-analysis arising from studies reporting multiple outcome measures taken on the same sample. The three-level approach is compared with the method of robust variance estimation, and with averaging effects within studies. A simulation study is performed, and the fixed and random effect estimates of the three methods are compared with each other. Both the robust variance estimation and three-level approach result in unbiased estimates of the fixed effects, corresponding standard errors and variances. Averaging effect sizes results in overestimated standard errors when the effect sizes within studies are truly independent. Although the robust variance and three-level approach are more complicated to use, they have the advantage that they do not require an estimate of the correlation between outcomes, and they still result in unbiased parameter estimates.},
  keywords = {averaging effects,dependent effect sizes,Meta-analysis,multilevel meta-analysis,robust variance estimation},
  annotation = {\_eprint: https://doi.org/10.1080/13645579.2016.1252189},
  file = {C\:\\Users\\micro\\Zotero\\storage\\XZPNQULH\\13645579.2016.html}
}

@article{mok1995sample,
  title = {Sample Size Requirements for 2-Level Designs in Educational Research},
  author = {Mok, Magdalena},
  year = {1995},
  journal = {Multilevel modelling newsletter},
  volume = {7},
  number = {2},
  pages = {11--15}
}

@book{mostellerEvidenceMattersRandomized2004,
  title = {Evidence {{Matters}}: {{Randomized Trials}} in {{Education Research}}},
  shorttitle = {Evidence {{Matters}}},
  author = {Mosteller, Frederick F. and Boruch, Robert F.},
  year = {2004},
  month = may,
  publisher = {{Brookings Institution Press}},
  abstract = {Opinions about education programs and practices are offered frequently\textemdash by children, parents, teachers, and policymakers. Credible studies of the impact of programs on the performance of children are far less frequent. Researchers use a variety of tools to determine their impact and efficacy, including sample surveys, narrative studies, and exploratory research. However, randomized field trials, which are commonly used in other disciplines, are rarely employed to measure the impact of education practice. Evidence Matters explores the history and current status of research in education and encourages the more frequent use of such trials. Judith Gueron (Manpower Demonstration Research Corporation), discusses the challenges involved in randomized trials and offers practical advice drawn experience. Robert Boruch (Wharton School, University of Pennsylvania), Dorothy de Moya (Campbell Collaboration Secretariat), and Brooke Snyder (University of Pennsylvania) explore the use of randomized field trials in education and other fields. David Cohen, Stephen Raudenbush, and Deborah Loewenberg Ball (all from the University of Michigan) review the history of progress in education over the past forty years and urge increased research on coherent instruction regimes. Maris Vinovskis (University of Michigan) examines the history and role of the U.S. Department of Education in developing rigorous evaluation of federal programs, and suggests a new National Center for Evaluation and Development. Thomas Cook and Monique Renee Payne (both from Northwestern University) take on the claim that randomized field trials are inappropriate in the U.S. education system. Gary Burtless (Brookings Institution) explores the political and professional factors that influence randomized field trials in economic programs, examining possible explanations for their lack of frequent use in education. Carol Weiss (Harvard University) provides a brief history of community studies in the United States and suggests a variety of alternatives to randomization. It is difficult to gauge the impact of various approaches in education. But the authors give a variety of concrete examples to illustrate the feasibility of randomized trials, and the circumstances under which they are appropriate. By offering a variety of suggestions to improve the methods used to evaluate education programs, the contributors to this volume seek to improve education in the United States. Frederick Mosteller is Roger I. Lee Professor in Mathematical Statistics, emeritus, in the department of statistics at Harvard University. Robert Boruch is the University Trustee Chair Professor the graduate school of education and statistics department at the Wharton School, University of Pennsylvania},
  googlebooks = {OcmGDwAAQBAJ},
  isbn = {978-0-8157-9818-7},
  langid = {english},
  keywords = {Education / Educational Policy \& Reform / General,Education / Evaluation \& Assessment}
}

@book{murrayDesignAnalysisGrouprandomized1998,
  title = {Design and {{Analysis}} of {{Group-randomized Trials}}},
  author = {Murray, David M. and Murray, Chair of the Division of Epidemiology David M.},
  year = {1998},
  publisher = {{Oxford University Press}},
  abstract = {This text provides the most comprehensive treatment of the design and analytic issues involved in group-randomized trials. GRTs are comparative studies conducted to evaluate the effect of a health promotion intervention in which the units of assignment are identifiable groups (e.g., schools, worksites) and the units of observation are members of those groups (e.g., students, workers). The book reviews the underlying issues, the most widely used research designs, and analytic strategies. There is an emphasis on mixed-model regression, with two chapters illustrating the analytic methods in SAS PROC MIXED and GLIMMIX. There is also a detailed chapter on power analysis and sample size calculation.},
  googlebooks = {9ERYAgAAQBAJ},
  isbn = {978-0-19-512036-3},
  langid = {english},
  keywords = {Medical / Research}
}

@article{muthen1995,
  title = {Complex {{Sample Data}} in {{Structural Equation Modeling}}},
  author = {Muthen, Bengt O. and Satorra, Albert},
  year = {1995},
  journal = {Sociological Methodology},
  volume = {25},
  pages = {267},
  issn = {00811750},
  doi = {10.2307/271070},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\LK5A2ZKJ\\Muthen and Satorra - 1995 - Complex Sample Data in Structural Equation Modelin.pdf}
}

@article{nakagawa,
  title = {The Coefficient of Determination {{R2}} and Intra-Class Correlation Coefficient from Generalized Linear Mixed-Effects Models Revisited and Expanded},
  author = {Nakagawa, Shinichi and Johnson, Paul C D and Schielzeth, Holger},
  pages = {11},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\222FFWAB\\Nakagawa et al. - The coefficient of determination R2 and intra-clas.pdf}
}

@article{nakagawa2017,
  title = {The Coefficient of Determination {{R2}} and Intra-Class Correlation Coefficient from Generalized Linear Mixed-Effects Models Revisited and Expanded},
  author = {Nakagawa, Shinichi and Johnson, Paul C. D. and Schielzeth, Holger},
  year = {2017},
  month = sep,
  journal = {Journal of The Royal Society Interface},
  volume = {14},
  number = {134},
  pages = {20170213},
  publisher = {{Royal Society}},
  doi = {10.1098/rsif.2017.0213},
  abstract = {The coefficient of determination R2 quantifies the proportion of variance explained by a statistical model and is an important summary statistic of biological interest. However, estimating R2 for generalized linear mixed models (GLMMs) remains challenging. We have previously introduced a version of R2 that we called  for Poisson and binomial GLMMs, but not for other distributional families. Similarly, we earlier discussed how to estimate intra-class correlation coefficients (ICCs) using Poisson and binomial GLMMs. In this paper, we generalize our methods to all other non-Gaussian distributions, in particular to negative binomial and gamma distributions that are commonly used for modelling biological data. While expanding our approach, we highlight two useful concepts for biologists, Jensen's inequality and the delta method, both of which help us in understanding the properties of GLMMs. Jensen's inequality has important implications for biologically meaningful interpretation of GLMMs, whereas the delta method allows a general derivation of variance associated with non-Gaussian distributions. We also discuss some special considerations for binomial GLMMs with binary or proportion data. We illustrate the implementation of our extension by worked examples from the field of ecology and evolution in the R environment. However, our method can be used across disciplines and regardless of statistical environments.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\GFEETEYJ\\Nakagawa et al. - 2017 - The coefficient of determination R2 and intra-clas.pdf;C\:\\Users\\micro\\Zotero\\storage\\M5EZUBM2\\rsif.2017.html}
}

@article{paccagnella2011,
  title = {Sample {{Size}} and {{Accuracy}} of {{Estimates}} in {{Multilevel Models}}},
  author = {Paccagnella, Omar},
  year = {2011},
  month = jan,
  journal = {Methodology},
  volume = {7},
  number = {3},
  pages = {111--120},
  publisher = {{Hogrefe Publishing}},
  issn = {1614-1881},
  doi = {10.1027/1614-2241/a000029},
  abstract = {In a multilevel framework several researches have investigated the behavior of estimates in finite samples, particularly for continuous dependent variables. Some findings show poor precise estimates for the variance components. On the other hand, discrete response multilevel models have been investigated less widely. In this paper we analyze the influence of different factors on the accuracy of estimates and standard errors of estimates in a binary response 2-level model, through a Monte Carlo simulation study. We investigate the hypothesis of: (a) small sample sizes; (b) different intraclass correlation coefficients; (c) different numbers of quadrature points in the estimation procedure. Standard errors of estimates are studied through a noncoverage indicator. In all instances we have considered, the point estimates are unbiased (even with very small sample sizes), while the variance components are underestimated. The accuracy of the standard errors of variance estimates needs a very large number of groups.},
  keywords = {accuracy of estimates,Monte Carlo simulation,multilevel modeling,noncoverage indicator,sample size}
}

@article{pagel2011,
  title = {Intracluster Correlation Coefficients and Coefficients of Variation for Perinatal Outcomes from Five Cluster-Randomised Controlled Trials in Low and Middle-Income Countries: Results and Methodological Implications},
  shorttitle = {Intracluster Correlation Coefficients and Coefficients of Variation for Perinatal Outcomes from Five Cluster-Randomised Controlled Trials in Low and Middle-Income Countries},
  author = {Pagel, Christina and Prost, Audrey and Lewycka, Sonia and Das, Sushmita and Colbourn, Tim and Mahapatra, Rajendra and Azad, Kishwar and Costello, Anthony and Osrin, David},
  year = {2011},
  month = jun,
  journal = {Trials},
  volume = {12},
  number = {1},
  pages = {151},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-12-151},
  abstract = {Public health interventions are increasingly evaluated using cluster-randomised trials in which groups rather than individuals are allocated randomly to treatment and control arms. Outcomes for individuals within the same cluster are often more correlated than outcomes for individuals in different clusters. This needs to be taken into account in sample size estimations for planned trials, but most estimates of intracluster correlation for perinatal health outcomes come from hospital-based studies and may therefore not reflect outcomes in the community. In this study we report estimates for perinatal health outcomes from community-based trials to help researchers plan future evaluations.},
  keywords = {Maternal Death,Maternal Mortality,Neonatal Death,Neonatal Mortality,Perinatal Outcome},
  file = {C\:\\Users\\micro\\Zotero\\storage\\PAQX27DB\\Pagel et al. - 2011 - Intracluster correlation coefficients and coeffici.pdf;C\:\\Users\\micro\\Zotero\\storage\\FSYYAIXR\\1745-6215-12-151.html}
}

@article{pal2004,
  title = {On Intra-Class Correlation Coefficient Estimation},
  author = {Pal, Nabendu and Lim, Wool K.},
  year = {2004},
  month = jul,
  journal = {Statistical Papers},
  volume = {45},
  number = {3},
  pages = {369--392},
  publisher = {{Springer Nature}},
  issn = {09325026},
  doi = {10.1007/BF02777578},
  abstract = {Consider the problem of estimating the intra-class correlation coefficient of a symmetric normal distribution. In a recent article (Pal and Lim (1999)) it has been shown that the three popular estimators, namely--the maximum likelihood estimator (MLE), the method of moments estimator (MME) and the unique minimum variance unbiased estimator (UMVUE), are second order admissible under the squared error loss function, in this paper we study the performance of the above mentioned estimators in terms of Pitman. Nearness Criterion (PNC) as well as Stochastic Domination Criterion (SDC). We then apply the aforementioned estimators to two real life data sets with moderate to large sample sizes, and bootstrap bias as well as mean squared errors are computed to compare the estimators. In terms of overall performance the MME seems most appealing among the three estimators considered here and this is the main contribution of our paper.},
  keywords = {Coefficient of concordance,Correlation (Statistics),Distribution (Probability theory),Estimation theory,Pitman Nearness Criterion,second order admissibility,Stochastic Domination Criterion,Symmetric functions,symmetric normal distribution},
  file = {C\:\\Users\\micro\\Zotero\\storage\\UQLD9F4L\\Pal and Lim - 2004 - On intra-class correlation coefficient estimation.pdf}
}

@article{peters2003,
  title = {Comparison of Methods for Analysing Cluster Randomized Trials: An Example Involving a Factorial Design},
  shorttitle = {Comparison of Methods for Analysing Cluster Randomized Trials},
  author = {Peters, TJ and Richards, SH and Bankhead, CR and Ades, AE and Sterne, JAC},
  year = {2003},
  month = oct,
  journal = {International Journal of Epidemiology},
  volume = {32},
  number = {5},
  pages = {840--846},
  issn = {0300-5771},
  doi = {10.1093/ije/dyg228},
  abstract = {Background Studies involving clustering effects are common, but there is little consistency in their analysis. Various analytical methods were compared for a factorial cluster randomized trial (CRT) of two primary care-based interventions designed to increase breast screening attendance.Methods Three cluster-level and five individual-level options were compared in respect of log odds ratios of attendance and their standard errors (SE), for the two intervention effects and their interaction. Cluster-level analyses comprised: (C1) unweighted regression of practice log odds; (C2) regression of log odds weighted by their inverse variance; (C3) random-effects meta-regression of log odds with practice as a random effect. Individual-level analyses comprised: (I1) standard logistic regression ignoring clustering; (I2) robust SE; (I3) generalized estimating equations; (I4) random-effects logistic regression; (I5) Bayesian random-effects logistic regression. Adjustments for stratification and baseline variables were investigated.Results As expected, method I1 was highly anti-conservative. The other, valid, methods exhibited considerable differences in parameter estimates and standard errors, even between the various random-effects methods based on the same statistical model. Method I4 was particularly sensitive to between-cluster variation and was computationally stable only after controlling for baseline uptake.Conclusions Commonly used methods for the analysis of CRT can give divergent results. Simulation studies are needed to compare results from different methods in situations typical of cluster trials but when the true model parameters are known.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\ILUZF5JB\\Peters et al. - 2003 - Comparison of methods for analysing cluster random.pdf;C\:\\Users\\micro\\Zotero\\storage\\85XL8QGN\\665733.html}
}

@article{preisser2003,
  title = {An Integrated Population-Averaged Approach to the Design, Analysis and Sample Size Determination of Cluster-Unit Trials},
  author = {Preisser, John S. and Young, Mary L. and Zaccaro, Daniel J. and Wolfson, Mark},
  year = {2003},
  journal = {Statistics in Medicine},
  volume = {22},
  number = {8},
  pages = {1235--1254},
  issn = {1097-0258},
  doi = {10.1002/sim.1379},
  abstract = {While the mixed model approach to cluster randomization trials is relatively well developed, there has been less attention given to the design and analysis of population-averaged models for randomized and non-randomized cluster trials. We provide novel implementations of familiar methods to meet these needs. A design strategy that selects matching control communities based upon propensity scores, a statistical analysis plan for dichotomous outcomes based upon generalized estimating equations (GEE) with a design-based working correlation matrix, and new sample size formulae are applied to a large non-randomized study to reduce underage drinking. The statistical power calculations, based upon Wald tests for summary statistics, are special cases of a general power method for GEE. Copyright \textcopyright{} 2003 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2003 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {cluster randomization,generalized estimating equations,logistic models,power},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.1379},
  file = {C\:\\Users\\micro\\Zotero\\storage\\TS4ZHXJG\\Preisser et al. - 2003 - An integrated population-averaged approach to the .pdf}
}

@article{preisser2007,
  title = {The {{Importance}} and {{Role}} of {{Intracluster Correlations}} in {{Planning Cluster Trials}}},
  author = {Preisser, John S. and Reboussin, Beth A. and Song, Eun-Young and Wolfson, Mark},
  year = {2007},
  month = sep,
  journal = {Epidemiology (Cambridge, Mass.)},
  volume = {18},
  number = {5},
  pages = {552--560},
  issn = {1044-3983},
  abstract = {There is increasing recognition of the critical role of intracluster correlations of health behavior outcomes in cluster intervention trials. This study examines the estimation, reporting, and use of intracluster correlations in planning cluster trials. We use an estimating equations approach to estimate the intracluster correlations corresponding to the multiple-time-point nested cross-sectional design. Sample size formulae incorporating 2 types of intracluster correlations are examined for the purpose of planning future trials. The traditional intracluster correlation is the correlation among individuals within the same community at a specific time point. A second type is the correlation among individuals within the same community at different time points. For a ``time \texttimes{} condition'' analysis of a pretest\textendash posttest nested cross-sectional trial design, we show that statistical power considerations based upon a posttest-only design generally are not an adequate substitute for sample size calculations that incorporate both types of intracluster correlations. Estimation, reporting, and use of intracluster correlations are illustrated for several dichotomous measures related to underage drinking collected as part of a large nonrandomized trial to enforce underage drinking laws in the United States from 1998 to 2004.},
  pmcid = {PMC2567827},
  pmid = {17879427},
  file = {C\:\\Users\\micro\\Zotero\\storage\\F2JNS2NF\\Preisser et al. - 2007 - The Importance and Role of Intracluster Correlatio.pdf}
}

@article{pustejovsky_meta-analysis_2022,
	title = {Meta-analysis with {Robust} {Variance} {Estimation}: {Expanding} the {Range} of {Working} {Models}},
	volume = {23},
	copyright = {© Society for Prevention Research 2021.},
	issn = {13894986},
	shorttitle = {Meta-analysis with {Robust} {Variance} {Estimation}},
	url = {https://www.proquest.com/docview/2646019038/abstract/A8708283A1A04079PQ/1},
	doi = {10.1007/s11121-021-01246-3},
	abstract = {In prevention science and related fields, large meta-analyses are common, and these analyses often involve dependent effect size estimates. Robust variance estimation (RVE) methods provide a way to include all dependent effect sizes in a single meta-regression model, even when the exact form of the dependence is unknown. RVE uses a working model of the dependence structure, but the two currently available working models are limited to each describing a single type of dependence. Drawing on flexible tools from multilevel and multivariate meta-analysis, this paper describes an expanded range of working models, along with accompanying estimation methods, which offer potential benefits in terms of better capturing the types of data structures that occur in practice and, under some circumstances, improving the efficiency of meta-regression estimates. We describe how the methods can be implemented using existing software (the “metafor” and “clubSandwich” packages for R), illustrate the proposed approach in a meta-analysis of randomized trials on the effects of brief alcohol interventions for adolescents and young adults, and report findings from a simulation study evaluating the performance of the new methods.},
	language = {English},
	number = {3},
	urldate = {2023-07-17},
	journal = {Prevention Science},
	author = {Pustejovsky, James E. and Link to external site, this link will open in a new window and Tipton, Elizabeth},
	month = apr,
	year = {2022},
	note = {Num Pages: 425-438
Place: New York, Netherlands
Publisher: Springer Nature B.V.},
	keywords = {Meta-analysis, Dependent effect sizes, Meta-regression, Robust standard errors},
	pages = {425--438},
	file = {Submitted Version:C\:\\Users\\betha\\Zotero\\storage\\AKJ3T5Q9\\Pustejovsky et al. - 2022 - Meta-analysis with Robust Variance Estimation Exp.pdf:application/pdf},
}



@article{puzioDifferentiatedLiteracyInstruction2020a,
  title = {Differentiated {{Literacy Instruction}}: {{Boondoggle}} or {{Best Practice}}?},
  shorttitle = {Differentiated {{Literacy Instruction}}},
  author = {Puzio, Kelly and Colby, Glenn T. and {Algeo-Nichols}, Dana},
  year = {2020},
  month = aug,
  journal = {Review of Educational Research},
  volume = {90},
  number = {4},
  pages = {459--498},
  publisher = {{American Educational Research Association}},
  issn = {0034-6543},
  doi = {10.3102/0034654320933536},
  abstract = {With increasingly diverse students, schools and districts are under pressure to meet rigorous standards and raise student achievement in reading and literacy. Most teachers respond by differentiating their instruction to some extent, but not all scholars and educators agree on whether differentiated instruction works. This systematic review and meta-analysis seeks to determine the effects of Tier 1 differentiation, which is provided by the general education classroom teacher, on literacy outcomes. Distinguishing between designed differentiation and interactional differentiation, the authors provide multiple examples of content, process, and product differentiation in the context of literacy instruction. Reviewing more than 20 years of literacy research, the authors located 18 studies with 25 study cohorts. Outcomes include fluency, decoding, letter-word reading, vocabulary, comprehension, and writing achievement. The overall weighted mean effect size (g) was +0.13 (p = .002) with 88\% of the individual point estimates being positive. Overall, the findings indicate that differentiated literacy instruction is an effective evidence-based practice at the elementary level. When teachers are supported to differentiate instruction, students have significantly higher literacy achievement scores, particularly for letter-word (g = +0.20, p = .014) and writing outcomes (g = +0.96, p {$<$} .001). The most successful programs took very different approaches to differentiation, including individualization, choice, and an alternate curriculum. However, across the studies, there was an alarming lack of information about the decision-making processes used to guide differentiation and there were no experimental or quasi-experimental studies on guided reading. This review may be helpful as schools clarify their vision for literacy differentiation.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\4K8LADQZ\\Puzio et al. - 2020 - Differentiated Literacy Instruction Boondoggle or.pdf}
}

@article{raudenbush1997,
  title = {Statistical Analysis and Optimal Design for Cluster Randomized Trials},
  author = {Raudenbush, Stephen W.},
  year = {1997},
  month = jun,
  journal = {Psychological Methods},
  volume = {2},
  number = {2},
  pages = {173--185},
  publisher = {{American Psychological Association}},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.2.2.173},
  abstract = {In many intervention studies, therapy outcome evaluations, and educational field trials, random treatment assignment of clusters rather than persons is desirable for political feasibility, logistics, or ecological validity. However, cluster randomized designs are widely regarded as lacking statistical precision. This article considers when and to what extent using a pretreatment covariate can increase experimental precision. To answer this question, the author first optimizes allocation of resources within and between clusters for the no-covariate case. Optimal sample sizes at each level depend on variation within and between clusters and on the cost of sampling at each level. Next, the author considers optimal allocation when a covariate is added. In this case, the explanatory power of the covariate at each level becomes highly relevant for choosing optimal sample sizes. A key conclusion is that statistical analysis that fully uses information about the covariate-outcome relationship can substantially increase the efficiency of the cluster randomized trial, especially when the cost of sampling clusters is high and the covariate accounts for substantial variation between clusters. Recent multilevel studies indicate that these conditions are common. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Analysis of Covariance,Cluster Analysis,covariate–outcome relationship information use in cluster randomized designs,precision of randomized trial analyses,Statistical Probability,Statistical Sample Parameters,Statistical Samples},
  file = {C\:\\Users\\micro\\Zotero\\storage\\2LT4QF3S\\Raudenbush - 1997 - Statistical analysis and optimal design for cluste.pdf}
}

@article{raudenbush1997a,
  title = {Statistical Analysis and Optimal Design for Cluster Randomized Trials},
  author = {Raudenbush, Stephen W.},
  year = {1997},
  month = jun,
  journal = {Psychological Methods},
  volume = {2},
  number = {2},
  pages = {173--185},
  publisher = {{American Psychological Association}},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.2.2.173},
  abstract = {In many intervention studies, therapy outcome evaluations, and educational field trials, random treatment assignment of clusters rather than persons is desirable for political feasibility, logistics, or ecological validity. However, cluster randomized designs are widely regarded as lacking statistical precision. This article considers when and to what extent using a pretreatment covariate can increase experimental precision. To answer this question, the author first optimizes allocation of resources within and between clusters for the no-covariate case. Optimal sample sizes at each level depend on variation within and between clusters and on the cost of sampling at each level. Next, the author considers optimal allocation when a covariate is added. In this case, the explanatory power of the covariate at each level becomes highly relevant for choosing optimal sample sizes. A key conclusion is that statistical analysis that fully uses information about the covariate-outcome relationship can substantially increase the efficiency of the cluster randomized trial, especially when the cost of sampling clusters is high and the covariate accounts for substantial variation between clusters. Recent multilevel studies indicate that these conditions are common. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Analysis of Covariance,Cluster Analysis,covariate–outcome relationship information use in cluster randomized designs,precision of randomized trial analyses,Statistical Probability,Statistical Sample Parameters,Statistical Samples},
  file = {C\:\\Users\\micro\\Zotero\\storage\\YLJFFUS2\\Raudenbush - 1997 - Statistical analysis and optimal design for cluste.pdf}
}

@article{raudenbush2009analyzing,
  title = {Analyzing Effect Sizes: {{Random-effects}} Models},
  author = {Raudenbush, Stephen W},
  year = {2009},
  journal = {The handbook of research synthesis and meta-analysis},
  volume = {2},
  pages = {295--316}
}

@misc{raudenbush2019,
  title = {{{HLM}} 8 for {{Windows}}},
  author = {Raudenbush, S W and Bryk, A S and Cheong, Y F and Congdon, R},
  year = {2019},
  address = {{Skokie, IL}},
  howpublished = {Scientific Software International, Inc}
}

@book{raudenbushHierarchicalLinearModels2002,
  title = {Hierarchical {{Linear Models}}: {{Applications}} and {{Data Analysis Methods}}},
  shorttitle = {Hierarchical {{Linear Models}}},
  author = {Raudenbush, Stephen W. and Bryk, Anthony S.},
  year = {2002},
  publisher = {{SAGE}},
  abstract = {Popular in the First Edition for its rich, illustrative examples and lucid explanations of the theory and use of hierarchical linear models (HLM), the book has been reorganized into four parts with four completely new chapters. The first two parts, Part I on "The Logic of Hierarchical Linear Modeling" and Part II on "Basic Applications" closely parallel the first nine chapters of the previous edition with significant expansions and technical clarifications, such as:  * An intuitive introductory summary of the basic procedures for estimation and inference used with HLM models that only requires a minimal level of mathematical sophistication in Chapter 3* New section on multivariate growth models in Chapter 6 * A discussion of research synthesis or meta-analysis applications in Chapter 7* Data analytic advice on centering of level-1 predictors and new material on plausible value intervals and robust standard estimators},
  googlebooks = {uyCV0CNGDLQC},
  isbn = {978-0-7619-1904-9},
  langid = {english},
  keywords = {Mathematics / Linear \& Nonlinear Programming,Medical / General,Reference / Research,Social Science / Research},
  file = {C\:\\Users\\micro\\Zotero\\storage\\EI6U8R5U\\Raudenbush and Bryk - 2002 - Hierarchical Linear Models Applications and Data .pdf}
}

@article{raykov2015,
  title = {Intraclass {{Correlation Coefficients}} in {{Hierarchical Design Studies With Discrete Response Variables}}: {{A Note}} on a {{Direct Interval Estimation Procedure}}},
  shorttitle = {Intraclass {{Correlation Coefficients}} in {{Hierarchical Design Studies With Discrete Response Variables}}},
  author = {Raykov, Tenko and Marcoulides, George A.},
  year = {2015},
  month = dec,
  journal = {Educational and Psychological Measurement},
  volume = {75},
  number = {6},
  pages = {1063--1070},
  publisher = {{SAGE Publications Inc}},
  issn = {0013-1644},
  doi = {10.1177/0013164414564052},
  abstract = {A latent variable modeling procedure that can be used to evaluate intraclass correlation coefficients in two-level settings with discrete response variables is discussed. The approach is readily applied when the purpose is to furnish confidence intervals at prespecified confidence levels for these coefficients in setups with binary or ordinal outcome measures and nesting of subjects within higher order units. The method can aid educational and behavioral researchers in their study of sources of observed outcome variability and model choice considerations in multilevel settings, and is illustrated with empirical survey data.},
  langid = {english},
  keywords = {hierarchical data,interval estimation,intraclass correlation coefficient,multilevel modeling},
  file = {C\:\\Users\\micro\\Zotero\\storage\\CVKYLBF7\\Raykov and Marcoulides - 2015 - Intraclass Correlation Coefficients in Hierarchica.pdf}
}

@manual{rcoreteam2020a,
  type = {Manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2020},
  address = {{Vienna, Austria}},
  organization = {{R Foundation for Statistical Computing}}
}

@phdthesis{rhoads,
  title = {Utilizing External Information about the Covariance Structure in Experiments with Clustering},
  author = {Rhoads, Christopher},
  year = {2008},
  month = jun,
  address = {{United States -- Illinois}},
  abstract = {The use of cluster randomized experiments to study the effects of treatments on groups of subjects has increased in recent years. Many of these experiments lack the necessary statistical power to detect practically meaningful effects of treatment. One method for improving power in cluster randomized experiments that has been advanced is to use external information about the intracluster correlation coefficient (ICC) to increase the degrees of freedom available to estimate the variance of the estimated treatment effect. This thesis contributes to the discussion about improving power in cluster randomized experiments through the use of external information about the ICC in the following fashion. First, I point out that existing proposals for incorporating external information about the ICC into an analysis of a cluster randomized experiment do not have well studied size and power properties. Secondly, I derive a method for studying the small sample size and power of existing proposals and show that none of these proposals can guarantee that the hypothesis test procedure proposed achieves the nominal type I error rate in small samples. Thirdly, I derive bounds on the amount of power improvement possible through the use of external information about the ICC. Finally, I propose a new method for incorporating external information about the ICC into an analysis of a cluster randomized experiment which results in a hypothesis test that can be shown to have the nominal level in small samples. I show how this method can be extended in order to be utilized for hypothesis testing in any linear model with normal errors and a non-diagonal covariance matrix.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9780549501923},
  langid = {english},
  school = {Northwestern University},
  keywords = {Cluster randomized experiments,Clustering,Covariance,External information,Intracluster correlation coefficients,Pure sciences},
  file = {C\:\\Users\\micro\\Zotero\\storage\\4GTC59K9\\Rhoads - Utilizing external information about the covarianc.pdf}
}

@article{rhoads2014,
  title = {Under {{What Circumstances Does External Knowledge About}} the {{Correlation Structure Improve Power}} in {{Cluster Randomized Designs}}?},
  author = {Rhoads, Christopher},
  year = {2014},
  month = apr,
  journal = {Journal of Research on Educational Effectiveness},
  volume = {7},
  number = {2},
  pages = {205--224},
  issn = {19345747},
  doi = {10.1080/19345747.2013.848962},
  abstract = {Recent publications have drawn attention to the idea of utilizing prior information about the correlation structure to improve statistical power in cluster randomized experiments. Because power in cluster randomized designs is a function of many different parameters, it has been difficult for applied researchers to discern a simple rule explaining when prior correlation information will substantially improve power. This article provides bounds on the maximum possible improvement in power as a function of a single parameter, the number of clusters at the highest level of a multilevel experiment. The maximum improvement in power is less than 0.05 unless the number of clusters at the highest level is less than 20. Thus, the utility of using prior correlation information is limited to experiments with very small cluster-level sample sizes. Situations where small cluster-level sample sizes could still result in experiments with good statistical power are discussed, as is the relative utility of prior information about intracluster correlations as compared with covariate information that can explain cluster level variability in the outcome.},
  keywords = {Analysis of variance,Cluster analysis (Statistics),Correlation (Statistics),Statistical power analysis,Statistical sampling},
  file = {C\:\\Users\\micro\\Zotero\\storage\\65SCZ8JI\\Rhoads - 2014 - Under What Circumstances Does External Knowledge A.pdf}
}

@article{rhoads2017,
  title = {Coherent {{Power Analysis}} in {{Multilevel Studies Using Parameters From Surveys}}},
  author = {Rhoads, Christopher},
  year = {2017},
  month = apr,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {42},
  number = {2},
  pages = {166--194},
  publisher = {{American Educational Research Association}},
  issn = {1076-9986},
  doi = {10.3102/1076998616675607},
  abstract = {Researchers designing multisite and cluster randomized trials of educational interventions will usually conduct a power analysis in the planning stage of the study. To conduct the power analysis, researchers often use estimates of intracluster correlation coefficients and effect sizes derived from an analysis of survey data. When there is heterogeneity in treatment effects across the clusters in the study, these parameters will need to be adjusted to produce an accurate power analysis for a hierarchical trial design. The relevant adjustment factors are derived and presented in the current article. The adjustment factors depend upon the covariance between treatment effects and cluster-specific average values of the outcome variable, illustrating the need for better information about this parameter. The results in the article also facilitate understanding of the relative power of multisite and cluster randomized studies conducted on the same population by showing how the parameters necessary to compute power in the two types of designs are related. This is accomplished by relating parameters defined by linear mixed model specifications to parameters defined in terms of potential outcomes.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\PAD5SMWD\\Rhoads - 2017 - Coherent Power Analysis in Multilevel Studies Usin.pdf}
}

@article{rooney1996,
  title = {A {{Meta-Analysis}} of {{Smoking Prevention Programs After Adjustment}} for {{Errors}} in the {{Unit}} of {{Analysis}}},
  author = {Rooney, Brenda L. and Murray, David M.},
  year = {1996},
  month = feb,
  journal = {Health Education Quarterly},
  volume = {23},
  number = {1},
  pages = {48--64},
  publisher = {{SAGE Publications Inc}},
  issn = {0195-8402},
  doi = {10.1177/109019819602300104},
  abstract = {This article presents the results of a meta-analysis designed to test the prevailing view that we largely understand why adolescents start to smoke and how to delay it. This view has developed even though none of the major reviews of the last 12 years has adjusted for the important methodological problems that all of those reviews identified as common in the published literature. School-based smoking prevention programs based on peer or social-type programs, published between 1974 and 1991, were included in this meta-analysis. Treatment characteristics were used to predict an effect size after adjustment for study design and population characteristics, and in particular, after a post hoc correction for errors in the original unit of analysis. The results suggest that the average effect for peer or social-type programs is likely to be quite limited in magnitude, and that the reduction in smoking may be only 0.10 standard deviation units, or perhaps 5\%. Even under optimal conditions, the reduction in smoking may be only 0.50 to 0.75 standard deviation units, or perhaps 20\%-30\%.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\CQ72NWJT\\Rooney and Murray - 1996 - A Meta-Analysis of Smoking Prevention Programs Aft.pdf}
}

@article{rosseel2012,
  title = {{{lavaan}}: {{An R}} Package for Structural Equation Modeling},
  author = {Rosseel, Yves},
  year = {2012},
  journal = {Journal of Statistical Software},
  volume = {48},
  number = {2},
  pages = {1--36}
}

@article{rotondi2009,
  title = {Sample {{Size Estimation}} in {{Cluster Randomized Educational Trials}}: {{An Empirical Bayes Approach}}},
  shorttitle = {Sample {{Size Estimation}} in {{Cluster Randomized Educational Trials}}},
  author = {Rotondi, Michael A. and Donner, Allan},
  year = {2009},
  month = jun,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {34},
  number = {2},
  pages = {229--237},
  publisher = {{American Educational Research Association}},
  issn = {1076-9986},
  doi = {10.3102/1076998609332756},
  abstract = {The educational field has now accumulated an extensive literature reporting on values of the intraclass correlation coefficient, a parameter essential to determining the required size of a planned cluster randomized trial. We propose here a simple simulation-based approach including all relevant information that can facilitate this task. An example and corresponding computer code is attached.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\Z4SF83TF\\Rotondi and Donner - 2009 - Sample Size Estimation in Cluster Randomized Educa.pdf}
}

@article{sahai,
  title = {A {{BIBLIOGRAPHY ON VARIANCE COMPONENTS AN INTRODUCTION AND AN UPDATE}}: 1984-2002},
  author = {Sahai, Hardeo and Khurshid, Anwer},
  pages = {149},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\5A5WNRHG\\Sahai and Khurshid - A BIBLIOGRAPHY ON VARIANCE COMPONENTS AN INTRODUCT.pdf}
}

@article{searle1971linear,
  title = {Linear Models John Wiley \& Sons},
  author = {Searle, SR},
  year = {1971},
  journal = {Inc., New York-London-Sydney-Toronto}
}

@book{searle1992,
  title = {Variance Components},
  author = {Searle, Shayle R. and Casella, George and McCulloch, Charles E.},
  year = {1992},
  series = {Wiley Series in Probability and Mathematical Statistics. {{Applied}} Probability and Statistics},
  publisher = {{Wiley}},
  address = {{New York}},
  isbn = {0-471-62162-5},
  langid = {english},
  lccn = {91018067},
  keywords = {Analysis of variance}
}

@book{searle2009variance,
  title = {Variance Components},
  author = {Searle, Shayle R and Casella, George and McCulloch, Charles E},
  year = {2009},
  volume = {391},
  publisher = {{John Wiley \& Sons}}
}

@article{shan2020,
  title = {Estimation of Bias-Corrected Intraclass Correlation Coefficient for Unbalanced Clustered Studies with Continuous Outcomes},
  author = {Shan, Guogen},
  year = {2020},
  month = aug,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {0},
  number = {0},
  pages = {1--10},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610918.2020.1811332},
  abstract = {Intraclass correlation coefficient for data in a clustered study is traditionally estimated from a one-way random-effects model. This model assumes normality for the random cluster effect and the residual effect. When the normality assumption is questionable, we find that the estimated correlation could be much below the nominal level when data are highly skewed or data have low kurtosis. We develop a bias-corrected estimator based on the approach by Thomas and Hultquist for a study with unbalanced cluster sizes. For multivariate normal data or non-normal data with moderate skewness, we compare the performance of the new bias-corrected estimator with two existing estimators with regards to accuracy and precision. When correlation is small, the existing ANOVA estimator works well. When correlation is medium to large, the proposed new estimator has the correlation close to the nominal level, and its mean squared error is smaller than others.},
  keywords = {ANOVA,Bias corrected,Clustered study,Inter-rater study,Intraclass correlation coefficient,Skewness and kurtosis},
  annotation = {\_eprint: https://doi.org/10.1080/03610918.2020.1811332},
  file = {C\:\\Users\\micro\\Zotero\\storage\\C82QR288\\Shan - 2020 - Estimation of bias-corrected intraclass correlatio.pdf;C\:\\Users\\micro\\Zotero\\storage\\4I6TI37R\\03610918.2020.html}
}

@article{shieh2014,
  title = {Sample Size Requirements for the Design of Reliability Studies: Precision Consideration},
  shorttitle = {Sample Size Requirements for the Design of Reliability Studies},
  author = {Shieh, Gwowen},
  year = {2014},
  month = sep,
  journal = {Behavior Research Methods},
  volume = {46},
  number = {3},
  pages = {808--822},
  issn = {1554-3528},
  doi = {10.3758/s13428-013-0415-1},
  abstract = {In multilevel modeling, the intraclass correlation coefficient based on the one-way random-effects model is routinely employed to measure the reliability or degree of resemblance among group members. To facilitate the advocated practice of reporting confidence intervals in future reliability studies, this article presents exact sample size procedures for precise interval estimation of the intraclass correlation coefficient under various allocation and cost structures. Although the suggested approaches do not admit explicit sample size formulas and require special algorithms for carrying out iterative computations, they are more accurate than the closed-form formulas constructed from large-sample approximations with respect to the expected width and assurance probability criteria. This investigation notes the deficiency of existing methods and expands the sample size methodology for the design of reliability studies that have not previously been discussed in the literature.},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\3MGU8AU4\\Shieh - 2014 - Sample size requirements for the design of reliabi.pdf}
}

@article{smith1957,
  title = {On the {{Estimation}} of {{Intraclass Correlation}}},
  author = {Smith, C. a. B.},
  year = {1957},
  journal = {Annals of Human Genetics},
  volume = {21},
  number = {4},
  pages = {363--373},
  issn = {1469-1809},
  doi = {10.1111/j.1469-1809.1972.tb00291.x},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1972.tb00291.x},
  file = {C\:\\Users\\micro\\Zotero\\storage\\JNKUFP87\\j.1469-1809.1972.tb00291.html}
}

@article{snijders1993,
  title = {Standard {{Errors}} and {{Sample Sizes}} for {{Two-Level Research}}},
  author = {Snijders, Tom A. B. and Bosker, Roel J.},
  year = {1993},
  journal = {Journal of Educational Statistics},
  volume = {18},
  number = {3},
  pages = {237--259},
  publisher = {{[Sage Publications, Inc., American Educational Research Association, American Statistical Association]}},
  issn = {0362-9791},
  doi = {10.2307/1165134},
  abstract = {The hierarchical linear model approach to a two-level design is considered, some variables at the lower level having fixed and others having random regression coefficients. An approximation is derived to the covariance matrix of the estimators of the fixed regression coefficients (for variables at the lower and the higher level) under the assumption that the sample sizes at either level are large enough. This covariance matrix is expressed as a function of parameters occurring in the model. If a research planner can make a reasonable guess as to these parameters, this approximation can be used as a guide to the choice of sample sizes at either level.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\LBS5APEF\\Snijders and Bosker - 1993 - Standard Errors and Sample Sizes for Two-Level Res.pdf}
}

@article{spiegelhalter2001,
  title = {Bayesian Methods for Cluster Randomized Trials with Continuous Responses},
  author = {Spiegelhalter, David J.},
  year = {2001},
  journal = {Statistics in Medicine},
  volume = {20},
  number = {3},
  pages = {435--452},
  issn = {1097-0258},
  doi = {10.1002/1097-0258(20010215)20:3<435::AID-SIM804>3.0.CO;2-E},
  abstract = {Bayesian methods for cluster randomized trials extend the random-effects formulation by allowing both the use of external evidence on parameters and straightforward relaxation of the standard normality and constant variance assumptions. Care is required in specifying prior distributions on variance components, and a number of different options are explored with implied prior distributions for other parameters given in closed form. Markov chain Monte Carlo (MCMC) methods permit the fitting of very general models and the introduction of parameter uncertainty into power calculations. We illustrate these ideas using a published example in which general practices were randomized to intervention or control, and show that different choices of supposedly `non-informative' prior distributions can have substantial influence on conclusions. We also illustrate the use of forward simulation methods in power calculations with uncertainty on multiple inputs. Bayesian methods have the potential to be very useful but guidance is required as to appropriate strategies for robust analysis. Our current experience leads us to recommend a standard `non-informative' prior distribution for the within-cluster sampling variance, and an independent prior on the intraclass correlation coefficient (ICC). The latter may exploit background evidence or, as a reference analysis, be a uniform ICC or a `uniform shrinkage' prior. Copyright \textcopyright{} 2001 John Wiley \& Sons, Ltd.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/1097-0258\%2820010215\%2920\%3A3\%3C435\%3A\%3AAID-SIM804\%3E3.0.CO\%3B2-E},
  file = {C\:\\Users\\micro\\Zotero\\storage\\PWCFCFBX\\Spiegelhalter - 2001 - Bayesian methods for cluster randomized trials wit.pdf;C\:\\Users\\micro\\Zotero\\storage\\UCTLJFC9\\1097-0258(20010215)203435AID-SIM8043.0.html}
}

@article{spilke2005,
  title = {A {{Simulation Study}} on {{Tests}} of {{Hypotheses}} and {{Confidence Intervals}} for {{Fixed Effects}} in {{Mixed Models}} for {{Blocked Experiments}} with {{Missing Data}}},
  author = {Spilke, Joachim and Piepho, Hans-Peter and Hu, Xiyuan},
  year = {2005},
  journal = {Journal of Agricultural, Biological, and Environmental Statistics},
  volume = {10},
  number = {3},
  pages = {374--389},
  publisher = {{[International Biometric Society, Springer]}},
  issn = {1085-7117},
  abstract = {This article considers the analysis of experiments with missing data from various experimental designs frequently used in agricultural research (randomized complete blocks, split plots, strip plots). We investigate the small sample properties of REML-based Wald-type F tests using linear mixed models. Several methods for approximating the denominator degrees of freedom are employed, all of which are available with the MIXED procedure of the SAS System (8.02). The simulation results show that the Kenward-Roger method provides the best control of the Type I error rate and is not inferior to other methods in terms of power.}
}

@misc{spss,
  title = {{{IBM SPSS}} Statistics for Windows},
  author = {{IBM Corp.}},
  year = {2017},
  address = {{Armonk, NY: IBM Corp}}
}

@phdthesis{stockford2009,
  title = {Meta-Analysis of Intraclass Correlation Coefficients from Multilevel Models of Educational Achievement},
  author = {Stockford, Shawn M.},
  year = {2009},
  address = {{United States -- Arizona}},
  abstract = {A meta-analysis of 176 intraclass correlation coefficients (ICCs) from 63 studies employing multilevel models to analyze K-12 student achievement in the United States was conducted by applying hierarchical linear models to examine relationships between research design characteristics and the magnitude of educational cluster effects. Average ICCs from two-level models using either classroom or school as the cluster unit were about .22, suggesting that a substantial portion of variability in student exam scores is attributable to a combination of classrooms and schools. Sixteen ICCs from three-level models allowed for separate estimates of classroom and school cluster effects, yielding average classroom- and school-level effects of .11 and .15, respectively. Distributions of cluster effects from two-level models were found to vary significantly around the mean ICC, while the distributions of separate classroom effects and school effects from three-level models did not. Conditional model results for distributions of two-level model ICCs showed that achievement scores from mathematics exams tended to yield higher cluster effects than those from literacy exams. Cluster effects for middle-school grade samples tended to be higher than those for other grade levels. Samples with larger average classroom sizes tended to have lower ICCs. Findings imply educational researchers should be cautious with analyses that combine scores across domains or grade levels because their cluster effects are likely to be heterogeneous. The influence of class size on the strength of cluster effects should be investigated with a larger sample; if replicated, the effect may have implications for the design of studies involving class size research. Greater awareness of the impact of ignored cluster levels in multilevel models and their influence on estimated random effects is recommended.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9781109331219},
  langid = {english},
  school = {Arizona State University},
  keywords = {Achievement,Cluster effects,Education,Meta-analysis,Multilevel models,School effects},
  file = {C\:\\Users\\micro\\Zotero\\storage\\V7QTZIYS\\Stockford - 2009 - Meta-analysis of intraclass correlation coefficien.pdf}
}

@misc{SupplementWhatWorks,
  title = {Supplement to the {{What Works Clearninghouse Procedures Handbook}}, {{Version}} 4.1},
  howpublished = {https://ies.ed.gov/ncee/wwc/Handbooks},
  file = {C\:\\Users\\micro\\Zotero\\storage\\ZZBQNC36\\WWC  Handbooks and Other Resources.pdf;C\:\\Users\\micro\\Zotero\\storage\\2DCBRE93\\Handbooks.html}
}

@article{swiger1964,
  title = {The {{Variance}} of {{Intraclass Correlation Involving Groups}} with {{One Observation}}},
  author = {Swiger, L. A. and Harvey, W. R. and Everson, D. O. and Gregory, K. E.},
  year = {1964},
  month = dec,
  journal = {Biometrics},
  volume = {20},
  number = {4},
  pages = {818},
  issn = {0006341X},
  doi = {10.2307/2528131},
  abstract = {An approximate formula is derived for the variance of int,raclass correlation when unequal numbers of observatioils per group occur. The effect on the variance of t of adding groups with single observations is examined using the formula and results obtained by empirically generating data on a c o m p \textasciitilde{} t \textasciitilde{} e rT. he empirical results indicate that the approximate formula is satisfactory over the range of numbers used. Adding a group witjh fewer than the average number of observations per group tends to reduce Tit by increasing the degrees of freedom for groups by one, but tends t o increase V t by decreasing the average precision of estimating group means. The net effect can be either negative or positive, depending on t, s and the ni's. Robertson [I9621 pointed out that, when the ratio of the between group mean square t o the within group mean square is small, exclusion of groups below half the average size will reduce the variance of the between group component. He further suggested a method for combining estimates of the between group component when n is highly variable.},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\MZDX6E8P\\Swiger et al. - 1964 - The Variance of Intraclass Correlation Involving G.pdf}
}

@article{tanner-smith2014,
  title = {Robust Variance Estimation with Dependent Effect Sizes: Practical Considerations Including a Software Tutorial in {{Stata}} and Spss},
  shorttitle = {Robust Variance Estimation with Dependent Effect Sizes},
  author = {{Tanner-Smith}, Emily E. and Tipton, Elizabeth},
  year = {2014},
  journal = {Research Synthesis Methods},
  volume = {5},
  number = {1},
  pages = {13--30},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1091},
  abstract = {Methodologists have recently proposed robust variance estimation as one way to handle dependent effect sizes in meta-analysis. Software macros for robust variance estimation in meta-analysis are currently available for Stata (StataCorp LP, College Station, TX, USA) and spss (IBM, Armonk, NY, USA), yet there is little guidance for authors regarding the practical application and implementation of those macros. This paper provides a brief tutorial on the implementation of the Stata and spss macros and discusses practical issues meta-analysts should consider when estimating meta-regression models with robust variance estimates. Two example databases are used in the tutorial to illustrate the use of meta-analysis with robust variance estimates. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {dependent effect sizes,robust variance estimation,software tutorial},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1091},
  file = {C\:\\Users\\micro\\Zotero\\storage\\5RU2CK7L\\Tanner-Smith and Tipton - 2014 - Robust variance estimation with dependent effect s.pdf;C\:\\Users\\micro\\Zotero\\storage\\7532JPIM\\jrsm.html}
}

@article{tanner-smith2016,
  title = {Handling {{Complex Meta-analytic Data Structures Using Robust Variance Estimates}}: A {{Tutorial}} in {{R}}},
  shorttitle = {Handling {{Complex Meta-analytic Data Structures Using Robust Variance Estimates}}},
  author = {{Tanner-Smith}, Emily E. and Tipton, Elizabeth and Polanin, Joshua R.},
  year = {2016},
  month = mar,
  journal = {Journal of Developmental and Life-Course Criminology},
  volume = {2},
  number = {1},
  pages = {85--112},
  issn = {2199-465X},
  doi = {10.1007/s40865-016-0026-5},
  abstract = {Identifying and understanding causal risk factors for crime over the life-course is a key area of inquiry in developmental criminology. Prospective longitudinal studies provide valuable information about the relationships between risk factors and later criminal offending. Meta-analyses that synthesize findings from these studies can summarize the predictive strength of different risk factors for crime, and offer unique opportunities for examining the developmental variability of risk factors. Complex data structures are common in such meta-analyses, whereby primary studies provide multiple (dependent) effect sizes.},
  langid = {english},
  file = {C\:\\Users\\micro\\Zotero\\storage\\P3856AB7\\Tanner-Smith et al. - 2016 - Handling Complex Meta-analytic Data Structures Usi.pdf}
}

@article{taylor2021,
  title = {Promoting {{Knowledge Accumulation About Intervention Effects}}: {{Exploring Strategies}} for {{Standardizing Statistical Approaches}} and {{Effect Size Reporting}}},
  shorttitle = {Promoting {{Knowledge Accumulation About Intervention Effects}}},
  author = {Taylor, Joseph A. and Pigott, Terri and Williams, Ryan},
  year = {2021},
  month = oct,
  journal = {Educational Researcher},
  pages = {0013189X211051319},
  publisher = {{American Educational Research Association}},
  issn = {0013-189X},
  doi = {10.3102/0013189X211051319},
  abstract = {Toward the goal of more rapid knowledge accumulation via better meta-analyses, this article explores statistical approaches intended to increase the precision and comparability of effect sizes from education research. The featured estimate of the proposed approach is a standardized mean difference effect size whose numerator is a mean difference that has been adjusted for baseline differences in the outcome measure, at a minimum, and whose denominator is the total variance. The article describes the utility and efficiency of covariate adjustment through baseline measures and the need to standardize effects on a total variance that accounts for variation at multiple levels. As computation of the total variance can be complex in multilevel studies, a shiny application is provided to assist with computation of the total variance and subsequent effect size. Examples are provided for how to interpret and input the required calculator inputs.},
  langid = {english},
  keywords = {educational policy,effect size,evaluation,experimental design,hierarchical linear modeling,meta-analysis,program evaluation,quasi-experimental analysis,regression analyses,validity/reliability},
  file = {C\:\\Users\\micro\\Zotero\\storage\\8GT87FLI\\Taylor et al. - 2021 - Promoting Knowledge Accumulation About Interventio.pdf}
}

@article{tipton2015,
  title = {Small-{{Sample Adjustments}} for {{Tests}} of {{Moderators}} and {{Model Fit Using Robust Variance Estimation}} in {{Meta-Regression}}},
  author = {Tipton, Elizabeth and Pustejovsky, James E.},
  year = {2015},
  month = dec,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {40},
  number = {6},
  pages = {604--634},
  publisher = {{American Educational Research Association}},
  issn = {1076-9986},
  doi = {10.3102/1076998615606099},
  abstract = {Meta-analyses often include studies that report multiple effect sizes based on a common pool of subjects or that report effect sizes from several samples that were treated with very similar research protocols. The inclusion of such studies introduces dependence among the effect size estimates. When the number of studies is large, robust variance estimation (RVE) provides a method for pooling dependent effects, even when information on the exact dependence structure is not available. When the number of studies is small or moderate, however, test statistics and confidence intervals based on RVE can have inflated Type I error. This article describes and investigates several small-sample adjustments to F-statistics based on RVE. Simulation results demonstrate that one such test, which approximates the test statistic using Hotelling's T2 distribution, is level-{$\alpha$} and uniformly more powerful than the others. An empirical application demonstrates how results based on this test compare to the large-sample F-test.},
  langid = {english},
  keywords = {cluster robust standard errors,F-tests,meta-analysis},
  file = {C\:\\Users\\micro\\Zotero\\storage\\D569LT3Z\\Tipton and Pustejovsky - 2015 - Small-Sample Adjustments for Tests of Moderators a.pdf}
}

@article{tipton2015a,
  title = {Small Sample Adjustments for Robust Variance Estimation with Meta-Regression},
  author = {Tipton, Elizabeth},
  year = {2015},
  month = sep,
  journal = {Psychological Methods},
  volume = {20},
  number = {3},
  pages = {375--393},
  issn = {1939-1463},
  doi = {10.1037/met0000011},
  abstract = {Although primary studies often report multiple outcomes, the covariances between these outcomes are rarely reported. This leads to difficulties when combining studies in a meta-analysis. This problem was recently addressed with the introduction of robust variance estimation. This new method enables the estimation of meta-regression models with dependent effect sizes, even when the dependence structure is unknown. Although robust variance estimation has been shown to perform well when the number of studies in the meta-analysis is large, previous simulation studies suggest that the associated tests often have Type I error rates that are much larger than nominal. In this article, I introduce 6 estimators with better small sample properties and study the effectiveness of these estimators via 2 simulation studies. The results of these simulations suggest that the best estimator involves correcting both the residuals and degrees of freedom used in the robust variance estimator. These studies also suggest that the degrees of freedom depend on not only the number of studies but also the type of covariates in the meta-regression. The fact that the degrees of freedom can be small, even when the number of studies is large, suggests that these small-sample corrections should be used more generally. I conclude with an example comparing the results of a meta-regression with robust variance estimation with the results from the corrected estimator.},
  langid = {english},
  pmid = {24773356},
  keywords = {Data Interpretation; Statistical,Humans,Meta-Analysis as Topic,Regression Analysis}
}

@article{turner2006,
  title = {Constructing Intervals for the Intracluster Correlation Coefficient Using {{Bayesian}} Modelling, and Application in Cluster Randomized Trials},
  author = {Turner, Rebecca M. and Omar, Rumana Z. and Thompson, Simon G.},
  year = {2006},
  journal = {Statistics in Medicine},
  volume = {25},
  number = {9},
  pages = {1443--1456},
  issn = {1097-0258},
  doi = {10.1002/sim.2304},
  abstract = {Studies in health research are commonly carried out in clustered settings, where the individual response data are correlated within clusters. Estimation and modelling of the extent of between-cluster variation contributes to understanding of the current study and to design of future studies. It is common to express between-cluster variation as an intracluster correlation coefficient (ICC), since this measure is directly comparable across outcomes. ICCs are generally reported unaccompanied by confidence intervals. In this paper, we describe a Bayesian modelling approach to interval estimation of the ICC. The flexibility of this framework allows useful extensions which are not easily available in existing methods, for example assumptions other than Normality for continuous outcome data, adjustment for individual-level covariates and simultaneous interval estimation of several ICCs. There is also the opportunity to incorporate prior beliefs on likely values of the ICC. The methods are exemplified using data from a cluster randomized trial. Copyright \textcopyright{} 2005 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Bayesian methods,cluster randomized trials,clustered data,interval estimation,intracluster correlation coefficient},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.2304},
  file = {C\:\\Users\\micro\\Zotero\\storage\\YMMIE8KA\\Turner et al. - 2006 - Constructing intervals for the intracluster correl.pdf}
}

@article{turnerAllowingImprecisionIntracluster2004,
  title = {Allowing for Imprecision of the Intracluster Correlation Coefficient in the Design of Cluster Randomized Trials},
  author = {Turner, Rebecca M. and Prevost, A. Toby and Thompson, Simon G.},
  year = {2004},
  journal = {Statistics in Medicine},
  volume = {23},
  number = {8},
  pages = {1195--1214},
  issn = {1097-0258},
  doi = {10.1002/sim.1721},
  abstract = {The sample size required for a cluster randomized trial depends on the magnitude of the intracluster correlation coefficient (ICC). The usual sample size calculation makes no allowance for the fact that the ICC is not known precisely in advance. We develop methods which allow for the uncertainty in a previously observed ICC, using a variety of distributional assumptions. Distributions for the power are derived, reflecting this uncertainty. Further, the observed ICC in a future study will not equal its true value, and we consider the impact of this on power. We implement calculations within a Bayesian simulation approach, and provide one simplification that can be performed using simple simulation within spreadsheet software. In our examples, recognizing the uncertainty in a previous ICC estimate decreases expected power, especially when the power calculated naively from the ICC estimate is high. To protect against the possibility of low power, sample sizes may need to be very substantially increased. Recognizing the variability in the future observed ICC has little effect if prior uncertainty has already been taken into account. We show how our method can be extended to the case in which multiple prior ICC estimates are available. The methods presented in this paper can be used by applied researchers to protect against loss of power, or to choose a design which reduces the impact of uncertainty in the ICC. Copyright \textcopyright{} 2004 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2004 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Bayesian methods,cluster randomized trials,intracluster correlation coefficient,power,sample size,study design},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.1721},
  file = {C\:\\Users\\micro\\Zotero\\storage\\MF9RW8CW\\Turner et al. - 2004 - Allowing for imprecision of the intracluster corre.pdf;C\:\\Users\\micro\\Zotero\\storage\\LXHTGCRS\\sim.html}
}

@article{turnerPriorDistributionsIntracluster2005,
  title = {Prior Distributions for the Intracluster Correlation Coefficient, Based on Multiple Previous Estimates, and Their Application in Cluster Randomized Trials},
  author = {Turner, Rebecca M and Thompson, Simon G and Spiegelhalter, David J},
  year = {2005},
  month = apr,
  journal = {Clinical Trials},
  volume = {2},
  number = {2},
  pages = {108--118},
  publisher = {{SAGE Publications}},
  issn = {1740-7745},
  doi = {10.1191/1740774505cn072oa},
  abstract = {Numerous estimates for the intracluster correlation coefficient (ICC) are available in research databases and publications. When planning a cluster randomized trial, an anticipated value for the ICC is required; currently, researchers base their choice informally on the magnitude of previous ICC estimates. In this paper, we make use of the wealth of ICC information by formally constructing informative prior distributions, while acknowledging the varying relevance and precision of the estimates available. Typically, for a planned trial in a given clinical setting, multiple relevant ICC estimates are available from each of several completed studies. Our preferred model allows for the imprecision in each ICC estimate around its underlying true value and, separately, allows for the similarity of ICC values from the same study. The relevance of each previous estimate to the planned clinical setting is considered, and estimates corresponding to less relevant outcomes or population types are given less influence. We find that such downweighting can increase the precision of the anticipated ICC. In trial design, the prior distribution constructed allows uncertainty about the ICC to be acknowledged, and we describe how to choose a design that provides adequate power across the range of likely ICC values. Prior information on the ICC can also be incorporated in analysis of the trial data, when taking a Bayesian approach. The methods proposed enable available ICC information to be summarised appropriately by an informative prior distribution, which is of direct practical use in cluster randomized trials.},
  file = {C\:\\Users\\micro\\Zotero\\storage\\Z4UDK28A\\Turner et al. - 2005 - Prior distributions for the intracluster correlati.pdf}
}

@article{ukoumunne2003,
  title = {Non-Parametric Bootstrap Confidence Intervals for the Intraclass Correlation Coefficient},
  author = {Ukoumunne, Obioha C. and Davison, Anthony C. and Gulliford, Martin C. and Chinn, Susan},
  year = {2003},
  journal = {Statistics in Medicine},
  volume = {22},
  number = {24},
  pages = {3805--3821},
  issn = {1097-0258},
  doi = {10.1002/sim.1643},
  abstract = {The intraclass correlation coefficient {$\rho$}plays a key role in the design of cluster randomized trials. Estimates of {$\rho$} obtained from previous cluster trials and used to inform sample size calculation in planned trials may be imprecise due to the typically small numbers of clusters in such studies. It may be useful to quantify this imprecision. This study used simulation to compare different methods for assigning bootstrap confidence intervals to {$\rho$}for continuous outcomes from a balanced design. Data were simulated for combinations of numbers of clusters (10, 30, 50), intraclass correlation coefficients (0.001, 0.01, 0.05, 0.3) and outcome distributions (normal, non-normal continuous). The basic, bootstrap-t, percentile, bias corrected and bias corrected accelerated bootstrap intervals were compared with new methods using the basic and bootstrap-t intervals applied to a variance stabilizing transformation of {$\rho$}. The standard bootstrap methods provided coverage levels for 95 per cent intervals that were markedly lower than the nominal level for data sets with only 10 clusters, and only provided close to 95 per cent coverage when there were 50 clusters. Application of the bootstrap-t method to the variance stabilizing transformation of {$\rho$}improved upon the performance of the standard bootstrap methods, providing close to nominal coverage. Copyright \textcopyright{} 2003 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {bootstrap,confidence intervals,intraclass correlation coefficient},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.1643},
  file = {C\:\\Users\\micro\\Zotero\\storage\\VK8RXQKX\\Ukoumunne et al. - 2003 - Non-parametric bootstrap confidence intervals for .pdf;C\:\\Users\\micro\\Zotero\\storage\\SZTL8NXV\\sim.html}
}

@article{ukoumunneComparisonConfidenceInterval2002,
  title = {A Comparison of Confidence Interval Methods for the Intraclass Correlation Coefficient in Cluster Randomized Trials},
  author = {Ukoumunne, Obioha C.},
  year = {2002},
  journal = {Statistics in Medicine},
  volume = {21},
  number = {24},
  pages = {3757--3774},
  issn = {1097-0258},
  doi = {10.1002/sim.1330},
  abstract = {A Correction has been published for this article in Statistics in Medicine 23(18) 2004, 2935. This study compared different methods for assigning confidence intervals to the analysis of variance estimator of the intraclass correlation coefficient ({$\rho$}). The context of the comparison was the use of {$\rho$} to estimate the variance inflation factor when planning cluster randomized trials. The methods were compared using Monte Carlo simulations of unbalanced clustered data and data from a cluster randomized trial of an intervention to improve the management of asthma in a general practice setting. The coverage and precision of the intervals were compared for data with different numbers of clusters, mean numbers of subjects per cluster and underlying values of {$\rho$}. The performance of the methods was also compared for data with Normal and non-Normally distributed cluster specific effects. Results of the simulations showed that methods based upon the variance ratio statistic provided greater coverage levels than those based upon large sample approximations to the standard error of {$\rho$}. Searle's method provided close to nominal coverage for data with Normally distributed random effects. Adjusted versions of Searle's method to allow for lack of balance in the data generally did not improve upon it either in terms of coverage or precision. Analyses of the trial data, however, showed that limits provided by Thomas and Hultquist's method may differ from those of the other variance ratio statistic methods when the arithmetic mean differs markedly from the harmonic mean cluster size. The simulation results demonstrated that marked non-Normality in the cluster level random effects compromised the performance of all methods. Confidence intervals for the methods were generally wide relative to the underlying size of {$\rho$}suggesting that there may be great uncertainty associated with sample size calculations for cluster trials where large clusters are randomized. Data from cluster based studies with sample sizes much larger than those typical of cluster randomized trials are required to estimate {$\rho$} with a reasonable degree of precision. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {cluster randomized trials,intraclass correlation coefficient},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.1330},
  file = {C\:\\Users\\micro\\Zotero\\storage\\MITYISG5\\Ukoumunne - 2002 - A comparison of confidence interval methods for th.pdf;C\:\\Users\\micro\\Zotero\\storage\\948ESTTL\\sim.html}
}

@article{veroniki2016,
  title = {Methods to Estimate the Between-Study Variance and Its Uncertainty in Meta-Analysis},
  author = {Veroniki, Areti Angeliki and Jackson, Dan and Viechtbauer, Wolfgang and Bender, Ralf and Bowden, Jack and Knapp, Guido and Kuss, Oliver and Higgins, Julian PT and Langan, Dean and Salanti, Georgia},
  year = {2016},
  journal = {Research Synthesis Methods},
  volume = {7},
  number = {1},
  pages = {55--79},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1164},
  abstract = {Meta-analyses are typically used to estimate the overall/mean of an outcome of interest. However, inference about between-study variability, which is typically modelled using a between-study variance parameter, is usually an additional aim. The DerSimonian and Laird method, currently widely used by default to estimate the between-study variance, has been long challenged. Our aim is to identify known methods for estimation of the between-study variance and its corresponding uncertainty, and to summarise the simulation and empirical evidence that compares them. We identified 16 estimators for the between-study variance, seven methods to calculate confidence intervals, and several comparative studies. Simulation studies suggest that for both dichotomous and continuous data the estimator proposed by Paule and Mandel and for continuous data the restricted maximum likelihood estimator are better alternatives to estimate the between-study variance. Based on the scenarios and results presented in the published studies, we recommend the Q-profile method and the alternative approach based on a `generalised Cochran between-study variance statistic' to compute corresponding confidence intervals around the resulting estimates. Our recommendations are based on a qualitative evaluation of the existing literature and expert consensus. Evidence-based recommendations require an extensive simulation study where all methods would be compared under the same scenarios. \textcopyright{} 2015 The Authors. Research Synthesis Methods published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {bias,confidence interval,coverage probability,heterogeneity,mean squared error},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1164},
  file = {C\:\\Users\\micro\\Zotero\\storage\\JNWV2TQF\\Veroniki et al. - 2016 - Methods to estimate the between-study variance and.pdf;C\:\\Users\\micro\\Zotero\\storage\\D8QZPPFU\\jrsm.html}
}

@article{viechtbauer2010,
  ids = {viechtbauer2010a},
  title = {Conducting Meta-Analyses in {{R}} with the {{metafor}} Package},
  author = {Viechtbauer, Wolfgang},
  year = {2010},
  journal = {Journal of Statistical Software},
  volume = {36},
  number = {3},
  pages = {1--48}
}

@article{viechtbauer2015,
  title = {A Comparison of Procedures to Test for Moderators in Mixed-Effects Meta-Regression Models},
  author = {Viechtbauer, Wolfgang and {L{\'o}pez-L{\'o}pez}, Jos{\'e} Antonio and {S{\'a}nchez-Meca}, Julio and {Mar{\'i}n-Mart{\'i}nez}, Fulgencio},
  year = {2015},
  month = sep,
  journal = {Psychological Methods},
  series = {Meta-{{Analysis Topics}}},
  volume = {20},
  number = {3},
  pages = {360--374},
  publisher = {{American Psychological Association}},
  issn = {1082-989X},
  doi = {10.1037/met0000023},
  abstract = {Several alternative methods are available when testing for moderators in mixed-effects meta-regression models. A simulation study was carried out to compare different methods in terms of their Type I error and statistical power rates. We included the standard (Wald-type) test, the method proposed by Knapp and Hartung (2003) in 2 different versions, the Huber\textendash White method, the likelihood ratio test, and the permutation test in the simulation study. These methods were combined with 7 estimators for the amount of residual heterogeneity in the effect sizes. Our results show that the standard method, applied in most meta-analyses up to date, does not control the Type I error rate adequately, sometimes leading to overly conservative, but usually to inflated, Type I error rates. Of the different methods evaluated, only the Knapp and Hartung method and the permutation test provide adequate control of the Type I error rate across all conditions. Due to its computational simplicity, the Knapp and Hartung method is recommended as a suitable option for most meta-analyses. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {heterogeneity estimator,Meta Analysis,meta-analysis,meta-regression,moderator analysis,standardized mean difference,Statistical Estimation,Statistical Power,Testing,Type I Errors},
  file = {C\:\\Users\\micro\\Zotero\\storage\\KN2TDI9F\\Viechtbauer et al. - 2015 - A comparison of procedures to test for moderators .pdf}
}

@article{xiao2012,
  title = {Resampling Approaches for Common Intraclass Correlation Coefficients},
  author = {Xiao, Yuanhui and Liu, Jiawei and Bhandary, Madhusudan},
  year = {2012},
  month = sep,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {82},
  number = {9},
  pages = {1357--1366},
  publisher = {{Taylor \& Francis}},
  issn = {0094-9655},
  doi = {10.1080/00949655.2011.581668},
  abstract = {In this paper, we present several resampling methods for interval estimation for the common intraclass correlation coefficients. Comparisons are made on the coverage probabilities and average lengths with confidence intervals estimated by using the generalized pivots. Most of the methods proposed in this article produce confidence intervals with better probabilities and shorter average lengths than that produced by using generalized pivots.},
  keywords = {longitudinal data,maximum likelihood,model selection,multiple comparisons,non-parametric statistics},
  annotation = {\_eprint: https://doi.org/10.1080/00949655.2011.581668},
  file = {C\:\\Users\\micro\\Zotero\\storage\\B2JQEAH9\\Xiao et al. - 2012 - Resampling approaches for common intraclass correl.pdf;C\:\\Users\\micro\\Zotero\\storage\\D8HKYTUY\\00949655.2011.html}
}

@article{xu2003,
  title = {Measuring Explained Variation in Linear Mixed Effects Models},
  author = {Xu, Ronghui},
  year = {2003},
  journal = {Statistics in Medicine},
  volume = {22},
  number = {22},
  pages = {3527--3541},
  issn = {1097-0258},
  doi = {10.1002/sim.1572},
  abstract = {We generalize the well-known R2 measure for linear regression to linear mixed effects models. Our work was motivated by a cluster-randomized study conducted by the Eastern Cooperative Oncology Group, to compare two different versions of informed consent document. We quantify the variation in the response that is explained by the covariates under the linear mixed model, and study three types of measures to estimate such quantities. The first type of measures make direct use of the estimated variances; the second type of measures use residual sums of squares in analogy to the linear regression; the third type of measures are based on the Kullback\textendash Leibler information gain. All the measures can be easily obtained from software programs that fit linear mixed models. We study the performance of the measures through Monte Carlo simulations, and illustrate the usefulness of the measures on data sets. Copyright \textcopyright{} 2003 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {empirical Bayes,explained randomness,Kullback–Leibler information,predicted random effects,residual sum of squares},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.1572},
  file = {C\:\\Users\\micro\\Zotero\\storage\\A5B8SZMF\\Xu - 2003 - Measuring explained variation in linear mixed effe.pdf;C\:\\Users\\micro\\Zotero\\storage\\WWUWV29Y\\sim.html}
}

@article{zhan2021,
  title = {Methods for Dealing with Unequal Cluster Sizes in Cluster Randomized Trials: {{A}} Scoping Review},
  shorttitle = {Methods for Dealing with Unequal Cluster Sizes in Cluster Randomized Trials},
  author = {Zhan, Denghuang and Xu, Liang and Ouyang, Yongdong and Sawatzky, Richard and Wong, Hubert},
  year = {2021},
  month = jul,
  journal = {PLOS ONE},
  volume = {16},
  number = {7},
  pages = {e0255389},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0255389},
  abstract = {In a cluster-randomized trial (CRT), the number of participants enrolled often varies across clusters. This variation should be considered during both trial design and data analysis to ensure statistical performance goals are achieved. Most methodological literature on the CRT design has assumed equal cluster sizes. This scoping review focuses on methodology for unequal cluster size CRTs. EMBASE, Medline, Google Scholar, MathSciNet and Web of Science databases were searched to identify English-language articles reporting on methodology for unequal cluster size CRTs published until March 2021. We extracted data on the focus of the paper (power calculation, Type I error etc.), the type of CRT, the type and the range of parameter values investigated (number of clusters, mean cluster size, cluster size coefficient of variation, intra-cluster correlation coefficient, etc.), and the main conclusions. Seventy-nine of 5032 identified papers met the inclusion criteria. Papers primarily focused on the parallel-arm CRT (p-CRT, n = 60, 76\%) and the stepped-wedge CRT (n = 14, 18\%). Roughly 75\% of the papers addressed trial design issues (sample size/power calculation) while 25\% focused on analysis considerations (Type I error, bias, etc.). The ranges of parameter values explored varied substantially across different studies. Methods for accounting for unequal cluster sizes in the p-CRT have been investigated extensively for Gaussian and binary outcomes. Synthesizing the findings of these works is difficult as the magnitude of impact of the unequal cluster sizes varies substantially across the combinations and ranges of input parameters. Limited investigations have been done for other combinations of a CRT design by outcome type, particularly methodology involving binary outcomes\textemdash the most commonly used type of primary outcome in trials. The paucity of methodological papers outside of the p-CRT with Gaussian or binary outcomes highlights the need for further methodological development to fill the gaps.},
  langid = {english},
  keywords = {Approximation methods,Cluster trials,Covariance,Database searching,Information retrieval,Maximum likelihood estimation,Research errors,Simulation and modeling},
  file = {C\:\\Users\\micro\\Zotero\\storage\\BJQWBNX2\\Zhan et al. - 2021 - Methods for dealing with unequal cluster sizes in .pdf;C\:\\Users\\micro\\Zotero\\storage\\7R6K4FYY\\article.html}
}

@misc{zotero-4258,
  title = {{Improved generalized estimating equations for incomplete longitudinal binary data, covariance estimation in small samples, and ordinal data}},
  publisher = {{University of North Carolina at Chapel Hill}},
  langid = {http://id.loc.gov/vocabulary/iso639-2/eng},
  annotation = {Context Object: url\_ver=Z39.88-2004\&ctx\_ver=Z39.88-2004\&rft\_val\_fmt=info\%3Aofi\%2Ffmt\%3Akev\%3Amtx\%3Adc\&rfr\_id=info\%3Asid\%2Fblacklight.rubyforge.org\%3Agenerator\&rft.title=Improved+generalized+estimating+equations+for+incomplete+longitudinal+binary+data\%2C+covariance+estimation+in+small+samples\%2C+and+ordinal+data\&rft.publisher=University+of+North+Carolina+at+Chapel+Hill\&rft.format=Dissertation\&rft.language=http\%3A\%2F\%2Fid.loc.gov\%2Fvocabulary\%2Fiso639-2\%2Feng},
  file = {C\:\\Users\\micro\\Zotero\\storage\\HL2QZ2RL\\6w924c447.html}
}

@misc{zotero-4278,
  title = {Empirically Supported Treatments or Type {{I}} Errors? {{Problems}} with the Analys...: {{EBSCOhost}}},
  howpublished = {http://web.b.ebscohost.com.ezproxy.lib.utexas.edu/ehost/detail/detail?vid=0\&sid=aa90ed4e-6128-4c02-a007-8f498799397d\%40pdc-v-sessmgr03\&bdata=JnNpdGU9ZWhvc3QtbGl2ZQ\%3d\%3d\#AN=2005-13740-015\&db=pdh},
  file = {C\:\\Users\\micro\\Zotero\\storage\\5D4XP7NA\\detail.html}
}

@misc{zotero-4340,
  title = {The {{Variance}} of {{Intraclass Correlations}} in {{Three-}} and {{Four-Level Models}} - {{Larry V}}. {{Hedges}}, {{E}}. {{C}}. {{Hedberg}}, {{Arend M}}. {{Kuyper}}, 2012},
  howpublished = {https://journals-sagepub-com.ezproxy.lib.utexas.edu/doi/10.1177/0013164412445193},
  file = {C\:\\Users\\micro\\Zotero\\storage\\2KIM7DYU\\0013164412445193.html}
}

@misc{zotero-4443,
  title = {Confidence Intervals for Intraclass Correlation Coefficients in Variance Components Models - {{Nino Demetrashvili}}, {{Ernst C Wit}}, {{Edwin R}} van Den {{Heuvel}}, 2016},
  howpublished = {https://journals-sagepub-com.ezproxy.lib.utexas.edu/doi/full/10.1177/0962280214522787},
  file = {C\:\\Users\\micro\\Zotero\\storage\\55SBKESX\\0962280214522787.html}
}

@misc{zotero-4480,
  title = {Robust Variance Estimation with Dependent Effect Sizes: Practical Considerations Including a Software Tutorial in {{Stata}} and Spss - {{Tanner}}-{{Smith}} - 2014 - {{Research Synthesis Methods}} - {{Wiley Online Library}}},
  howpublished = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1091},
  file = {C\:\\Users\\micro\\Zotero\\storage\\7AYD4ZAE\\jrsm.html}
}

@article{zou2004,
  title = {Confidence {{Interval Estimation}} of the {{Intraclass Correlation Coefficient}} for {{Binary Outcome Data}}},
  author = {Zou, Guangyong and Donner, Allan},
  year = {2004},
  journal = {Biometrics},
  volume = {60},
  number = {3},
  pages = {807--811},
  issn = {1541-0420},
  doi = {10.1111/j.0006-341X.2004.00232.x},
  abstract = {We obtain closed-form asymptotic variance formulae for three point estimators of the intraclass correlation coefficient that may be applied to binary outcome data arising in clusters of variable size. Our results include as special cases those that have previously appeared in the literature (Fleiss and Cuzick, 1979, Applied Psychological Measurement3, 537\textendash 542; Bloch and Kraemer, 1989, Biometrics45, 269\textendash 287; Altaye, Donner, and Klar, 2001, Biometrics57, 584\textendash 588). Simulation results indicate that confidence intervals based on the estimator proposed by Fleiss and Cuzick provide coverage levels close to nominal over a wide range of parameter combinations. Two examples are presented.},
  langid = {english},
  keywords = {Agreement,Cluster-randomization trials,Common correlation model,Delta method,Exchangeability,Kappa,Reliability,Variance},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.0006-341X.2004.00232.x},
  file = {C\:\\Users\\micro\\Zotero\\storage\\NLZWQ5CT\\Zou and Donner - 2004 - Confidence Interval Estimation of the Intraclass C.pdf;C\:\\Users\\micro\\Zotero\\storage\\VQKW6VTW\\j.0006-341X.2004.00232.html}
}


